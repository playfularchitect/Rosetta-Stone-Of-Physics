\# \[note\] v13.13.9-onecell-megacell++++ — full bundle  
\# Fixes: robust MDL\* for zero numerators, strict ledger filter (0\<p\<1), single header print,  
\# MC floors, MDL\*-aware nearest-fraction display, A/B CI cleanup, BF (spike & spike+slab),  
\# MC PB tails, FWER, e-values, LOO, jackknife, registry/ledger merge.  
import os, re, math, hashlib, random, inspect, sys, json, csv, datetime, contextlib, io  
from dataclasses import dataclass  
from fractions import Fraction  
from typing import List, Tuple, Optional, Dict  
import numpy as np

\# \------------------------ Artifact helpers & exporters \------------------------

def ensure\_outdir(base: str \= "results") \-\> str:  
    ts \= datetime.datetime.now().strftime("%Y%m%d-%H%M%S")  
    outdir \= os.path.join(base, f"run\_{ts}")  
    os.makedirs(outdir, exist\_ok=True)  
    os.makedirs(os.path.join(outdir, "sections"), exist\_ok=True)  
    os.makedirs(os.path.join(outdir, "ledger"), exist\_ok=True)  
    return outdir

def write\_text(path: str, text: str):  
    with open(path, "w", encoding="utf-8") as f:  
        f.write(text)

def write\_json(path: str, obj):  
    with open(path, "w", encoding="utf-8") as f:  
        json.dump(obj, f, indent=2, sort\_keys=True)

def write\_csv(path: str, header: List\[str\], rows: List\[List\]):  
    with open(path, "w", encoding="utf-8", newline="") as f:  
        w \= csv.writer(f)  
        w.writerow(header)  
        w.writerows(rows)

class Tee:  
    def \_\_init\_\_(self, \*streams):  
        self.streams \= streams  
    def write(self, data):  
        for s in self.streams:  
            s.write(data)  
            s.flush()  
    def flush(self):  
        for s in self.streams:  
            s.flush()

def dump\_environment(outdir: str):  
    info \= {  
        "python\_version": sys.version,  
        "numpy\_version": getattr(np, "\_\_version\_\_", "unknown"),  
        "platform": os.name,  
        "cwd": os.getcwd(),  
    }  
    write\_json(os.path.join(outdir, "environment.json"), info)

def dump\_registry(outdir: str, registry: List\[Fraction\], file\_hashes: Dict\[str,str\], reg\_hash: str):  
    reg\_list \= \[f"{fr.numerator}/{fr.denominator}" for fr in sorted(registry, key=lambda f:(f.numerator, f.denominator))\]  
    write\_text(os.path.join(outdir, "ledger", "registry\_fractions.txt"), "\\n".join(reg\_list))  
    write\_text(os.path.join(outdir, "ledger", "registry\_hash.txt"), reg\_hash \+ "\\n")  
    write\_json(os.path.join(outdir, "ledger", "file\_hashes.json"), file\_hashes)

\# \--- exporters \---

def export\_candidates(outdir: str, registry: List\[Fraction\]):  
    rows \= \[\]  
    for b in BLOCKS:  
        naA,\_ \= nearest\_fraction(b.pA, registry)  
        nbB,\_ \= nearest\_fraction(b.pB, registry)  
        ph \= b.kX/b.n  
        lo,hi \= wilson\_ci(b.kX, b.n, 1.96)  
        nd,\_ \= nearest\_fraction(ph, ALL\_DYADICS)  
        rows.append(\[  
            b.dataset, b.sheet, b.tag, b.n, b.kX,  
            f"{b.pA:.12f}", f"{naA.numerator}/{naA.denominator}", mdl\_star(naA),  
            f"{b.pB:.12f}", f"{nbB.numerator}/{nbB.denominator}", mdl\_star(nbB),  
            f"{ph:.11f}", f"{lo:.11f}", f"{hi:.11f}", f"{nd.numerator}/{nd.denominator}", mdl\_star(nd)  
        \])  
    write\_csv(os.path.join(outdir, "sections", "candidates.csv"),  
              \["dataset","sheet","tag","n","kX","pA","nearestA","MDL\*A","pB","nearestB","MDL\*B","pXOR","CI\_lo","CI\_hi","nearest\_dyadic","MDL\*\_dy"\],  
              rows)

def export\_nearest\_dyadic\_z(outdir: str):  
    rows \= \[\]  
    for b in BLOCKS:  
        nd, z \= nearest\_dyadic\_z(b, ALL\_DYADICS)  
        ph \= b.kX/b.n  
        rows.append(\[b.dataset, b.tag, f"{ph:.11e}", f"{float(nd):.11e}", f"{z:.3f}"\])  
    write\_csv(os.path.join(outdir, "sections", "nearest\_dyadic\_z.csv"),  
              \["dataset","tag","p\_hat","nearest\_dyadic","z"\], rows)

def export\_bayes(outdir: str):  
    lnBF\_sp\_ALL, lnBF\_ss\_ALL, eps\_ALL \= total\_lnBF(BLOCKS, ALL\_DYADICS, slab\_ab=(0.5,0.5))  
    lnBF\_sp\_TNY, lnBF\_ss\_TNY, eps\_TNY \= total\_lnBF(BLOCKS, TINY\_DYADICS, slab\_ab=(0.5,0.5))  
    write\_json(os.path.join(outdir, "sections", "bayes.json"), {  
        "ALL": {"lnBF\_spike": lnBF\_sp\_ALL, "lnBF\_spike\_slab": lnBF\_ss\_ALL, "eps\_star": eps\_ALL},  
        "TINY": {"lnBF\_spike": lnBF\_sp\_TNY, "lnBF\_spike\_slab": lnBF\_ss\_TNY, "eps\_star": eps\_TNY},  
    })

def export\_predictive\_holdout(outdir: str):  
    rows \= \[\]  
    for ds in DATASET\_NAMES:  
        idx\_train \= \[i for i,b in enumerate(BLOCKS) if b.dataset \!= ds\]  
        idx\_test  \= \[i for i,b in enumerate(BLOCKS) if b.dataset \== ds\]  
        \_, lnBF\_ss\_train, eps\_star \= total\_lnBF\_by(BLOCKS, idx\_train, ALL\_DYADICS, slab\_ab=(0.5,0.5))  
        ln\_spike\_t, ln\_ss\_t, \_ \= total\_lnBF\_by(BLOCKS, idx\_test, ALL\_DYADICS, slab\_ab=(0.5,0.5), eps=eps\_star)  
        rows.append(\[ds, eps\_star, ln\_ss\_t\])  
    write\_csv(os.path.join(outdir, "sections", "predictive\_holdout.csv"), \["dataset","eps\_star\_train","predictive\_lnBF\_test"\], rows)

def export\_pb\_tails(outdir: str):  
    rows \= \[\]  
    summary \= \[\]  
    for z in Z\_FAMILY:  
        for dy\_pool, dy\_name in \[(ALL\_DYADICS,"ALL"), (TINY\_DYADICS,"TINY")\]:  
            (obs\_b, tail\_b, \_), (obs\_c, tail\_c, \_) \= mc\_pb\_tail\_both(BLOCKS, z, 0.5, 0.5, MC\_SIMS\_PRIMARY, dy\_pool)  
            tb\_disp \= max(tail\_b, 1.0/MC\_SIMS\_PRIMARY)  
            tc\_disp \= max(tail\_c, 1.0/MC\_SIMS\_PRIMARY)  
            se\_b\_disp \= math.sqrt(max(tb\_disp\*(1-tb\_disp), 0.0) / MC\_SIMS\_PRIMARY)  
            se\_c\_disp \= math.sqrt(max(tc\_disp\*(1-tc\_disp), 0.0) / MC\_SIMS\_PRIMARY)  
            rows.append(\[z, dy\_name, obs\_b, f"{tb\_disp:.12f}", f"{se\_b\_disp:.6g}", obs\_c, f"{tc\_disp:.12f}", f"{se\_c\_disp:.6g}"\])  
            summary.append({"z":z, "dy":dy\_name, "obs\_blocks":obs\_b, "p\_blocks":tb\_disp, "se\_blocks":se\_b\_disp, "obs\_clusters":obs\_c, "p\_clusters":tc\_disp, "se\_clusters":se\_c\_disp})  
    write\_csv(os.path.join(outdir, "sections", "pb\_tails.csv"),  
              \["z","dy\_pool","obs\_blocks","p\_blocks","se\_blocks","obs\_clusters","p\_clusters","se\_clusters"\], rows)  
    return summary

def export\_fwer(outdir: str):  
    cb \= 0; cc \= 0  
    obs\_b \= \[\]; obs\_c \= \[\]  
    for z in Z\_FAMILY:  
        for pool in (ALL\_DYADICS, TINY\_DYADICS):  
            ob, \_, \_ \= mc\_pb\_tail(BLOCKS, z, 0.5, 0.5, MC\_SIMS\_PRIMARY, pool)  
            oc, \_, \_ \= cluster\_pb\_tail(BLOCKS, z, 0.5, 0.5, MC\_SIMS\_PRIMARY, pool)  
            obs\_b.append(ob); obs\_c.append(oc)  
    for \_ in range(MC\_SIMS\_FWER):  
        res\_b, res\_c \= \[\], \[\]  
        for z in Z\_FAMILY:  
            for pool in (ALL\_DYADICS, TINY\_DYADICS):  
                sim\_block\_hits \= 0  
                ds\_hit \= \[0\]\*len(DATASET\_NAMES)  
                for blk in BLOCKS:  
                    p \= np.random.beta(0.5,0.5)  
                    k \= np.random.binomial(blk.n, p)  
                    if ci\_contains\_any\_dyadic(k, blk.n, z, pool):  
                        sim\_block\_hits \+= 1  
                        ds\_hit\[DS\_INDEX\[blk.dataset\]\] \= 1  
                res\_b.append(sim\_block\_hits)  
                res\_c.append(sum(ds\_hit))  
        flag\_b \= any(res\_b\[i\] \>= obs\_b\[i\] for i in range(len(res\_b)))  
        flag\_c \= any(res\_c\[i\] \>= obs\_c\[i\] for i in range(len(res\_c)))  
        cb \+= 1 if flag\_b else 0  
        cc \+= 1 if flag\_c else 0  
    p\_b \= cb/MC\_SIMS\_FWER; p\_c \= cc/MC\_SIMS\_FWER  
    write\_json(os.path.join(outdir, "sections", "fwer.json"), {"p\_blocks": p\_b, "p\_clusters": p\_c, "sims": MC\_SIMS\_FWER})

def export\_evalues(outdir: str):  
    pvals \= per\_block\_pvals(BLOCKS, 2.24, TINY\_DYADICS, MC\_SIMS\_EVAL)  
    pvals \= \[max(p, 1.0/MC\_SIMS\_EVAL) for p in pvals\]  
    hits \= sum(1 for b in BLOCKS if ci\_contains\_any\_dyadic(b.kX, b.n, 2.24, TINY\_DYADICS))  
    tippett \= float(min(pvals))  
    m \= len(pvals); ps \= sorted(pvals)  
    simes \= float(min((m/(i+1))\*ps\[i\] for i in range(m)))  
    prod\_E \= float(np.prod(\[1.0/p for p in pvals\]))  
    write\_json(os.path.join(outdir, "sections", "evalues.json"), {  
        "hits": hits,  
        "tippett\_min\_p": tippett,  
        "simes\_p": simes,  
        "product\_E": prod\_E,  
        "per\_block\_pvals": pvals,  
        "sims": MC\_SIMS\_EVAL  
    })

def export\_cluster\_loo(outdir: str):  
    rows \= \[\]  
    ob\_all, pb\_all, \_ \= cluster\_pb\_tail(BLOCKS, 2.24, 0.5, 0.5, MC\_SIMS\_PRIMARY, TINY\_DYADICS)  
    rows.append(\["all", len(DATASET\_NAMES), ob\_all, max(pb\_all, 1.0/MC\_SIMS\_PRIMARY)\])  
    for ds in DATASET\_NAMES:  
        sub \= \[b for b in BLOCKS if b.dataset \!= ds\]  
        ob, pb, \_ \= cluster\_pb\_tail(sub, 2.24, 0.5, 0.5, MC\_SIMS\_PRIMARY, TINY\_DYADICS)  
        rows.append(\[f"drop {ds}", len(set(b.dataset for b in sub)), ob, max(pb, 1.0/MC\_SIMS\_PRIMARY)\])  
    write\_csv(os.path.join(outdir, "sections", "cluster\_loo.csv"), \["case","clusters","obs","p"\], rows)

def export\_jackknife(outdir: str):  
    rows \= \[\]  
    ob\_full, pb\_full, \_ \= mc\_pb\_tail(BLOCKS, 2.24, 0.5, 0.5, MC\_SIMS\_PRIMARY, TINY\_DYADICS)  
    rows.append(\["full", ob\_full, max(pb\_full, 1.0/MC\_SIMS\_PRIMARY), "-"\])  
    for i,b in enumerate(BLOCKS):  
        sub \= BLOCKS\[:i\] \+ BLOCKS\[i+1:\]  
        ob, pb, \_ \= mc\_pb\_tail(sub, 2.24, 0.5, 0.5, MC\_SIMS\_PRIMARY, TINY\_DYADICS)  
        rows.append(\[f"drop {b.dataset}|{b.tag}", ob, max(pb, 1.0/MC\_SIMS\_PRIMARY), i\])  
    write\_csv(os.path.join(outdir, "sections", "jackknife.csv"), \["case","obs","p","index"\], rows)

def export\_config(outdir: str, reg\_hash: str, file\_hashes: Dict\[str,str\]):  
    cfg \= {  
        "seed": SEED,  
        "MC\_SIMS\_PRIMARY": MC\_SIMS\_PRIMARY,  
        "MC\_SIMS\_FWER": MC\_SIMS\_FWER,  
        "MC\_SIMS\_EVAL": MC\_SIMS\_EVAL,  
        "Z\_FAMILY": list(Z\_FAMILY),  
        "TINY\_K\_MIN": TINY\_K\_MIN,  
        "MAX\_MDL\_NEAREST": MAX\_MDL\_NEAREST,  
        "FAST\_PVALS": FAST\_PVALS\_GATE,  
        "ASCII\_ONLY": USE\_ASCII,  
        "registry\_hash": reg\_hash,  
        "ledger\_paths": LEDGER\_PATHS,  
        "code\_fingerprint": code\_fingerprint(),  
    }  
    write\_json(os.path.join(outdir, "config.json"), cfg)

def snapshot\_code(outdir: str):  
    try:  
        src \= inspect.getsource(sys.modules.get(\_\_name\_\_))  
    except Exception:  
        src \= "\# source snapshot unavailable\\n"  
    write\_text(os.path.join(outdir, "script\_snapshot.py"), src)

def write\_readme(outdir: str):  
    md \= \[\]  
    md.append("\# Megacell v13.13.9-onecell-megacell++++ — Run Artifacts\\n")  
    md.append("This folder contains a full audit trail: config & environment, ledger registry and hashes, and per-section CSV/JSON outputs.\\n\\n")  
    md.append("\#\# Contents\\n")  
    md.append("- \`config.json\`: run configuration, seeds, fingerprint, registry hash.\\n")  
    md.append("- \`environment.json\`: Python & NumPy versions, platform info.\\n")  
    md.append("- \`ledger/\`: registry fractions, prereg hash, file SHA256s.\\n")  
    md.append("- \`sections/\`: CSV/JSON exports for candidates, PB tails, FWER, Bayes, predictive holdout, E-values, nearest-dyadic z, cluster LOO, jackknife.\\n\\n")  
    md.append("\#\# Verification tips\\n")  
    md.append("- Re-run with the same seed to reproduce MC paths.\\n")  
    md.append("- Compare \`registry\_hash\` and file SHA256s to validate the prereg ledger.\\n")  
    md.append("- Check that section CSVs match the corresponding rows in your console output.\\n")  
    write\_text(os.path.join(outdir, "README.md"), "".join(md))

\# \------------------------ Repro / constants \------------------------  
SEED \= 4312025  
np.random.seed(SEED); random.seed(SEED)  
MC\_SIMS\_PRIMARY \= 2000         \# MC for PB tails & per-block pvals  
MC\_SIMS\_FWER    \= 1000         \# family-wise sims  
MC\_SIMS\_EVAL    \= 1000         \# per-block pvals for e-values  
MAX\_MDL\_NEAREST \= 30           \# hide ultra-complex ledger fracs in "nearest"  
TINY\_K\_MIN      \= 8            \# tiny dyadics threshold (≤ 1/256)  
Z\_FAMILY        \= (1.96, 2.24, 2.58)  
LEDGER\_PATHS    \= \["./Master ledger sep 4th.md", "/content/Master ledger sep 4th.md"\]  
USE\_ASCII       \= bool(os.environ.get("ASCII\_ONLY"))  \# ASCII fallback for "≤"  
FAST\_PVALS\_GATE \= bool(os.environ.get("FAST\_PVALS"))  \# opt-in safety gate for early-exit pvals

\# \------------------------ Small utils \------------------------  
def bitlen\_nonneg(n:int)-\>int:  
    """Bit-length with conventions: bitlen(0)=1 (distinct zero tag), bitlen(1)=0, else n.bit\_length()."""  
    n \= abs(int(n))  
    if n \== 0: return 1  
    if n \== 1: return 0  
    return n.bit\_length()

def mdl\_star(fr:Fraction)-\>int:  
    """MDL\* \= bitlen(num) \+ bitlen(den) on reduced fraction, with bitlen(0)=1, bitlen(1)=0."""  
    fr \= Fraction(fr).limit\_denominator()  
    return bitlen\_nonneg(fr.numerator) \+ bitlen\_nonneg(fr.denominator)

def fmt\_tail(val: float, sims: int) \-\> str:  
    \# Floor any MC zero to \<=1/sims for conservative display  
    if val \<= 0.0:  
        floor \= f"{1.0/sims:.12f}"  
        return (f"\<={floor}" if USE\_ASCII else f"≤ {floor}")  
    return f"{val:.12f}"

def clamp(x, lo, hi): return max(lo, min(hi, x))

def sha256\_of\_file(path:str)-\>Optional\[str\]:  
    if not os.path.exists(path): return None  
    h \= hashlib.sha256()  
    with open(path,'rb') as f:  
        for chunk in iter(lambda: f.read(1\<\<16), b''):  
            h.update(chunk)  
    return h.hexdigest()

def sha256\_of\_ledger(fracs:List\[Fraction\])-\>str:  
    \# deterministic: sort by (numerator,denominator) and hash "num/den" lines  
    arr \= sorted(\[f"{Fraction(fr).limit\_denominator().numerator}/{Fraction(fr).limit\_denominator().denominator}" for fr in fracs\])  
    data \= ("\\n".join(arr)).encode()  
    return hashlib.sha256(data).hexdigest()

def code\_fingerprint(path: Optional\[str\]=None)-\>str:  
    """Prefer hashing the whole file; fallback to hashing bytecode of callables; final fallback is version tag."""  
    try:  
        if path is None:  
            mod \= sys.modules.get(\_\_name\_\_)  
            path \= (inspect.getsourcefile(mod) or getattr(mod, "\_\_file\_\_", None))  
        if path and os.path.exists(path):  
            with open(path, "rb") as f:  
                data \= f.read()  
            return hashlib.sha1(data).hexdigest()\[:12\]  
    except Exception:  
        pass  
    try:  
        blob \= \[\]  
        for k,v in sorted(globals().items()):  
            if callable(v) and hasattr(v, "\_\_code\_\_"):  
                blob.append(v.\_\_code\_\_.co\_code)  
        if blob:  
            return hashlib.sha1(b"".join(blob)).hexdigest()\[:12\]  
    except Exception:  
        pass  
    return "v13.13.9"

\# \------------------------ Ledger parsing & working set \------------------------  
FRAC\_RE \= re.compile(r"\\b(\\d+)\\s\*/\\s\*(\\d+)\\b")  
DEC\_RE  \= re.compile(r"\\b(\[01\]?\\.\\d+|\\d+\\.\\d+)\\b")  \# simple decimals

def parse\_fracs\_from\_text(txt:str)-\>List\[Fraction\]:  
    """Tokenize to avoid overlaps; parse a/b and decimals strictly between 0 and 1."""  
    toks \= re.findall(r"\[0-9./\]+", txt)  
    frs: List\[Fraction\] \= \[\]  
    for tok in toks:  
        if "/" in tok:  
            m \= re.fullmatch(r"(\\d+)\\s\*/\\s\*(\\d+)", tok)  
            if m:  
                a,b \= int(m.group(1)), int(m.group(2))  
                if b \!= 0:  
                    frs.append(Fraction(a, b).limit\_denominator())  
        else:  
            if "." in tok:  
                try:  
                    val \= float(tok)  
                    if 0 \< val \< 1:  
                        frs.append(Fraction(val).limit\_denominator(10\_000\_000))  
                except Exception:  
                    pass  
    return frs

def try\_load\_ledger(paths:List\[str\])-\>Tuple\[List\[Fraction\], Dict\[str, str\]\]:  
    found \= \[\]  
    hashes \= {}  
    for p in paths:  
        h \= sha256\_of\_file(p)  
        if h: hashes\[p\] \= h  
        if os.path.exists(p):  
            try:  
                with open(p,'r',encoding='utf-8') as f:  
                    txt \= f.read()  
                frs \= parse\_fracs\_from\_text(txt)  
                found.extend(frs)  
            except Exception:  
                pass  
    return found, hashes

\# Fallback core set (small, stable)  
FALLBACK \= \[  
    Fraction(1,2), Fraction(1,4), Fraction(1,6), Fraction(4,15),  
    Fraction(2048,4095), Fraction(2047,4095), Fraction(1732,3463),  
    Fraction(1974,3949), Fraction(1930,3861), Fraction(1909,3819),  
    Fraction(1,8), Fraction(3,8), Fraction(5,8),  
    Fraction(1,16), Fraction(1,32), Fraction(1,64), Fraction(1,128),  
    Fraction(1,256), Fraction(1,512), Fraction(1,1024), Fraction(1,2048),  
    Fraction(1,4096), Fraction(1,8192), Fraction(1,16384), Fraction(1,32768),  
\]

def working\_ledger()-\>Tuple\[List\[Fraction\], Dict\[str,str\]\]:  
    file\_fracs, hashes \= try\_load\_ledger(LEDGER\_PATHS)  
    uniq \= {}  
    for fr in (FALLBACK \+ file\_fracs):  
        fr \= Fraction(fr).limit\_denominator()  
        \# strict filter: keep only proper probabilities 0\<p\<1  
        if fr \<= 0 or fr \>= 1: continue  
        uniq\[fr\] \= True  
    frs \= list(uniq.keys())  
    return frs, hashes

\# Dyadics pool & tiny subset  
def dyadics\_set(kmin:int=2, kmax:int=16)-\>List\[Fraction\]:  
    return \[Fraction(1, 2\*\*k) for k in range(kmin, kmax+1)\]  
ALL\_DYADICS \= dyadics\_set(2,16)  
TINY\_DYADICS \= dyadics\_set(TINY\_K\_MIN, 16\)

\# \------------------------ Candidate blocks (fixed from data) \------------------------  
@dataclass  
class Block:  
    dataset:str  
    sheet:str  
    tag:str  
    n:int  
    kX:int  
    pA:float  
    pB:float

\# constants from your runs  
BLOCKS: List\[Block\] \= \[  
    Block("02\_54\_ALL.xlsx","5","cols7\_9\_10\_11", 203\_654\_890, 5550, 0.500066735446, 0.500046249810),  
    Block("02\_54\_ALL.xlsx","5","cols3\_5\_6\_7",   203\_690\_473, 6046, 0.500017956166, 0.500041334776),  
    Block("03\_43\_ALL.xlsx","5","cols7\_9\_10\_11", 107\_075\_552, 3140, 0.500065860039, 0.500043912919),  
    Block("03\_43\_ALL.xlsx","5","cols3\_5\_6\_7",   107\_118\_383, 3383, 0.500131588058, 0.500156261694),  
    Block("19\_45\_ALL.xlsx","5","cols7\_9\_10\_11", 182\_624\_410, 5214, 0.500227494233, 0.500206412713),  
    Block("19\_45\_ALL.xlsx","5","cols3\_5\_6\_7",   182\_694\_717, 5640, 0.499962437337, 0.499986324180),  
    Block("02\_54\_ALL.xlsx","5","cols2\_4\_5\_6",       8\_209, 2006, 0.263491290048, 0.175539042514),  
    Block("19\_45\_ALL.xlsx","34567","cols2\_4\_5\_6",  38\_826, 10190, 0.265466440014, 0.169345284088),  
\]

\# dataset clusters  
DATASET\_NAMES \= sorted(set(b.dataset for b in BLOCKS))  
DS\_INDEX \= {d:i for i,d in enumerate(DATASET\_NAMES)}

\# \------------------------ Intervals & CI coverage \------------------------  
def wilson\_ci(k:int, n:int, z:float)-\>Tuple\[float,float\]:  
    if n \<= 0: return (0.0, 1.0)  
    p \= k/n  
    z2 \= z\*z  
    den \= 1.0 \+ z2/n  
    center \= (p \+ z2/(2\*n)) / den  
    base \= max(p\*(1-p) \+ z2/(4\*n), 0.0)  
    rad \= z\*math.sqrt(max(base/n, 1e-300)) / den  
    lo \= clamp(center \- rad, 0.0, 1.0)  
    hi \= clamp(center \+ rad, 0.0, 1.0)  
    return lo, hi

def ci\_contains\_any\_dyadic(k:int, n:int, z:float, dy\_pool:List\[Fraction\])-\>bool:  
    lo, hi \= wilson\_ci(k, n, z)  
    for fr in dy\_pool:  
        val \= float(fr)  
        if lo \<= val \<= hi:  
            return True  
    return False

\# \------------------------ PB tails via MC (blocks & clusters) \------------------------  
def mc\_pb\_tail(blocks:List\[Block\], z:float, a:float, b:float, sims:int, dy\_pool:List\[Fraction\])-\>Tuple\[int,float,Tuple\[float,float\]\]:  
    \# observed  
    obs\_hits \= sum(1 for blk in blocks if ci\_contains\_any\_dyadic(blk.kX, blk.n, z, dy\_pool))  
    \# simulate  
    cnt \= 0  
    for \_ in range(sims):  
        sim\_hits \= 0  
        for blk in blocks:  
            p \= np.random.beta(a,b)  
            k \= np.random.binomial(blk.n, p)  
            if ci\_contains\_any\_dyadic(k, blk.n, z, dy\_pool): sim\_hits \+= 1  
        if sim\_hits \>= obs\_hits: cnt \+= 1  
    p \= cnt/sims  
    se \= math.sqrt(max(p\*(1-p), 0.0) / sims)  
    return obs\_hits, p, (p,se)

def cluster\_pb\_tail(blocks:List\[Block\], z:float, a:float, b:float, sims:int, dy\_pool:List\[Fraction\])-\>Tuple\[int,float,Tuple\[float,float\]\]:  
    \# observed cluster-hits  
    ds\_hits \= \[0\]\*len(DATASET\_NAMES)  
    for blk in blocks:  
        if ci\_contains\_any\_dyadic(blk.kX, blk.n, z, dy\_pool):  
            ds\_hits\[DS\_INDEX\[blk.dataset\]\] \= 1  
    obs \= sum(ds\_hits)

    cnt \= 0  
    for \_ in range(sims):  
        ds\_sim \= \[0\]\*len(DATASET\_NAMES)  
        for blk in blocks:  
            p \= np.random.beta(a,b)  
            k \= np.random.binomial(blk.n, p)  
            if ci\_contains\_any\_dyadic(k, blk.n, z, dy\_pool):  
                ds\_sim\[DS\_INDEX\[blk.dataset\]\] \= 1  
        if sum(ds\_sim) \>= obs: cnt \+= 1  
    p \= cnt/sims  
    se \= math.sqrt(max(p\*(1-p), 0.0) / sims)  
    return obs, p, (p,se)

\# Combined (blocks+clusters) to reuse draws within a (z, pool) pair  
\# Keeps null identical; reduces RNG/binomial cost \~×2 for section\_pb

def mc\_pb\_tail\_both(blocks:List\[Block\], z:float, a:float, b:float, sims:int, dy\_pool:List\[Fraction\]):  
    obs\_b \= sum(1 for blk in blocks if ci\_contains\_any\_dyadic(blk.kX, blk.n, z, dy\_pool))  
    ds\_obs\_flags \= \[0\]\*len(DATASET\_NAMES)  
    for blk in blocks:  
        if ci\_contains\_any\_dyadic(blk.kX, blk.n, z, dy\_pool):  
            ds\_obs\_flags\[DS\_INDEX\[blk.dataset\]\] \= 1  
    obs\_c \= sum(ds\_obs\_flags)

    cnt\_b \= 0; cnt\_c \= 0  
    for \_ in range(sims):  
        sim\_b \= 0  
        ds\_sim \= \[0\]\*len(DATASET\_NAMES)  
        for blk in blocks:  
            p \= np.random.beta(a,b)  
            k \= np.random.binomial(blk.n, p)  
            if ci\_contains\_any\_dyadic(k, blk.n, z, dy\_pool):  
                sim\_b \+= 1  
                ds\_sim\[DS\_INDEX\[blk.dataset\]\] \= 1  
        if sim\_b \>= obs\_b: cnt\_b \+= 1  
        if sum(ds\_sim) \>= obs\_c: cnt\_c \+= 1

    pb \= cnt\_b/sims; pc \= cnt\_c/sims  
    seb \= math.sqrt(max(pb\*(1-pb), 0.0) / sims)  
    sec \= math.sqrt(max(pc\*(1-pc), 0.0) / sims)  
    return (obs\_b, pb, (pb, seb)), (obs\_c, pc, (pc, sec))

\# \------------------------ “Nearest fraction” (MDL\*-aware) \------------------------  
def nearest\_fraction(p:float, frs:List\[Fraction\], max\_mdl:int=MAX\_MDL\_NEAREST)-\>Tuple\[Fraction,float\]:  
    best \= None; bestd \= 1e9; best\_mdl \= 10\*\*9  
    for fr in frs:  
        mdl \= mdl\_star(fr)  
        if mdl \> max\_mdl:  
            continue  
        d \= abs(p \- float(fr))  
        if (d \< bestd) or (abs(d \- bestd) \<= 1e-15 and mdl \< best\_mdl):  
            bestd \= d; best \= fr; best\_mdl \= mdl  
    if best is None:  
        best \= min(frs, key=lambda fr: abs(p \- float(fr)))  
        bestd \= abs(p \- float(best))  
    return best, bestd

\# \------------------------ Bayes Factors (log-domain, spike & slab) \------------------------  
def log\_beta(a,b):  \# log Beta(a,b)  
    return math.lgamma(a) \+ math.lgamma(b) \- math.lgamma(a+b)

def log\_beta\_binom\_noC(k:int, n:int, a:float, b:float)-\>float:  
    \# Beta-binomial marginal without choose-term (cancels in BF)  
    return log\_beta(k+a, n-k+b) \- log\_beta(a,b)

def prepare\_spike\_weights(dy\_pool:List\[Fraction\])-\>Tuple\[np.ndarray, np.ndarray\]:  
    \# MDL-weighted prior: w ∝ 2^{-MDL\*}  
    w\_logs \= \[\]  
    p\_logs \= \[\]  
    for fr in dy\_pool:  
        w\_logs.append(-mdl\_star(fr) \* math.log(2))  
        p \= float(fr)  
        p \= clamp(p, 1e-300, 1.0-1e-300)  
        p\_logs.append((math.log(p), math.log(1.0-p)))  
    m \= max(w\_logs)  
    w \= np.exp(np.array(w\_logs) \- m)  
    w \= w / np.sum(w)  
    return np.log(w), np.array(p\_logs)  \# (log\_w, \[(log p, log 1-p), ...\])

def logsumexp(arr:np.ndarray)-\>float:  
    m \= np.max(arr)  
    return m \+ math.log(np.sum(np.exp(arr \- m)))

def log\_spike\_marginal\_noC(k:int, n:int, logw:np.ndarray, plogs:np.ndarray)-\>float:  
    \# log sum\_i w\_i p\_i^k (1-p\_i)^{n-k}  
    terms \= logw \+ k\*plogs\[:,0\] \+ (n-k)\*plogs\[:,1\]  
    return logsumexp(terms)

def total\_lnBF(blocks:List\[Block\], dy\_pool:List\[Fraction\], slab\_ab=(0.5,0.5), eps:Optional\[float\]=None)-\>Tuple\[float,float,float\]:  
    a,b \= slab\_ab  
    logw, plogs \= prepare\_spike\_weights(dy\_pool)  
    \# spike-only vs slab (sum over blocks)  
    ln\_spike \= 0.0; ln\_slab \= 0.0  
    for blk in blocks:  
        ln\_spike \+= log\_spike\_marginal\_noC(blk.kX, blk.n, logw, plogs)  
        ln\_slab  \+= log\_beta\_binom\_noC(blk.kX, blk.n, a, b)  
    lnBF\_spike \= ln\_spike \- ln\_slab

    \# spike+slab mixture: maximize over eps in \[1e-9, 1-1e-9\]  
    def mix\_ll(eps\_):  
        eps\_ \= clamp(eps\_, 1e-9, 1-1e-9)  
        s \= 0.0  
        le \= math.log(eps\_); l1e \= math.log(1.0-eps\_)  
        for blk in blocks:  
            lsp \= log\_spike\_marginal\_noC(blk.kX, blk.n, logw, plogs)  
            lsl \= log\_beta\_binom\_noC(blk.kX, blk.n, a, b)  
            s \+= logsumexp(np.array(\[l1e \+ lsp, le \+ lsl\]))  
        return s

    if eps is None:  
        lo, hi \= 1e-9, 1-1e-9  
        for \_ in range(80):  
            m1 \= lo \+ (hi-lo)/3  
            m2 \= hi \- (hi-lo)/3  
            if mix\_ll(m1) \< mix\_ll(m2):  
                lo \= m1  
            else:  
                hi \= m2  
        eps\_star \= (lo+hi)/2  
    else:  
        eps\_star \= clamp(eps, 1e-9, 1-1e-9)

    ln\_mixture \= mix\_ll(eps\_star)  
    lnBF\_ss \= ln\_mixture \- ln\_slab  
    return lnBF\_spike, lnBF\_ss, eps\_star

def total\_lnBF\_by(blocks:List\[Block\], idxs:List\[int\], dy\_pool:List\[Fraction\], slab\_ab=(0.5,0.5), eps:Optional\[float\]=None)-\>Tuple\[float,float,float\]:  
    sub \= \[blocks\[i\] for i in idxs\]  
    return total\_lnBF(sub, dy\_pool, slab\_ab, eps)

\# \------------------------ A/B sign tests \------------------------  
def binom\_two\_sided\_p(n:int, k:int, p:float=0.5)-\>float:  
    from math import comb  
    \# exact tail; reflect for two-sided  
    pmf \= \[comb(n,i)\*(p\*\*i)\*((1-p)\*\*(n-i)) for i in range(n+1)\]  
    tail \= sum(pmf\[i\] for i in range(k, n+1))  
    return min(1.0, 2\*min(tail, 1-tail))

\# \------------------------ Per-block pvals (MC) for E-values \------------------------  
\# Optional safety-gated early exit: ONLY used if FAST\_PVALS=1; otherwise skip to maintain conservatism

def \_ci\_hit\_impossible\_upper\_bounds(n:int, z:float, target:float, k\_checks:List\[int\])-\>bool:  
    \# If even the upper bound for a set of small k cannot reach target, return True  
    for k in k\_checks:  
        hi \= wilson\_ci(k, n, z)\[1\]  
        if hi \>= target:  
            return False  
    return True

def per\_block\_pvals(blocks:List\[Block\], z:float, dy\_pool:List\[Fraction\], sims:int)-\>List\[float\]:  
    out \= \[\]  
    smallest \= float(min(dy\_pool))  
    for blk in blocks:  
        obs \= 1 if ci\_contains\_any\_dyadic(blk.kX, blk.n, z, dy\_pool) else 0  
        if obs \== 0:  
            out.append(1.0); continue  
        if FAST\_PVALS\_GATE:  
            \# Check a conservative set of k near 0; if none reach smallest dyadic, early-out  
            \# This is safe for TINY dyadics; gate keeps behavior opt-in.  
            k\_checks \= \[0, 1, max(2, int(smallest\*blk.n))\]  
            k\_checks \= sorted(set(k\_checks))  
            if \_ci\_hit\_impossible\_upper\_bounds(blk.n, z, smallest, k\_checks):  
                out.append(1.0); continue  
        hits \= 0  
        for \_ in range(sims):  
            p \= np.random.beta(0.5,0.5)  
            k \= np.random.binomial(blk.n, p)  
            if ci\_contains\_any\_dyadic(k, blk.n, z, dy\_pool): hits \+= 1  
        pval \= hits/sims  
        out.append(max(pval, 1.0/sims))  \# floor  
    return out

\# \------------------------ Display helpers \------------------------  
def nearest\_dyadic\_z(blk:Block, dy\_pool:List\[Fraction\])-\>Tuple\[Fraction,float\]:  
    ph \= blk.kX/blk.n  
    fr,\_ \= nearest\_fraction(ph, dy\_pool, max\_mdl=1\_000)  
    dy \= float(fr)  
    se \= math.sqrt(max(ph\*(1-ph)/blk.n, 1e-300))  
    z \= abs(ph \- dy)/se  
    return fr, z

def section\_candidates(registry:List\[Fraction\]):  
    print("=== CANDIDATE BLOCKS \===\\n")  
    for b in BLOCKS:  
        print(f"\[CAND\] file={b.dataset} sheet={b.sheet} tag={b.tag} C={{'11': {b.kX}}} tot={b.n:,}")  
        \# A/B: nearest registry fraction (MDL\*-aware)  
        naA,\_ \= nearest\_fraction(b.pA, registry)  
        nbB,\_ \= nearest\_fraction(b.pB, registry)  
        print(f"  \-\> A: p̂={b.pA:.12f}  CI=—  | Nearest registry fraction: {naA.numerator}/{naA.denominator} (MDL\*={mdl\_star(naA)})")  
        print(f"  \-\> B: p̂={b.pB:.12f}  CI=—  | Nearest registry fraction: {nbB.numerator}/{nbB.denominator} (MDL\*={mdl\_star(nbB)})")  
        \# XOR Wilson CI \+ nearest dyadic  
        ph \= b.kX/b.n  
        lo,hi \= wilson\_ci(b.kX, b.n, 1.96)  
        nd,\_ \= nearest\_fraction(ph, ALL\_DYADICS)  
        print(f"  \-\> XOR: p̂={ph:.11f}  CI={lo:.11f}..{hi:.11f} | Nearest dyadic: {nd.numerator}/{nd.denominator} (MDL\*={mdl\_star(nd)})")

def section\_ab\_sign():  
    As \= sum(1 for b in BLOCKS if b.pA \> 0.5)  
    Bs \= sum(1 for b in BLOCKS if b.pB \> 0.5)  
    pA \= binom\_two\_sided\_p(8, As, 0.5)  
    pB \= binom\_two\_sided\_p(8, Bs, 0.5)  
    print("\\n=== A/B sign tests around 1/2 (n=8 blocks each) \===")  
    print(f"  A: {As}/8 above 1/2 | two-sided binom p={pA:.6f}")  
    print(f"  B: {Bs}/8 above 1/2 | two-sided binom p={pB:.6f}")

def section\_pb(blocks: List\[Block\]):  
    print("\\n=== Proof P1 (dyadic CI-coverage) — MC-based PB tails (prereg family highlighted) \===")  
    for z in Z\_FAMILY:  
        for dy\_pool, dy\_name in \[(ALL\_DYADICS, "ALL"), (TINY\_DYADICS, "TINY")\]:  
            (obs\_b, tail\_b, (m\_b, se\_b)), (obs\_c, tail\_c, (m\_c, se\_c)) \= mc\_pb\_tail\_both(blocks, z, 0.5, 0.5, MC\_SIMS\_PRIMARY, dy\_pool)  
            tb\_disp \= max(tail\_b, 1.0/MC\_SIMS\_PRIMARY)  
            tc\_disp \= max(tail\_c, 1.0/MC\_SIMS\_PRIMARY)  
            se\_b\_disp \= math.sqrt(max(tb\_disp\*(1-tb\_disp), 0.0) / MC\_SIMS\_PRIMARY)  
            se\_c\_disp \= math.sqrt(max(tc\_disp\*(1-tc\_disp), 0.0) / MC\_SIMS\_PRIMARY)  
            star \= "  \<-- prereg primary" if (abs(z-2.24) \< 1e-9 and dy\_name \== "TINY") else ""  
            print(  
                f"  z={z:.2f} \[{dy\_name}\]  blocks={obs\_b:\>1}  clusters={obs\_c:\>1}  "  
                f"PB\_tail(blocks)={fmt\_tail(tail\_b, MC\_SIMS\_PRIMARY)}±{se\_b\_disp:.3g}  "  
                f"PB\_tail(clusters)={fmt\_tail(tail\_c, MC\_SIMS\_PRIMARY)}±{se\_c\_disp:.3g}{star}"  
            )  
def familywise\_mc(blocks: List\[Block\], sims: int \= MC\_SIMS\_FWER):  
    \# observed vector  
    def obs\_hits():  
        out\_b, out\_c \= \[\], \[\]  
        for z in Z\_FAMILY:  
            for pool in (ALL\_DYADICS, TINY\_DYADICS):  
                ob, \_, \_ \= mc\_pb\_tail(blocks, z, 0.5, 0.5, MC\_SIMS\_PRIMARY, pool)  
                oc, \_, \_ \= cluster\_pb\_tail(blocks, z, 0.5, 0.5, MC\_SIMS\_PRIMARY, pool)  
                out\_b.append(ob)  
                out\_c.append(oc)  
        return out\_b, out\_c

    obs\_b, obs\_c \= obs\_hits()

    def sim\_once():  
        res\_b, res\_c \= \[\], \[\]  
        for z in Z\_FAMILY:  
            for pool in (ALL\_DYADICS, TINY\_DYADICS):  
                sim\_block\_hits \= 0  
                ds\_hit \= \[0\] \* len(DATASET\_NAMES)  
                for blk in blocks:  
                    p \= np.random.beta(0.5, 0.5)  
                    k \= np.random.binomial(blk.n, p)  
                    if ci\_contains\_any\_dyadic(k, blk.n, z, pool):  
                        sim\_block\_hits \+= 1  
                        ds\_hit\[DS\_INDEX\[blk.dataset\]\] \= 1  
                res\_b.append(sim\_block\_hits)  
                res\_c.append(sum(ds\_hit))  
        flag\_b \= any(res\_b\[i\] \>= obs\_b\[i\] for i in range(len(res\_b)))  
        flag\_c \= any(res\_c\[i\] \>= obs\_c\[i\] for i in range(len(res\_c)))  
        return flag\_b, flag\_c

    cb \= 0  
    cc \= 0  
    for \_ in range(sims):  
        fb, fc \= sim\_once()  
        cb \+= 1 if fb else 0  
        cc \+= 1 if fc else 0  
    p\_b \= cb / sims  
    p\_c \= cc / sims  
    print("\\n=== Family-wise MC (FWER) over prereg endpoint family \===")  
    print(f"  FWER (blocks):   p≈ {fmt\_tail(p\_b, sims)}")  
    print(f"  FWER (clusters): p≈ {fmt\_tail(p\_c, sims)}")  
def section\_nearest\_dyadic\_z():  
    print("\\n=== Nearest-dyadic standardized distances (XOR) \===")  
    for b in BLOCKS:  
        nd, z \= nearest\_dyadic\_z(b, ALL\_DYADICS)  
        ph \= b.kX/b.n  
        print(f"  \[{b.dataset} | {b.tag}\] p̂={ph:.11e} nearest={float(nd):.11e}  z={z:.3f}")

def section\_bayes():  
    print("\\n=== XOR Bayes Factors — H0: Beta(1/2,1/2) vs H1: MDL-weighted dyadic mixture \===")  
    lnBF\_sp\_ALL, lnBF\_ss\_ALL, eps\_ALL \= total\_lnBF(BLOCKS, ALL\_DYADICS, slab\_ab=(0.5,0.5))  
    lnBF\_sp\_TNY, lnBF\_ss\_TNY, eps\_TNY \= total\_lnBF(BLOCKS, TINY\_DYADICS, slab\_ab=(0.5,0.5))  
    print(f"  \[dy=ALL\]  Spike-only lnBF={lnBF\_sp\_ALL: .6f} | Spike+Slab lnBF={lnBF\_ss\_ALL: .6f} (ε\*={eps\_ALL:.3f})")  
    print(f"  \[dy=TINY\] Spike-only lnBF={lnBF\_sp\_TNY: .6f} | Spike+Slab lnBF={lnBF\_ss\_TNY: .6f} (ε\*={eps\_TNY:.3f})")

def section\_predictive\_holdout():  
    print("\\n=== Predictive Bayes (hold-out by dataset) — dyadic spike weight ε tuned on train \===")  
    for ds in DATASET\_NAMES:  
        idx\_train \= \[i for i, b in enumerate(BLOCKS) if b.dataset \!= ds\]  
        idx\_test \= \[i for i, b in enumerate(BLOCKS) if b.dataset \== ds\]  
        \_, \_, eps\_star \= total\_lnBF\_by(BLOCKS, idx\_train, ALL\_DYADICS, slab\_ab=(0.5, 0.5))  
        \_, ln\_ss\_t, \_ \= total\_lnBF\_by(BLOCKS, idx\_test, ALL\_DYADICS, slab\_ab=(0.5, 0.5), eps=eps\_star)  
        note \= ""  
        if eps\_star \>= 0.999:  
            note \= "  (ε\*=1 collapses mixture to slab ⇒ predictive lnBF(test)=0)"

        elif eps\_star \<= 0.001:  
            note \= "  (ε\*=0 collapses mixture to spike)"  
        print(f"  hold-out={ds:\<13} | ε\*(train)={eps\_star:.3f} | predictive lnBF(test)={ln\_ss\_t: .6f}{note}")  
def section\_evalues():  
    print("\\n=== E-values & meta-combination (primary z=2.24, TINY) \===")  
    pvals \= per\_block\_pvals(BLOCKS, 2.24, TINY\_DYADICS, MC\_SIMS\_EVAL)  
    pvals \= \[max(p, 1.0/MC\_SIMS\_EVAL) for p in pvals\]  
    hits \= sum(1 for b in BLOCKS if ci\_contains\_any\_dyadic(b.kX, b.n, 2.24, TINY\_DYADICS))  
    tippett \= min(pvals)  
    m \= len(pvals)  
    ps \= sorted(pvals)  
    simes \= min((m/i) \* ps\[i-1\] for i in range(1, m+1))  
    prod\_E \= float(np.prod(\[1.0/p for p in pvals\]))  
    print(f"  observed hits={hits}")  
    print(f"  Tippett min-p={tippett:.12f}")  
    print(f"  Simes p≈{simes:.12f}")  
    print(f"  Product E-value={prod\_E:.3g}  (interpret via Markov bound)")  
def section\_min\_T():  
    zs \= \[\]  
    for b in BLOCKS:  
        nd, z \= nearest\_dyadic\_z(b, ALL\_DYADICS)  
        zs.append(z)  
    T\_obs \= sum(zs)  
    samples \= \[\]  
    for \_ in range(200):  
        zsum \= 0.0  
        for blk in BLOCKS:  
            p \= np.random.beta(0.5, 0.5)  
            k \= np.random.binomial(blk.n, p)  
            ph \= k / blk.n  
            nd, \_ \= nearest\_fraction(ph, ALL\_DYADICS, max\_mdl=1\_000)  
            se \= math.sqrt(max(ph \* (1 \- ph) / blk.n, 1e-300))  
            zsum \+= abs(ph \- float(nd)) / se  
        samples.append(zsum)  
    med \= float(np.median(samples))  
    lo \= float(np.percentile(samples, 5))  
    hi \= float(np.percentile(samples, 95))  
    p\_ge \= sum(1 for s in samples if s \>= T\_obs) / len(samples)  
    print("\\n=== Min-distance statistic T (ALL dyadics) — exploratory \===")  
    print("  Note: dyadic set excludes 1/2; under H0 near-1/2 draws are far from allowed dyadics, inflating null T.")  
    print(f"  T\_obs={T\_obs:.3f} | H0 median={med:.3f} \[5–95%: {lo:.3f}..{hi:.3f}\] | MC p≥obs={p\_ge:.12f}")  
def section\_cluster\_LOO():  
    print("\\n=== Cluster PB — leave-one-dataset-out (primary endpoint z=2.24, TINY) \===")  
    ob, pb, \_ \= cluster\_pb\_tail(BLOCKS, 2.24, 0.5, 0.5, MC\_SIMS\_PRIMARY, TINY\_DYADICS)  
    print(f"  (all datasets): clusters={len(DATASET\_NAMES)} | obs={ob} | PB tail={fmt\_tail(pb, MC\_SIMS\_PRIMARY)}")  
    for ds in DATASET\_NAMES:  
        sub \= \[b for b in BLOCKS if b.dataset \!= ds\]  
        ob, pb, \_ \= cluster\_pb\_tail(sub, 2.24, 0.5, 0.5, MC\_SIMS\_PRIMARY, TINY\_DYADICS)  
        print(f"     (drop {ds}): clusters={len(set(b.dataset for b in sub))} | obs={ob} | PB tail={fmt\_tail(pb, MC\_SIMS\_PRIMARY)}")  
def section\_jackknife():  
    print("\\n=== Jackknife-over-blocks (z=2.24, TINY) \===")  
    ob, pb, \_ \= mc\_pb\_tail(BLOCKS, 2.24, 0.5, 0.5, MC\_SIMS\_PRIMARY, TINY\_DYADICS)  
    print(f"  Full-set: obs={ob} | PB tail={fmt\_tail(pb, MC\_SIMS\_PRIMARY)}")  
    for i,b in enumerate(BLOCKS):  
        sub \= BLOCKS\[:i\] \+ BLOCKS\[i+1:\]  
        ob, pb, \_ \= mc\_pb\_tail(sub, 2.24, 0.5, 0.5, MC\_SIMS\_PRIMARY, TINY\_DYADICS)  
        print(f"   \- drop \[{b.dataset} | {b.tag}\] → obs={ob} | PB tail={fmt\_tail(pb, MC\_SIMS\_PRIMARY)}")

\# \------------------------ Self-tests (harmless invariants) \------------------------  
assert bitlen\_nonneg(0)==1 and bitlen\_nonneg(1)==0 and bitlen\_nonneg(2)==2  
assert mdl\_star(Fraction(1,2)) \<= mdl\_star(Fraction(1,3))  \# dyadic no more complex than 1/3 under bitlen() MDL\*  
\# Wilson symmetry sanity near 0.5  
\_a,\_b \= wilson\_ci(50,100,1.96); assert 0 \< \_a \< 0.5 \< \_b \< 1  
\# Nearest-fraction tie-break by MDL\*  
frs\_\_ \= \[Fraction(1,7), Fraction(1,8)\]  
best\_\_,\_ \= nearest\_fraction(0.13, frs\_\_); assert best\_\_ \== Fraction(1,8)  
\# Spike weights normalized  
lw\_\_, pl\_\_ \= prepare\_spike\_weights(ALL\_DYADICS); assert np.isclose(np.exp(lw\_\_).sum(),1.0)

\# \------------------------ RUN \------------------------  
\# Ledger merge  
registry, file\_hashes \= working\_ledger()  
wksize \= len(registry)  
reg\_hash \= sha256\_of\_ledger(registry)

for p in LEDGER\_PATHS:  
    h \= file\_hashes.get(p)  
    if h:  
        print(f"\[ledger\] found: {p}  SHA256={h}")  
print(f"\[ledger\] working set size: {wksize} (fallback \+ file merges)")  
print(f"\[preregistered-ledger-hash\] {reg\_hash}")

print(f"\\n\[note\] v13.13.9-onecell-megacell++++ starting…\\n")

\# Candidates  
section\_candidates(registry)

\# A/B around 1/2  
section\_ab\_sign()

\# PB tails (blocks & clusters) over prereg family, with MC floors  
section\_pb(BLOCKS)

\# Family-wise error (MC)  
familywise\_mc(BLOCKS, sims=MC\_SIMS\_FWER)

\# Nearest-dyadic Z distances  
section\_nearest\_dyadic\_z()

\# Bayes Factors (spike & spike+slab), stable logs  
section\_bayes()

\# Predictive Bayes (hold-out by dataset)  
section\_predictive\_holdout()

\# E-values / combinations for primary endpoint  
section\_evalues()

\# Exploratory T-stat  
section\_min\_T()

\# Cluster LOO  
section\_cluster\_LOO()

\# Jackknife over blocks  
section\_jackknife()

print(f"\\n\[repro\] seed={SEED}  code\_fingerprint={code\_fingerprint()}")  
print("✅ DONE — megacell ran all sections with robust MDL\* (zero-safe), strict ledger filter, MC floors, and full feature set.")

\# \=== Artifact bundle (post-run) \===  
OUTDIR \= ensure\_outdir("results")  
export\_config(OUTDIR, reg\_hash, file\_hashes)  
dump\_environment(OUTDIR)  
dump\_registry(OUTDIR, registry, file\_hashes, reg\_hash)  
export\_candidates(OUTDIR, registry)  
export\_nearest\_dyadic\_z(OUTDIR)  
export\_bayes(OUTDIR)  
export\_predictive\_holdout(OUTDIR)  
export\_pb\_tails(OUTDIR)  
export\_fwer(OUTDIR)  
export\_evalues(OUTDIR)  
export\_cluster\_loo(OUTDIR)  
export\_jackknife(OUTDIR)  
snapshot\_code(OUTDIR)  
write\_readme(OUTDIR)  
print("\\n\[artifacts\] written to: " \+ OUTDIR)
