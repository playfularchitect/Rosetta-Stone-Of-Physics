# UR-Law: The Universal Ledger (Ω3)

This folder contains the complete computational workflow and output for my **Universal Ledger (UR-Law)** framework.

The core hypothesis is that the fundamental constants of the Standard Model can be derived as **exact integer fractions** from a basis set of `{49, 50, 137}`.

This isn't just a "what-if" theory. This is a complete, auditable, computational pipeline. The log files here show the entire process: I prove the knowns, generate predictions for the unknowns, and then—most importantly—I prove that my results are **rigorous, non-trivial, and statistically significant.**

---

## The Core Concept

The model works by describing a target fraction `p/q` (the physical constant) using a "two-rail" method. Each "rail" (`P1` and `P2`) shows that the numerator (`p`) and denominator (`q`) are simple integer multiples (`A`, `B`) of my core moduli (`49` or `50`) plus a very small integer "offset" (`e`).

The entire framework is built on finding these simple, low-complexity (`E`) constructions.

---

## My Workflow: From Proof to Prediction

The output log file (e.g., `Mindmelt To (UU) Output`) in this repo is not just a data dump; it's a *story*. It's designed to be read from top to bottom. It shows the logical flow from proof to prediction to verification.

Here are the main phases:

### Phase 1: Proof & Audit (Modules Ω3, Ω4)
* **What I'm doing:** First, I prove that my model works. I take the 19 known constants of the Standard Model and show that every single one (`19/19`) can be successfully and simply constructed by the UR-Law.
* **What it shows:** The `UNIVERSAL LEDGER (Ω3)` proves the 19 constants. The `STRICT AUDIT (Ω4)` "canonizes" them, confirming they all pass a single, strict `E≤8` complexity cap.

### Phase 2: Packaging & Analysis (Modules Ω5, Ω6)
* **What I'm doing:** I package the 19 proven constants into a machine-readable CSV (`Ω5`) and run a Minimum Description Length (MDL) analysis (`Ω6`).
* **What it shows:** The "Codebook MDL" (`Ω6`) shows how the model is designed to be efficient by re-using components, creating a dictionary of `rail_id`s that makes the system scalable.

### Phase 3: Prediction Generation (Modules Ω-PRED, Ω-ΩMEGA)
* **What I'm doing:** Now that I've proven the law on the "training set," I use the *exact same rules* to generate new, out-of-sample predictions for unknown values.
* **What it shows:** `Ω-PRED` generates the *simplest* deterministic predictions. `Ω-ΩMEGA` is a "mass forge" that generates a Top-10 list of candidates for 12+ new parameters.

### Phase 4: Non-Triviality & Guarding (Modules Ω-ΩMEGAb2, Ω-NUKE)
* **What I'm doing:** This is where the *real* work begins. I'm not interested in "trivial" or "easy" answers. I run new forges with "anti-trivial" guards.
* **What it shows:** `Ω-ΩMEGAb2` (Strong Mode) bans all the "training" rails. `Ω-NUKE v2` (Ultra-Strict) bans simple patterns like `A=0` or non-diverse offsets. This ensures the predictions I find are robust and non-obvious.

### Phase 5: Registration & Time-Locking (Module Ω-REG)
* **What I'm doing:** I take my best prediction set (`Ω-NUKE`) and generate a `DATASET SHA256` hash for it.
* **What it shows:** The `NOTARIZATION FORM` provides a public, verifiable, time-locked "receipt." This proves I generated these exact predictions on a specific date, *before* any new experimental data could confirm them.

### Phase 6: The Gauntlet (Statistical Null Tests)
* **What I'm doing:** This is the most important part. I run a gauntlet of "null" experiments to prove my predictions aren't just random chance. I generate *millions* of random fractions *using the same strict guards* and see if any of them *ever* accidentally "hit" one of my predictions.
* **What it shows:**
    * `Ω-NULL` (and `b`, `c`): I run tests with 20k, 200k, and **1,000,000** draws. **Zero hits.**
    * `Ω-TORTURE`: I invent *even stricter* "torture" guards, generate 12 new predictions, and *still* get **zero hits** on a 200k-draw null test.
    * `Ω-IRONCLAD`: My most stringent test. It generates 24 predictions under "ironclad" guards (e.g., `49/50-only`, `id-only`, `E≤4`, `pairwise-coprime denominators`).
    * `Ω-MONTE10M`: The final boss. A **10,000,000-draw** (ten million) Monte Carlo null test on my "IRONCLAD" predictions.

---

##  Key Results & Highlights

1.  **✅ 100% Success on Knowns:** All **19/19** Standard Model parameters were successfully proven and "canonized" by the UR-Law (`E≤8`).
2.  ** Robust Predictions:** I've generated multiple sets of *new, exact-fraction predictions* (for PMNS, Higgs, etc.) that survive ultra-strict "NUKE" and "IRONCLAD" anti-trivial guards.
3.  ** Statistically Invincible:** My "IRONCLAD" predictions were tested against a **10,000,000-draw** (ten million) Monte Carlo null experiment.
    * **Result: 0 hits.**
    * This means the probability of my predictions being a random-chance artifact is **p ≤ 2.99e-07** (less than 1-in-3.3-million).
4.  ** Moduli-Specific:** The `Ω-SWAP` "adversarial" test proves the results are *specific* to the `{49, 50}` moduli. Swapping them for `{48, 51}` (or other pairs) breaks the model and produces **zero** hits.

##  The Predictions (Where to Find Them)

You're probably here for the predictions. Look in the `Mindmelt To (UU) Output` file for these modules.

* **`Ω-NUKE v2`**: Contains a robust, "anti-trivial" prediction set.
* **`Ω-IRONCLAD`**: Contains the most stringently-generated 24 predictions.

Both sets are time-locked with a `DATASET SHA256` hash, which is my formal, notarized claim.
