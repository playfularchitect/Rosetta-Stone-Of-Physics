
# =================================================================================================
# MODULE Ω-SKELETON v0R (READABILITY-FIRST) — Universal Planck Table Engine per "UPT-Blueprint-Plus v2"
# Vivi The Physics Slayer! × Evan — append-only, LEGO-style
# -------------------------------------------------------------------------------------------------
# Goals for v0R (R = Readable):
# • Same public API as v0 PLUS, now with ultra-readable stdout sections + AI-friendly markers.
# • Deterministic banners, fixed-width ASCII tables, JSON snapshots with BEGIN/END tags.
# • Meta integrity: per-row hashes, registry digest, s_D snapshot lines.
# • Emits CSV/JSONL/MD/ASCII files, and prints structured logs designed for humans & parsers.
# • Demo inserts 3 toy rows + 1 constraint row.
# -------------------------------------------------------------------------------------------------
# LEGEND for stdout:
#   [BEGIN SECTION:<name>] ... [END SECTION:<name>]          → machine-scan blocks
#   [KV] key = value                                         → machine-parsable key-values
#   [JSON:<label>] { ... }                                   → inline JSON blobs
#   Tables: single header line + dashes + rows (fixed-width)
# -------------------------------------------------------------------------------------------------

from __future__ import annotations
import csv, json, hashlib, os, time, math, sys
from decimal import Decimal, getcontext, ROUND_HALF_UP
from fractions import Fraction
from typing import Dict, Any, List, Tuple

# ----------------------[ Precision / Globals ]----------------------
getcontext().prec = 120

BANNER = "=" * 108
SUB    = "-" * 108

# Artifact paths
OUT_DIR       = "/content"
REPORTS_DIR   = os.path.join(OUT_DIR, "reports")
MASTER_CSV    = os.path.join(OUT_DIR, "UPT_master.csv")
MASTER_JSONL  = os.path.join(OUT_DIR, "UPT_master.jsonl")
TS            = int(time.time())
REPORT_MD     = os.path.join(REPORTS_DIR, f"UPT_report_{TS}.md")
REPORT_ASCII  = os.path.join(REPORTS_DIR, f"UPT_ascii_{TS}.txt")
VERSION       = f"Ω-skeleton-v0R@{TS}"

# In-memory registry
REGISTRY: List[Dict[str, Any]] = []

# ----------------------[ Domain Multipliers Catalog s_D ]----------------------
S_DOMAIN: Dict[str, Fraction] = {
    "EM":   Fraction(1, 1),
    "G":    Fraction(1, 1),
    "QCD":  Fraction(1, 1),
    "COS":  Fraction(1, 1),
    "INFO": Fraction(1, 1),
    "CUSTOM": Fraction(1, 1),
}

# ----------------------[ Universal Units ]----------------------
def U_EM(p_index: int) -> Decimal:
    """U_EM(p) = 1 / (49 * 50 * 137^p)."""
    base = Decimal(49) * Decimal(50) * (Decimal(137) ** int(p_index))
    return Decimal(1) / base

def U_domain(family: str, p_index: int, s_domain: Fraction | None = None) -> Decimal:
    if s_domain is None:
        s_domain = S_DOMAIN.get(family, Fraction(1,1))
    scale = Decimal(s_domain.numerator) / Decimal(s_domain.denominator)
    return scale * U_EM(p_index)

# ----------------------[ MDL bit counts ]----------------------
def UPT_bits_int(n: int) -> int:
    if n == 0:
        return 1
    return int(abs(n)).bit_length() + 1  # include sign bit

def UPT_bits_pq(p: int, q: int) -> int:
    return UPT_bits_int(p) + UPT_bits_int(q) + 2

# ----------------------[ DNA fingerprinting ]----------------------
MODS_DEFAULT = (23, 49, 50, 137)

def _residues(n: int, mods: Tuple[int, ...]) -> Dict[str, int]:
    return {str(m): (n % m) for m in mods}

def UPT_fingerprint(target: str = 'k',
                    mods: Tuple[int, int, int, int] = MODS_DEFAULT,
                    multi: bool = False,
                    *,
                    k_value: int | None = None,
                    pq_num: int | None = None,
                    pq_den: int | None = None) -> Dict[str, Any]:
    out: Dict[str, Any] = {}
    if target not in ('k', 'pq_num', 'pq_den', 'pq_sum', 'multi'):
        raise ValueError("target must be one of {'k','pq_num','pq_den','pq_sum','multi'}")
    if target == 'k' or multi:
        if k_value is None:
            raise ValueError("Provide k_value for k residues.")
        out['dna_k_mods'] = _residues(int(k_value), mods)
    if target in ('pq_num','pq_sum','multi'):
        if pq_num is None:
            raise ValueError("Provide pq_num for pq residues.")
        out.setdefault('dna_pq_mods', {})
        out['dna_pq_mods']['num'] = _residues(int(pq_num), mods)
    if target in ('pq_den','pq_sum','multi'):
        if pq_den is None:
            raise ValueError("Provide pq_den for pq residues.")
        out.setdefault('dna_pq_mods', {})
        out['dna_pq_mods']['den'] = _residues(int(pq_den), mods)
    if target in ('pq_sum','multi'):
        psum = int((pq_num or 0) + (pq_den or 0))
        out.setdefault('dna_pq_mods', {})
        out['dna_pq_mods']['sum'] = _residues(psum, mods)
    return out

# ----------------------[ Quantization ]----------------------
def _round_nearest_up(x: Decimal) -> int:
    return int(x.to_integral_value(rounding=ROUND_HALF_UP))

def UPT_quantize(value_dimless: Decimal,
                 U_family: str,
                 p_index: int,
                 s_domain: Fraction | None = None) -> Dict[str, Any]:
    U = U_domain(U_family, p_index, s_domain)
    if U == 0:
        raise ZeroDivisionError("Unit U evaluated to zero.")
    k = _round_nearest_up(value_dimless / U)
    approx = Decimal(k) * U
    residual = value_dimless - approx
    rel_error = (abs(residual) / (abs(value_dimless) if value_dimless != 0 else Decimal(1)))
    return {"U": U, "k": k, "approx": approx, "residual": residual, "rel_error": rel_error}

# ----------------------[ Schema / I/O ]----------------------
CSV_COLUMNS = [
    "uid","domain","tier","name","symbol","definition",
    "value_si","unit_si","value_dimless","norm_ref",
    "pq_num","pq_den","pq_value",
    "U_family","U_scale","p_index","k","U_value",
    "residual","rel_error",
    "bits_pq","bits_k",
    "dna_mod_23","dna_mod_49","dna_mod_50","dna_mod_137","dna_strategy",
    "dna_k_mods","dna_pq_mods",
    "provenance","status","calibration_target","notes",
    "hash_core","hash_entry","version"
]

def _hash_core_subset(row: Dict[str, Any]) -> str:
    core_fields = {
        "domain": row.get("domain",""),
        "symbol": row.get("symbol",""),
        "value_dimless": row.get("value_dimless",""),
        "pq_num": row.get("pq_num",""),
        "pq_den": row.get("pq_den",""),
    }
    payload = json.dumps(core_fields, sort_keys=True, default=str)
    return hashlib.sha256(payload.encode("utf-8")).hexdigest()

def _hash_full_row(row: Dict[str, Any]) -> str:
    payload = json.dumps({k: row.get(k, "") for k in CSV_COLUMNS if k != "hash_entry"}, sort_keys=True, default=str)
    return hashlib.sha256(payload.encode("utf-8")).hexdigest()

def UPT_add_entry(**kwargs) -> Dict[str, Any]:
    row = {k: kwargs.get(k, "") for k in CSV_COLUMNS if k not in ("hash_core","hash_entry")}
    try: p = int(row.get("pq_num") or 0)
    except Exception: p = 0
    try: q = int(row.get("pq_den") or 1)
    except Exception: q = 1
    bits_pq = UPT_bits_pq(p, q)
    try: k_val = int(row.get("k") or 0)
    except Exception: k_val = 0
    bits_k = UPT_bits_int(k_val)

    dna_simple = {
        "dna_mod_23": (k_val % 23) if isinstance(k_val, int) else "",
        "dna_mod_49": (k_val % 49) if isinstance(k_val, int) else "",
        "dna_mod_50": (k_val % 50) if isinstance(k_val, int) else "",
        "dna_mod_137": (k_val % 137) if isinstance(k_val, int) else "",
    }
    try:
        dna_rich = UPT_fingerprint(target='multi', multi=True, k_value=k_val, pq_num=p, pq_den=q)
    except Exception:
        dna_rich = {"dna_k_mods": {}, "dna_pq_mods": {}}

    row["pq_value"] = row.get("pq_value") or (str(Decimal(p)/Decimal(q)) if q else "")
    row["bits_pq"]  = bits_pq
    row["bits_k"]   = bits_k
    row.update(dna_simple)
    row["dna_strategy"] = row.get("dna_strategy") or "multi"
    row["dna_k_mods"] = json.dumps(dna_rich.get("dna_k_mods", {}))
    row["dna_pq_mods"] = json.dumps(dna_rich.get("dna_pq_mods", {}))
    row["U_value"]   = row.get("U_value") or row.get("U_numeric") or row.get("U") or ""

    row["hash_core"]  = _hash_core_subset(row)
    row["hash_entry"] = _hash_full_row(row)
    row["version"]    = row.get("version") or VERSION

    REGISTRY.append(row)
    return row

# ----------------------[ Pretty Printing / Logs ]----------------------

def kv(key: str, value: str):
    print(f"[KV] {key} = {value}")

def begin(name: str):
    print(f"[BEGIN SECTION:{name}]")

def end(name: str):
    print(f"[END SECTION:{name}]")

def json_blob(label: str, obj: Any):
    print(f"[JSON:{label}] " + json.dumps(obj, sort_keys=True, default=str))


def ascii_table(rows: List[Dict[str, Any]], headers: List[str]) -> str:
    widths = {h: max(len(h), *(len(str(r.get(h, ""))) for r in rows)) for h in headers}
    def fmt_row(r):
        return " | ".join(f"{str(r.get(h, '')):<{widths[h]}}" for h in headers)
    out = []
    out.append(" | ".join(f"{h:<{widths[h]}}" for h in headers))
    out.append("-+-".join("-"*widths[h] for h in headers))
    for r in rows:
        out.append(fmt_row(r))
    return "\n".join(out)

# ----------------------[ Emit ]----------------------

def UPT_emit(out_dir: str = OUT_DIR, ascii_rows_preview: int = 32) -> None:
    os.makedirs(out_dir, exist_ok=True)
    os.makedirs(REPORTS_DIR, exist_ok=True)

    # CSV
    with open(MASTER_CSV, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=CSV_COLUMNS)
        w.writeheader()
        for row in REGISTRY:
            w.writerow({k: row.get(k, "") for k in CSV_COLUMNS})

    # JSONL
    with open(MASTER_JSONL, "w") as f:
        for row in REGISTRY:
            f.write(json.dumps(row, default=str) + "\n")

    # Simple stats
    by_domain = {}
    worst = {"uid": None, "rel_error": Decimal(-1)}
    best  = {"uid": None, "rel_error": Decimal(1)}
    for r in REGISTRY:
        d = r.get("domain","?")
        by_domain[d] = by_domain.get(d, 0) + 1
        try:
            re = Decimal(str(r.get("rel_error") or "0"))
            if re > worst["rel_error"]:
                worst = {"uid": r.get("uid"), "rel_error": re}
            if re < best["rel_error"]:
                best = {"uid": r.get("uid"), "rel_error": re}
        except Exception:
            pass

    # ASCII preview
    preview_headers = ["group","name","p/q","approx","bits"]
    # Build easy preview rows from REGISTRY
    preview_rows: List[Dict[str, Any]] = []
    for r in REGISTRY:
        grp = r.get("domain","?")
        nm  = r.get("symbol") or r.get("name")
        pq  = f"{r.get('pq_num')}/{r.get('pq_den')}" if r.get('pq_den') not in ("", None, 0) else (r.get('value_dimless') or "")
        approx = r.get("pq_value") or r.get("value_dimless")
        bits  = r.get("bits_pq", "")
        preview_rows.append({"group": grp, "name": nm, "p/q": pq, "approx": approx, "bits": bits})

    # Write ASCII file
    ascii_txt = ascii_table(preview_rows, preview_headers)
    with open(REPORT_ASCII, "w") as f:
        f.write(ascii_txt + "\n")

    # MD report (compact)
    with open(REPORT_MD, "w") as f:
        f.write(f"# UPT Report — {time.ctime()}\n\n")
        f.write(f"- Version: `{VERSION}`\n")
        f.write(f"- Rows: {len(REGISTRY)}\n")
        f.write("- Counts by domain:\n")
        for d, n in sorted(by_domain.items()):
            f.write(f"  - {d}: {n}\n")
        f.write(f"- Worst rel_error: uid={worst['uid']} val={worst['rel_error']}\n")
        f.write(f"- Best  rel_error: uid={best['uid']}  val={best['rel_error']}\n")
        f.write("\n## Domain Multipliers s_D (snapshot)\n\n")
        for d, frac in S_DOMAIN.items():
            f.write(f"- {d}: {frac} ≈ {float(frac)}\n")
        reg_digest = hashlib.sha256("".join(sorted(r["hash_entry"] for r in REGISTRY)).encode()).hexdigest()
        f.write(f"\n- Registry digest: `{reg_digest}`\n")

    # ----------- STDOUT (structured) -----------
    print(BANNER)
    print(" UNIVERSAL PLANCK TABLE — EMIT (READABILITY-FIRST) ")
    print(BANNER)
    begin("ARTIFACTS")
    kv("CSV", MASTER_CSV)
    kv("JSONL", MASTER_JSONL)
    kv("REPORT_MD", REPORT_MD)
    kv("REPORT_ASCII", REPORT_ASCII)
    kv("VERSION", VERSION)
    end("ARTIFACTS")

    begin("COUNTS")
    for d, n in sorted(by_domain.items()):
        kv(f"domain[{d}]", n)
    end("COUNTS")

    begin("RESIDUALS")
    kv("worst.uid", worst['uid'])
    kv("worst.rel_error", str(worst['rel_error']))
    kv("best.uid",  best['uid'])
    kv("best.rel_error",  str(best['rel_error']))
    end("RESIDUALS")

    begin("s_D_SNAPSHOT")
    for d, frac in S_DOMAIN.items():
        kv(f"s_D[{d}]", f"{frac.numerator}/{frac.denominator}")
    end("s_D_SNAPSHOT")

    # JSON snapshots for AI parsers
    begin("JSON_SNAPSHOTS")
    json_blob("s_D", {d: {"num": frac.numerator, "den": frac.denominator, "float": float(frac)} for d, frac in S_DOMAIN.items()})
    reg_digest = hashlib.sha256("".join(sorted(r["hash_entry"] for r in REGISTRY)).encode()).hexdigest()
    json_blob("registry_digest", {"sha256": reg_digest, "rows": len(REGISTRY)})
    # First N rows (minimal fields)
    json_blob("registry_preview", [
        {"uid": r.get("uid"), "domain": r.get("domain"), "tier": r.get("tier"),
         "symbol": r.get("symbol"), "value_dimless": r.get("value_dimless"),
         "pq": [r.get("pq_num"), r.get("pq_den")], "k": r.get("k"),
         "U_family": r.get("U_family"), "p_index": r.get("p_index")}
        for r in REGISTRY[:16]
    ])
    end("JSON_SNAPSHOTS")

    # Human table preview
    print(SUB)
    print("REGISTRY initial (readable preview)")
    print(SUB)
    print(ascii_txt)
    print(SUB)
    print("Tip: use [JSON:registry_preview] above for programmatic ingestion.")
    print(BANNER)

# ----------------------[ Constraints & Calibration ]----------------------

def UPT_add_constraint(name: str,
                       target_value: Decimal,
                       measured_value: Decimal,
                       U_family: str,
                       p_index: int,
                       s_domain: Fraction | None = None,
                       **meta) -> Dict[str, Any]:
    deviation = measured_value - target_value
    qz = UPT_quantize(abs(deviation), U_family, p_index, s_domain)
    uid = meta.get("uid", f"UPT/Derived/7/{name}#{VERSION}")
    row = dict(
        uid=uid, domain="Derived", tier=meta.get("tier", 7),
        name=f"Constraint: {name}", symbol=meta.get("symbol","Δ"),
        definition=meta.get("definition","Deviation quantized on U_D(p)"),
        value_si="", unit_si="", value_dimless=str(measured_value),
        norm_ref=meta.get("norm_ref","custom"),
        pq_num=meta.get("pq_num",0), pq_den=meta.get("pq_den",1), pq_value="",
        U_family=U_family, U_scale=f"{S_DOMAIN.get(U_family, Fraction(1,1))} * U_EM(p)",
        p_index=p_index, k=qz["k"], U_value=str(qz["U"]),
        residual=str(qz["residual"]), rel_error=str(qz["rel_error"]),
        provenance=meta.get("provenance","UPT-skeleton-v0R"), status="derived",
        calibration_target=meta.get("calibration_target",""),
        notes=f"target={target_value}, measured={measured_value}, deviation={deviation}",
        version=VERSION
    )
    out = UPT_add_entry(**row)

    # Log a tiny readable block for the constraint
    begin("CONSTRAINT")
    kv("name", name)
    kv("target", str(target_value))
    kv("measured", str(measured_value))
    kv("deviation", str(deviation))
    kv("unit_family", U_family)
    kv("p_index", p_index)
    kv("k", out.get("k"))
    kv("U", out.get("U_value"))
    kv("residual", out.get("residual"))
    kv("rel_error", out.get("rel_error"))
    json_blob("constraint_row_uid", {"uid": out.get("uid")})
    end("CONSTRAINT")
    return out


def UPT_calibrate(domain: str,
                  anchors: List[Tuple[Decimal, str, int, int, str]],
                  max_den: int = 10000) -> Fraction:
    begin("CALIBRATE")
    kv("domain", domain)
    kv("anchors.count", len(anchors))
    kv("max_den", max_den)
    digest = hashlib.sha256(json.dumps(
        [(str(v), fam, p, int(k), lbl) for (v, fam, p, k, lbl) in anchors],
        sort_keys=True
    ).encode()).hexdigest()
    kv("anchors.digest", digest)
    print("[NOTE] v0R calibrator is a no-op (solver is in domain modules). Returning current s_D.")
    end("CALIBRATE")
    return S_DOMAIN.get(domain, Fraction(1,1))

# ----------------------[ Plot placeholders ]----------------------

def UPT_plot_residuals():
    begin("PLOT_PLACEHOLDER"); kv("plot", "residual_histogram"); end("PLOT_PLACEHOLDER")

def UPT_plot_dna_spectrum():
    begin("PLOT_PLACEHOLDER"); kv("plot", "dna_residue_spectrum"); end("PLOT_PLACEHOLDER")

# ----------------------[ Registry Init ]----------------------

def UPT_init_registry() -> None:
    global REGISTRY
    REGISTRY = []
    os.makedirs(OUT_DIR, exist_ok=True)
    os.makedirs(REPORTS_DIR, exist_ok=True)
    print(BANNER)
    print(" UNIVERSAL PLANCK TABLE — REGISTRY INIT (Blueprint v2+: hashes + Meta + DNA + Readable) ")
    print(BANNER)
    kv("design", "Single-lattice: X ≈ k · U_D(p), s_D rational")
    kv("exact_core", "p/q + integer k + DNA residues + MDL bits")
    kv("hashes", "hash_core + hash_entry")
    kv("mode", "append-only")
    print(SUB)

# =================================================================================================
#                                     DEMO / TOY INSERTS
# =================================================================================================
if __name__ == "__main__":
    UPT_init_registry()

    # META snapshot row
    meta_uid = f"UPT/Meta/8/sD_snapshot#{VERSION}"
    meta_row = dict(
        uid=meta_uid, domain="Meta", tier=8,
        name="Domain multipliers snapshot", symbol="s_D",
        definition="Snapshot of s_D domain multipliers (rational).",
        value_si="", unit_si="",
        value_dimless=json.dumps({d: f"{frac.numerator}/{frac.denominator}" for d, frac in S_DOMAIN.items()}),
        norm_ref="n/a",
        pq_num=0, pq_den=1, pq_value="",
        U_family="CUSTOM", U_scale="n/a", p_index=0, k=0, U_value="",
        residual="0", rel_error="0",
        bits_pq=0, bits_k=0,
        dna_mod_23=0, dna_mod_49=0, dna_mod_50=0, dna_mod_137=0, dna_strategy="k",
        dna_k_mods=json.dumps({}), dna_pq_mods=json.dumps({}),
        provenance="UPT v2+ skeleton v0R", status="measured", calibration_target="",
        notes="Initial skeleton snapshot", version=VERSION
    )
    UPT_add_entry(**meta_row)

    # TOY 1: α (toy p/q)
    alpha_p, alpha_q = 1, 137
    alpha_val  = Decimal(alpha_p) / Decimal(alpha_q)
    p_idx_alpha   = 1
    qz_alpha      = UPT_quantize(alpha_val, "EM", p_idx_alpha, S_DOMAIN["EM"])
    uid_alpha     = f"UPT/EM/1/alpha#{VERSION}"
    row_alpha = dict(
        uid=uid_alpha, domain="EM", tier=1,
        name="fine-structure constant (toy)", symbol="α",
        definition="toy seed; exact p/q=1/137",
        value_si="", unit_si="",
        value_dimless=f"{alpha_p}/{alpha_q}", norm_ref="natural(ħ=c=k_B=1)",
        pq_num=alpha_p, pq_den=alpha_q, pq_value=str(alpha_val),
        U_family="EM", U_scale="U_EM(p)",
        p_index=p_idx_alpha, k=qz_alpha["k"], U_value=str(qz_alpha["U"]),
        residual=str(qz_alpha["residual"]), rel_error=str(qz_alpha["rel_error"]),
        provenance="UPT v0R", status="measured",
        calibration_target="alpha@toy", notes="plumbing proof",
        version=VERSION
    )
    UPT_add_entry(**row_alpha)

    # TOY 2: Gravity area-per-bit over l_P^2 (toy rational near 4 ln 2)
    Abit_num, Abit_den = 2772589, 10**6
    Abit_val = Decimal(Abit_num) / Decimal(Abit_den)
    p_idx_grav = 6
    qz_grav = UPT_quantize(Abit_val, "G", p_idx_grav, S_DOMAIN["G"])
    uid_grav = f"UPT/Gravity/4/Abit_over_lP2#{VERSION}"
    row_grav = dict(
        uid=uid_grav, domain="Gravity", tier=4,
        name="area-per-bit over ℓ_P^2 (toy)", symbol="A_bit/ℓ_P^2",
        definition="toy rational near 4 ln 2",
        value_si="", unit_si="",
        value_dimless=f"{Abit_num}/{Abit_den}", norm_ref="Planck",
        pq_num=Abit_num, pq_den=Abit_den, pq_value=str(Abit_val),
        U_family="G", U_scale="s_G * U_EM(p)",
        p_index=p_idx_grav, k=qz_grav["k"], U_value=str(qz_grav["U"]),
        residual=str(qz_grav["residual"]), rel_error=str(qz_grav["rel_error"]),
        provenance="UPT v0R", status="calibration",
        calibration_target="A_bit@Planck", notes="will refine in G module",
        version=VERSION
    )
    UPT_add_entry(**row_grav)

    # TOY 3: Cosmology Ω_b (toy p/q)
    Om_p, Om_q = 12, 250
    Om_val     = Decimal(Om_p) / Decimal(Om_q)
    p_idx_cos  = 2
    qz_cos     = UPT_quantize(Om_val, "COS", p_idx_cos, S_DOMAIN["COS"])
    uid_cos    = f"UPT/Cosmology/6/Omega_b#{VERSION}"
    row_cos = dict(
        uid=uid_cos, domain="Cosmology", tier=6,
        name="baryon density fraction (toy)", symbol="Ω_b",
        definition="toy fraction; real module imports official value & Planck norm",
        value_si="", unit_si="",
        value_dimless=f"{Om_p}/{Om_q}", norm_ref="critical-density",
        pq_num=Om_p, pq_den=Om_q, pq_value=str(Om_val),
        U_family="COS", U_scale="s_COS * U_EM(p)",
        p_index=p_idx_cos, k=qz_cos["k"], U_value=str(qz_cos["U"]),
        residual=str(qz_cos["residual"]), rel_error=str(qz_cos["rel_error"]),
        provenance="UPT v0R", status="measured",
        calibration_target="Omega_b@toy", notes="plumbing proof",
        version=VERSION
    )
    UPT_add_entry(**row_cos)

    # TOY CONSTRAINT: Cosmic closure (Ω_tot − 1 → 0)
    measured_tot = Decimal(1)
    UPT_add_constraint(
        name="cosmic_closure",
        target_value=Decimal(1),
        measured_value=measured_tot,
        U_family="COS",
        p_index=0,
        symbol="Ω_tot−1",
        tier=7,
        calibration_target="closure@toy",
        provenance="UPT v0R",
        definition="Cosmic closure: Ω_tot−1 quantized on U_COS(p)"
    )

    # -------- Derived view examples (tiny): alpha_inverse --------
    begin("DERIVED")
    alpha_inv = Decimal(alpha_q) / Decimal(alpha_p)
    kv("alpha_inverse", str(alpha_inv))
    json_blob("alpha_inverse", {"p": alpha_q, "q": alpha_p, "value": str(alpha_inv)})
    end("DERIVED")

    # EMIT artifacts + previews
    UPT_emit()

    # Final takeaway
    print(BANNER)
    print(" ONE-LINE TAKEAWAY ")
    print(BANNER)
    print("Readable stdout + JSON snapshots + hashes + ASCII tables; CSV/JSONL/MD written. Ready for domain modules.")
    print(BANNER)

# =============================================================================
# MODULE: UPT_EM_v1 — Real Electromagnetism/Electroweak on Universal Lattice
# Vivi The Physics Slayer! × Evan — append-only LEGO block (paste-and-run)
# =============================================================================
# What this module does
# - Defines the universal unit family U_EM(p) = 1 / (49*50*137^p), p ∈ [0..64]
# - Seeds *standard* reference values internally (no user inputs required):
#     α^-1 ≈ 137.035999084 (CODATA 2022),  sin^2θ_W(M_Z) ≈ 0.23122 (MS-bar),
#     M_W = 80.379 GeV, M_Z = 91.1876 GeV,  v ≈ 246.21965 GeV
# - Normalizes to dimensionless where needed (e.g., M_W/v, M_Z/v)
# - Quantizes each X onto the lattice: choose p (scan 0..64), compute k=round(X/U), residuals
# - Prints clean, parse-friendly logs + neat ASCII tables
# - Writes artifacts: CSV, JSONL, MD summary, ASCII snapshot in /content
# - Adds a constraint row: custodial ρ (tree) ≡ M_W^2/(M_Z^2 cos^2θ_W) → 1 (report deviation)
# - Computes DNA residues (mods={23,49,50,137}), MDL bits, SHA-256 hashes
# - ZERO external dependencies beyond the Python stdlib
# =============================================================================

from decimal import Decimal, getcontext
from fractions import Fraction
import hashlib, json, csv, os, math, time, datetime as dt
from typing import Dict, Any, Tuple, List

# -----------------------------------------------------------------------------
# High precision context
# -----------------------------------------------------------------------------
getcontext().prec = 120

# -----------------------------------------------------------------------------
# Helpers: banners & structured logging
# -----------------------------------------------------------------------------
def banner(title: str):
    line = "=" * 92
    print(f"\n{line}\n{title}\n{line}")

def section_begin(name: str):
    print(f"[BEGIN SECTION:{name}]")

def section_end(name: str):
    print(f"[END SECTION:{name}]")

def kv(k: str, v: Any):
    print(f"[KV] {k} = {v}")

def json_blob(label: str, obj: Any):
    print(f"[JSON:{label}] {json.dumps(obj, ensure_ascii=False)}")

def fixed_table(headers: List[str], rows: List[List[str]], widths: List[int]):
    # Simple fixed-width ASCII table
    def fmt_row(r):
        return "  ".join(str(v)[:w].ljust(w) for v, w in zip(r, widths))
    print(fmt_row(headers))
    print("-" * (sum(widths) + 2 * (len(widths) - 1)))
    for r in rows:
        print(fmt_row(r))

# -----------------------------------------------------------------------------
# Universal Unit Family: U_EM(p) = 1 / (49 * 50 * 137^p)
# -----------------------------------------------------------------------------
def U_EM(p: int) -> Decimal:
    if p < 0:
        raise ValueError("p must be >= 0")
    base = Decimal(49) * Decimal(50) * (Decimal(137) ** Decimal(p))
    return Decimal(1) / base

# -----------------------------------------------------------------------------
# MDL bits & DNA residues
# -----------------------------------------------------------------------------
def bits_int(n: int) -> int:
    n = abs(int(n))
    return 1 if n == 0 else n.bit_length()

def bits_pq(p: int, q: int) -> int:
    return bits_int(p) + bits_int(q)

def dna_residues_on_int(k: int, mods=(23,49,50,137)) -> Dict[str, int]:
    return {str(m): int(k % m) for m in mods}

# -----------------------------------------------------------------------------
# Hash helpers (stable field order)
# -----------------------------------------------------------------------------
def sha256_of_string(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def row_hash_core(domain: str, symbol: str, value_dimless: str, pq_num: int, pq_den: int) -> str:
    s = json.dumps(
        {"domain": domain, "symbol": symbol, "value_dimless": value_dimless,
         "pq_num": pq_num, "pq_den": pq_den}, separators=(",", ":"), ensure_ascii=False)
    return sha256_of_string(s)

def row_hash_entry(row: Dict[str, Any]) -> str:
    # Stable order by sorting keys
    s = json.dumps({k: row[k] for k in sorted(row.keys())},
                   separators=(",", ":"), ensure_ascii=False)
    return sha256_of_string(s)

# -----------------------------------------------------------------------------
# Quantization engine
# -----------------------------------------------------------------------------
def quantize_on_U(value: Decimal, family: str, p_scan=(0,64), s_domain: Fraction = Fraction(1,1)) -> Dict[str, Any]:
    """
    family: currently only "EM".
    s_domain: rational scale s_D (dimensionless); default 1/1 for EM
    """
    if family != "EM":
        raise NotImplementedError("Only EM family supported in this module.")
    sD = Decimal(s_domain.numerator) / Decimal(s_domain.denominator)
    best = None
    p_best, k_best, res_best, rel_best, U_best = None, None, None, None, None
    p0, p1 = p_scan
    for p in range(p0, p1+1):
        U = sD * U_EM(p)
        if U == 0:
            continue
        k = int((value / U).to_integral_value(rounding=getcontext().rounding))  # quantize via nearest
        approx = Decimal(k) * U
        res = value - approx
        rel = (abs(res) / abs(value)) if value != 0 else Decimal(0)
        if best is None or rel < rel_best:
            best = True
            p_best, k_best, res_best, rel_best, U_best = p, k, res, rel, U
    return dict(p=p_best, k=int(k_best), U=str(U_best), residual=str(res_best), rel_error=str(rel_best))

# -----------------------------------------------------------------------------
# Artifact writers
# -----------------------------------------------------------------------------
OUTDIR = "/content"
os.makedirs(OUTDIR, exist_ok=True)
os.makedirs(os.path.join(OUTDIR, "reports"), exist_ok=True)

RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

MODULE_VERSION = "UPT_EM_v1.0.0"

CSV_PATH = os.path.join(OUTDIR, f"UPT_master_EM_{RUN_TS}.csv")
JSONL_PATH = os.path.join(OUTDIR, f"UPT_master_EM_{RUN_TS}.jsonl")
MD_PATH = os.path.join(OUTDIR, "reports", f"UPT_report_EM_{RUN_TS}.md")
ASCII_PATH = os.path.join(OUTDIR, "reports", f"UPT_ascii_EM_{RUN_TS}.txt")

MASTER_ROWS: List[Dict[str, Any]] = []

def write_csv(rows: List[Dict[str, Any]], path: str):
    if not rows:
        return
    # Stable field order
    fields = list(sorted(rows[0].keys()))
    with open(path, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fields)
        w.writeheader()
        for r in rows:
            w.writerow({k: r.get(k, "") for k in fields})

def write_jsonl(rows: List[Dict[str, Any]], path: str):
    with open(path, "w") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

def write_ascii_report(rows: List[Dict[str, Any]], path: str):
    with open(path, "w") as f:
        f.write("UPT — Electromagnetism/Electroweak (EM) Snapshot\n")
        f.write("="*72 + "\n")
        cols = ["uid","domain","tier","symbol","value_dimless","U_family","p_index","k","rel_error"]
        widths = [38,6,4,12,18,7,7,12,18]
        def fmt_row(r):
            vals = [str(r.get(c,"")) for c in cols]
            return "  ".join(v[:w].ljust(w) for v,w in zip(vals,widths))
        f.write(fmt_row({c:c for c in cols}) + "\n")
        f.write("-"*sum(widths) + "\n")
        for r in rows:
            f.write(fmt_row(r) + "\n")

def write_md_summary(rows: List[Dict[str, Any]], path: str, extra: str = ""):
    with open(path, "w") as f:
        f.write(f"# UPT — EM/EW on Universal Lattice\n")
        f.write(f"- Module: {MODULE_VERSION}\n")
        f.write(f"- Timestamp (UTC): {RUN_TS}\n")
        f.write(f"- Rows: {len(rows)}\n\n")
        if extra:
            f.write(extra + "\n")

# -----------------------------------------------------------------------------
# Registry row builder
# -----------------------------------------------------------------------------
def add_row(**kwargs):
    row = dict(**kwargs)
    # fill hashes last
    core = row_hash_core(row["domain"], row["symbol"], row["value_dimless"], int(row["pq_num"]), int(row["pq_den"]))
    row["hash_core"] = core
    row["hash_entry"] = row_hash_entry(row)
    MASTER_ROWS.append(row)
    return row

# -----------------------------------------------------------------------------
# Reference values (no user inputs)
# -----------------------------------------------------------------------------
# CODATA 2022: alpha^-1 = 137.035999084(21)
alpha_inv = Decimal("137.035999084")
alpha = Decimal(1) / alpha_inv                      # ≈ 0.0072973525628...

# MS-bar sin^2θ_W(M_Z) ~ 0.23122 (illustrative standard value)
s2W = Decimal("0.23122")
c2W = Decimal(1) - s2W

# PDG-like masses (illustrative standards)
MW = Decimal("80.379")       # GeV
MZ = Decimal("91.1876")      # GeV
v  = Decimal("246.21965")    # GeV

MW_over_v = MW / v
MZ_over_v = MZ / v

# -----------------------------------------------------------------------------
# Quantize all targets on U_EM(p), p in [0..64]
# -----------------------------------------------------------------------------
banner("EM/EW QUANTIZATION ON UNIVERSAL LATTICE  U(p)=1/(49·50·137^p)")
section_begin("targets")

targets = [
    # name, symbol, domain, tier, value_dimless (string), fraction p/q (exact where meaningful)
    ("fine-structure constant", "alpha", "QED", 1, str(alpha), Fraction(2639, 361638)),  # small-denominator example used previously
    ("weak mixing (MSbar) s2W", "sin2_thetaW", "EW", 1, str(s2W), Fraction(23122, 100000)),
    ("W over v", "MW_over_v", "EW", 1, str(MW_over_v), Fraction(17807, 54547)),  # nice small denom example
    ("Z over v", "MZ_over_v", "EW", 1, str(MZ_over_v), Fraction(18749, 50625)),
    ("alpha_inverse", "alpha_inv", "Derived", 2, str(alpha_inv), Fraction(361638, 2639)),
]

mods = (23,49,50,137)
rows_for_table = []

for name, sym, domain, tier, val_str, frac in targets:
    X = Decimal(val_str)
    qres = quantize_on_U(X, family="EM", p_scan=(0,64), s_domain=Fraction(1,1))
    p_idx = int(qres["p"]); k = int(qres["k"])
    U_val = Decimal(qres["U"]); residual = Decimal(qres["residual"]); rel = Decimal(qres["rel_error"])
    dna_k = dna_residues_on_int(k, mods=mods)
    row = add_row(
        uid=f"UPT/{domain}/{tier}/{sym}#EM_v1",
        domain=domain,
        tier=tier,
        name=name,
        symbol=sym,
        definition="Quantized on U_EM(p) with s_EM=1.",
        value_si="", unit_si="",
        value_dimless=str(X),
        norm_ref="natural",
        pq_num=int(frac.numerator), pq_den=int(frac.denominator),
        pq_value=str(Decimal(frac.numerator)/Decimal(frac.denominator)),
        U_family="EM",
        U_scale="1 * U_EM(p)",
        p_index=p_idx,
        k=k,
        U_value=str(U_val),
        residual=str(residual),
        rel_error=str(rel),
        bits_pq=int(bits_pq(frac.numerator, frac.denominator)),
        bits_k=int(bits_int(k)),
        dna_mod_23=int(dna_k["23"]),
        dna_mod_49=int(dna_k["49"]),
        dna_mod_50=int(dna_k["50"]),
        dna_mod_137=int(dna_k["137"]),
        dna_strategy="k",
        dna_k_mods=json.dumps(dna_k, ensure_ascii=False),
        dna_pq_mods=json.dumps({}, ensure_ascii=False),
        provenance="UPT_EM_v1 builtin",
        status="measured",
        calibration_target="",
        notes="",
        version=MODULE_VERSION
    )
    rows_for_table.append([
        row["symbol"],
        f"{p_idx}",
        f"{k}",
        f"{Decimal(row['U_value']):.12E}",
        f"{Decimal(row['value_dimless']):.12E}",
        f"{Decimal(row['residual']):.12E}",
        f"{Decimal(row['rel_error']):.3E}"
    ])

headers = ["symbol","p","k","U(p)","X_dimless","residual","rel_error"]
widths  = [12,4,20,14,16,16,12]
fixed_table(headers, rows_for_table, widths)

section_end("targets")

# -----------------------------------------------------------------------------
# Constraint: Custodial ρ at tree level should be 1
# ρ_tree = MW^2 / ( MZ^2 * cos^2θ_W )
# -----------------------------------------------------------------------------
banner("CUSTODIAL ρ (TREE) — CONSTRAINT ROW")
section_begin("rho_tree_constraint")

rho_tree = (MW**2) / ( (MZ**2) * c2W )
dev_from_1 = rho_tree - Decimal(1)

kv("rho_tree", f"{rho_tree:.18f}")
kv("deviation_from_1", f"{dev_from_1:.3E}")

# Quantize the deviation on the same lattice (diagnostic)
q_dev = quantize_on_U(dev_from_1.copy_abs(), family="EM", p_scan=(0,64), s_domain=Fraction(1,1))
p_dev = int(q_dev["p"]); k_dev = int(q_dev["k"])
U_dev = Decimal(q_dev["U"]); res_dev = Decimal(q_dev["residual"]); rel_dev = Decimal(q_dev["rel_error"])

row_constraint = add_row(
    uid="UPT/Derived/3/rho_tree_dev#EM_v1",
    domain="Derived",
    tier=3,
    name="Custodial rho (tree) deviation from 1",
    symbol="rho_tree_dev",
    definition="rho_tree = MW^2 / (MZ^2 cos^2θW); deviation = rho_tree - 1",
    value_si="", unit_si="",
    value_dimless=str(dev_from_1),
    norm_ref="natural",
    pq_num=0, pq_den=1, pq_value="0",
    U_family="EM",
    U_scale="1 * U_EM(p)",
    p_index=p_dev,
    k=k_dev,
    U_value=str(U_dev),
    residual=str(res_dev),
    rel_error=str(rel_dev),
    bits_pq=int(bits_pq(0,1)),
    bits_k=int(bits_int(k_dev)),
    dna_mod_23=int(k_dev % 23),
    dna_mod_49=int(k_dev % 49),
    dna_mod_50=int(k_dev % 50),
    dna_mod_137=int(k_dev % 137),
    dna_strategy="k",
    dna_k_mods=json.dumps(dna_residues_on_int(k_dev), ensure_ascii=False),
    dna_pq_mods=json.dumps({}, ensure_ascii=False),
    provenance="UPT_EM_v1 builtin",
    status="derived",
    calibration_target="",
    notes="Tree-level check; nonzero due to input MS-bar sin^2θW vs on-shell ρ.",
    version=MODULE_VERSION
)

rows_for_constraint = [[
    f"{row_constraint['symbol']}",
    f"{row_constraint['p_index']}",
    f"{row_constraint['k']}",
    f"{Decimal(row_constraint['U_value']):.12E}",
    f"{Decimal(row_constraint['value_dimless']):.3E}",
    f"{Decimal(row_constraint['residual']):.3E}",
    f"{Decimal(row_constraint['rel_error']):.3E}",
]]
fixed_table(["symbol","p","k","U(p)","X_dimless","residual","rel_error"],
            rows_for_constraint, [12,4,20,14,16,16,12])

section_end("rho_tree_constraint")

# -----------------------------------------------------------------------------
# Emit artifacts
# -----------------------------------------------------------------------------
banner("EMIT ARTIFACTS")
section_begin("write_files")

write_csv(MASTER_ROWS, CSV_PATH)
write_jsonl(MASTER_ROWS, JSONL_PATH)
write_ascii_report(MASTER_ROWS, ASCII_PATH)

# MD summary (mini)
extra_md = []
extra_md.append("## Lattice Summary (EM/EW)\n")
extra_md.append("| symbol | p | k | U(p) | X_dimless | residual | rel_error |")
extra_md.append("|---|---:|---:|---:|---:|---:|---:|")
for r in MASTER_ROWS:
    extra_md.append(f"| {r['symbol']} | {r['p_index']} | {r['k']} | {Decimal(r['U_value']):.3E} | "
                    f"{Decimal(r['value_dimless']):.3E} | {Decimal(r['residual']):.3E} | {Decimal(r['rel_error']):.3E} |")
write_md_summary(MASTER_ROWS, MD_PATH, extra="\n".join(extra_md))

kv("CSV", CSV_PATH)
kv("JSONL", JSONL_PATH)
kv("MD", MD_PATH)
kv("ASCII", ASCII_PATH)

section_end("write_files")

# -----------------------------------------------------------------------------
# Final registry digest + compact on-screen ASCII snapshot
# -----------------------------------------------------------------------------
banner("REGISTRY DIGEST & SNAPSHOT")
section_begin("digest")

digest = sha256_of_string(json.dumps(sorted([r["hash_entry"] for r in MASTER_ROWS]), ensure_ascii=False))
kv("rows", len(MASTER_ROWS))
kv("registry_digest_sha256", digest)

# On-screen compact table
headers = ["uid","domain","tier","symbol","value_dimless","U_family","p_index","k","rel_error"]
widths  = [38,6,4,12,18,7,7,12,18]
rows = []
for r in MASTER_ROWS:
    rows.append([
        r["uid"], r["domain"], str(r["tier"]), r["symbol"],
        f"{Decimal(r['value_dimless']):.8E}", r["U_family"], str(r["p_index"]), str(r["k"]),
        f"{Decimal(r['rel_error']):.3E}"
    ])
fixed_table(headers, rows, widths)

section_end("digest")

banner("UPT_EM_v1 — COMPLETE")
print("NEXT: If outputs look good, we proceed to UPT_QCD_v1 or UPT_G_v1 as the next LEGO block.\n")
# =============================================================================
# END MODULE
# =============================================================================

# ====================== UPT_G_v1r1 — Gravity on the Universal Lattice (stdout-friendly) ======================
# Emits human+AI readable sections to stdout AND writes CSV/JSONL/MD/ASCII. No deprecation warnings.
# ============================================================================================================

import math, json, csv, hashlib, os, warnings
import datetime as dt
from fractions import Fraction

# ---- silence DeprecationWarnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# ---- run timestamp (timezone-aware; no deprecation)
RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

# ---- IO paths
OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)

CSV_PATH   = os.path.join(OUT_DIR,   f"UPT_master_G_{RUN_TS}.csv")
JSONL_PATH = os.path.join(OUT_DIR,   f"UPT_master_G_{RUN_TS}.jsonl")
MD_PATH    = os.path.join(REPORT_DIR,f"UPT_report_G_{RUN_TS}.md")
ASC_PATH   = os.path.join(REPORT_DIR,f"UPT_ascii_G_{RUN_TS}.txt")

# ============================================================================================================
# Universal lattice & helpers
# ============================================================================================================
def U_EM(p: int) -> Fraction:
    return Fraction(1, 49*50*137**p)

def U_scaled(p: int, s: Fraction) -> Fraction:
    return s * U_EM(p)

def bits_int(n: int) -> int:
    return 1 if n == 0 else n.bit_length()

def bits_pq(p: int, q: int) -> int:
    return bits_int(abs(p)) + bits_int(abs(q))

def sha256_str(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def to_float(fr: Fraction) -> float:
    return fr.numerator / fr.denominator

def fingerprint_k(k: int, mods=(23,49,50,137)):
    return {m: (k % m) for m in mods}

def row_hashes_core(domain, symbol, value_dimless, pq_num, pq_den):
    core = f"{domain}|{symbol}|{value_dimless}|{pq_num}|{pq_den}"
    return sha256_str(core)

def row_hash_full(row: dict):
    ordered_keys = [
        "uid","domain","tier","name","symbol","definition",
        "value_si","unit_si","value_dimless","norm_ref",
        "pq_num","pq_den","pq_value",
        "U_family","U_scale","p_index","k","U_value",
        "residual","rel_error","bits_pq","bits_k",
        "dna_mod_23","dna_mod_49","dna_mod_50","dna_mod_137",
        "dna_strategy","dna_k_mods","dna_pq_mods",
        "provenance","status","calibration_target",
        "notes","hash_core","version"
    ]
    payload = "|".join(str(row.get(k,"")) for k in ordered_keys)
    return sha256_str(payload)

def qtz(X_dimless: Fraction, U_family: str, p_index: int, s_domain: Fraction):
    U = U_scaled(p_index, s_domain)
    k_exact = X_dimless / U
    k = int(k_exact.numerator // k_exact.denominator)
    r = k_exact - k
    if r > Fraction(1,2):
        k += 1
    elif r == Fraction(1,2):
        if (k % 2) != 0: k += 1
    residual = X_dimless - k*U
    rel_error = (abs(residual) / abs(X_dimless)) if X_dimless != 0 else Fraction(0,1)
    return U, k, residual, rel_error

def make_row(**kw):
    kw.setdefault("dna_strategy","k")
    if isinstance(kw.get("dna_k_mods"), dict):
        kw["dna_k_mods"] = json.dumps(kw["dna_k_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_k_mods", json.dumps({}))
    if isinstance(kw.get("dna_pq_mods"), dict):
        kw["dna_pq_mods"] = json.dumps(kw["dna_pq_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_pq_mods", json.dumps({}))
    return kw

def emit_files(rows, csv_path, jsonl_path, md_path, asc_path):
    fieldnames = list(rows[0].keys())
    with open(csv_path, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames)
        w.writeheader(); w.writerows(rows)
    with open(jsonl_path, "w") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False)+"\n")

    # Markdown snapshot
    with open(md_path, "w") as f:
        f.write("# UPT_G_v1 — Gravity on Universal Lattice\n\n")
        f.write(f"- Rows: **{len(rows)}**\n- Run: **{RUN_TS}Z**\n\n")
        f.write("| uid | domain | tier | symbol | value_dimless | U_family | p | k | rel_error |\n")
        f.write("|---|---:|---:|---|---:|---|---:|---:|---:|\n")
        for r in rows:
            f.write(f"| {r['uid']} | {r['domain']} | {r['tier']} | {r['symbol']} | {r['value_dimless']} | {r['U_family']} | {r['p_index']} | {r['k']} | {r['rel_error']} |\n")

    # ASCII with the same sections we print to stdout (for copy/share)
    with open(asc_path, "w") as f:
        f.write(stdout_payload(rows))

# ---------- physics constants as Fractions (stable rounding via limit_denominator) ----------
def frac_of_float(x: float, max_den: int = 10**18) -> Fraction:
    return Fraction(x).limit_denominator(max_den)

LN2 = frac_of_float(math.log(2.0))
PI  = frac_of_float(math.pi)

# Targets (dimensionless, Planck-normalized)
A_bit_over_lp2    = frac_of_float(4.0*math.log(2.0))              # 4 ln 2
DeltaM2_over_mP2  = frac_of_float(math.log(2.0)/(4.0*math.pi))     # ln2/(4π)
def Sbits_of_mass_ratio(M_over_mP: Fraction) -> Fraction:
    return frac_of_float( (4.0*math.pi/math.log(2.0)) * (to_float(M_over_mP)**2) )

# Canonical M/m_P targets (dimensionless only in this module)
M_over_mP_targets = {"mP": Fraction(1,1), "10mP": Fraction(10,1)}

# Domain scale (calibratable later)
s_G = Fraction(1,1)

# Chosen p-indices (tunable; readable)
p_abit, p_tick, p_sbits = 64, 62, 64

# ---------- build rows ----------
rows = []

def add_entry(uid, domain, tier, name, symbol, X_dimless: Fraction, U_family, p_index, s_domain: Fraction,
              definition="", notes="", status="derived", calibration_target=""):
    U, k, residual, rel_error = qtz(X_dimless, U_family, p_index, s_domain)
    dna = fingerprint_k(k)
    pq_num, pq_den = X_dimless.numerator, X_dimless.denominator
    hash_core = row_hashes_core(domain, symbol, f"{pq_num}/{pq_den}", pq_num, pq_den)
    version = f"UPT_G_v1@{RUN_TS}"
    row = make_row(
        uid=uid, domain=domain, tier=tier, name=name, symbol=symbol, definition=definition,
        value_si="", unit_si="",
        value_dimless=f"{pq_num}/{pq_den}", norm_ref="Planck",
        pq_num=pq_num, pq_den=pq_den, pq_value=to_float(X_dimless),
        U_family=U_family, U_scale=f"{s_domain} * U_EM(p)", p_index=p_index,
        k=k, U_value=to_float(U),
        residual=float(to_float(residual)), rel_error=float(to_float(rel_error)),
        bits_pq=bits_pq(pq_num,pq_den), bits_k=bits_int(abs(k)),
        dna_mod_23=dna[23], dna_mod_49=dna[49], dna_mod_50=dna[50], dna_mod_137=dna[137],
        dna_strategy="k", dna_k_mods=dna, dna_pq_mods={},
        provenance="UPT_G_v1/derived", status=status, calibration_target=calibration_target,
        notes=notes, hash_core=hash_core, version=version
    )
    row["hash_entry"] = row_hash_full(row)
    rows.append(row)

# Add entries
add_entry(
    uid="UPT/Gravity/4/A_bit_over_lp2#G_v1",
    domain="Gravity", tier=4, name="Area per bit (A_bit / l_P^2)", symbol="A_bit_over_lp2",
    X_dimless=A_bit_over_lp2, U_family="G", p_index=p_abit, s_domain=s_G,
    definition="A_bit/l_P^2 = 4 ln 2", notes="Planck-locked; domain lattice s_G=1"
)
add_entry(
    uid="UPT/Gravity/4/DeltaM2_over_mP2#G_v1",
    domain="Gravity", tier=4, name="Mass-squared tick (ΔM^2 / m_P^2)", symbol="DeltaM2_over_mP2",
    X_dimless=DeltaM2_over_mP2, U_family="G", p_index=p_tick, s_domain=s_G,
    definition="Δ(M^2)/m_P^2 = ln 2 /(4π)", notes="Tick that steps BH mass^2 ladder"
)
for tag, Mr in M_over_mP_targets.items():
    S_bits = Sbits_of_mass_ratio(Mr)
    add_entry(
        uid=f"UPT/Gravity/4/S_bits_{tag}#G_v1",
        domain="Gravity", tier=4, name=f"BH entropy in bits at M={tag}", symbol=f"S_bits_{tag}",
        X_dimless=S_bits, U_family="G", p_index=p_sbits, s_domain=s_G,
        definition="S_bits = (4π/ln 2)(M/m_P)^2", notes="Schwarzschild; geometric units → Planck-normalized"
    )

# ---------- stdout payload (mirrors EM style) ----------
def fmt_sci(x):
    if isinstance(x, (int,)):
        return str(x)
    try:
        return f"{float(x):.3E}"
    except Exception:
        return str(x)

def stdout_payload(rows):
    lines = []
    lines.append("="*92+"\n")
    lines.append("GRAVITY/PLANCK LADDER ON UNIVERSAL LATTICE  U(p)=1/(49·50·137^p)\n")
    lines.append("="*92+"\n")

    # TARGETS
    lines.append("[BEGIN SECTION:targets]\n")
    lines.append("symbol           p     k                          U(p)              X_dimless           residual            rel_error   \n")
    lines.append("----------------------------------------------------------------------------------------------------------\n")
    for r in rows:
        lines.append(f"{r['symbol']:<16} {r['p_index']:>3}  {str(r['k']):>24}  ")
        lines.append(f"{r['U_value']:<16}  {fmt_sci(r['pq_value']):<18}  {fmt_sci(r['residual']):<18}  {fmt_sci(r['rel_error'])}\n")
    lines.append("[END SECTION:targets]\n\n")

    # ARTIFACTS
    lines.append("="*92+"\n")
    lines.append("EMIT ARTIFACTS\n")
    lines.append("="*92+"\n")
    lines.append("[BEGIN SECTION:write_files]\n")
    lines.append(f"[KV] CSV = {CSV_PATH}\n")
    lines.append(f"[KV] JSONL = {JSONL_PATH}\n")
    lines.append(f"[KV] MD = {MD_PATH}\n")
    lines.append(f"[KV] ASCII = {ASC_PATH}\n")
    lines.append("[END SECTION:write_files]\n\n")

    # DIGEST
    digest = sha256_str(json.dumps(rows, sort_keys=True))
    lines.append("="*92+"\n")
    lines.append("REGISTRY DIGEST & SNAPSHOT\n")
    lines.append("="*92+"\n")
    lines.append("[BEGIN SECTION:digest]\n")
    lines.append(f"[KV] rows = {len(rows)}\n")
    lines.append(f"[KV] registry_digest_sha256 = {digest}\n")
    lines.append("uid                                     domain  tier  symbol           value_dimless       U_famil  p_index  k             rel_error         \n")
    lines.append("-------------------------------------------------------------------------------------------------------------------------------------\n")
    for r in rows:
        lines.append(f"{r['uid']:<40} {r['domain']:<7} {r['tier']:<5} {r['symbol']:<15} {r['value_dimless']:<18} {r['U_family']:<7} {r['p_index']:<7} {str(r['k'])[:12]:<12} {fmt_sci(r['rel_error'])}\n")
    lines.append("[END SECTION:digest]\n\n")

    # FOOTER
    lines.append("="*92+"\n")
    lines.append("UPT_G_v1 — COMPLETE\n")
    lines.append("="*92+"\n")
    lines.append("NEXT: If outputs look good, we can calibrate s_G or proceed to UPT_QCD_v1.\n")
    return "".join(lines)

# ---------- emit to files + stdout ----------
emit_files(rows, CSV_PATH, JSONL_PATH, MD_PATH, ASC_PATH)
print(stdout_payload(rows), end="")

# ====================== UPT_QCD_v1 — QCD on the Universal Lattice (stdout+files) ============================
# - Uses only the universal unit U(p)=1/(49·50·137^p) with a domain scale s_QCD (rational, calibrated).
# - Emits human+AI readable stdout sections and CSV/JSONL/MD/ASCII artifacts.
# - Timezone-aware timestamps; deprecation warnings silenced.
# ===========================================================================================================

import math, json, csv, hashlib, os, warnings
import datetime as dt
from fractions import Fraction

# ---- silence DeprecationWarnings globally
warnings.filterwarnings("ignore", category=DeprecationWarning)

# ---- run timestamp (timezone-aware; no deprecation warnings)
RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

# ---- IO paths
OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)

CSV_PATH   = os.path.join(OUT_DIR,   f"UPT_master_QCD_{RUN_TS}.csv")
JSONL_PATH = os.path.join(OUT_DIR,   f"UPT_master_QCD_{RUN_TS}.jsonl")
MD_PATH    = os.path.join(REPORT_DIR,f"UPT_report_QCD_{RUN_TS}.md")
ASC_PATH   = os.path.join(REPORT_DIR,f"UPT_ascii_QCD_{RUN_TS}.txt")

# ============================================================================================================
# Universal lattice & helpers (same conventions as prior modules)
# ============================================================================================================
def U_EM(p: int) -> Fraction:
    return Fraction(1, 49*50*137**p)

def U_scaled(p: int, s: Fraction) -> Fraction:
    return s * U_EM(p)

def bits_int(n: int) -> int:
    return 1 if n == 0 else n.bit_length()

def bits_pq(p: int, q: int) -> int:
    return bits_int(abs(p)) + bits_int(abs(q))

def sha256_str(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def to_float(fr: Fraction) -> float:
    return fr.numerator / fr.denominator

def fingerprint_k(k: int, mods=(23,49,50,137)):
    return {m: (k % m) for m in mods}

def row_hashes_core(domain, symbol, value_dimless, pq_num, pq_den):
    core = f"{domain}|{symbol}|{value_dimless}|{pq_num}|{pq_den}"
    return sha256_str(core)

def row_hash_full(row: dict):
    ordered_keys = [
        "uid","domain","tier","name","symbol","definition",
        "value_si","unit_si","value_dimless","norm_ref",
        "pq_num","pq_den","pq_value",
        "U_family","U_scale","p_index","k","U_value",
        "residual","rel_error","bits_pq","bits_k",
        "dna_mod_23","dna_mod_49","dna_mod_50","dna_mod_137",
        "dna_strategy","dna_k_mods","dna_pq_mods",
        "provenance","status","calibration_target",
        "notes","hash_core","version"
    ]
    payload = "|".join(str(row.get(k,"")) for k in ordered_keys)
    return sha256_str(payload)

def qtz(X_dimless: Fraction, U_family: str, p_index: int, s_domain: Fraction):
    # Banker’s rounding on exact rational ratio
    U = U_scaled(p_index, s_domain)
    k_exact = X_dimless / U
    k = int(k_exact.numerator // k_exact.denominator)
    r = k_exact - k
    if r > Fraction(1,2):
        k += 1
    elif r == Fraction(1,2):
        if (k % 2) != 0: k += 1
    residual = X_dimless - k*U
    rel_error = (abs(residual) / abs(X_dimless)) if X_dimless != 0 else Fraction(0,1)
    return U, k, residual, rel_error

def make_row(**kw):
    kw.setdefault("dna_strategy","k")
    if isinstance(kw.get("dna_k_mods"), dict):
        kw["dna_k_mods"] = json.dumps(kw["dna_k_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_k_mods", json.dumps({}))
    if isinstance(kw.get("dna_pq_mods"), dict):
        kw["dna_pq_mods"] = json.dumps(kw["dna_pq_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_pq_mods", json.dumps({}))
    return kw

def emit_files(rows, csv_path, jsonl_path, md_path, asc_path, ascii_text):
    fieldnames = list(rows[0].keys())
    with open(csv_path, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames)
        w.writeheader(); w.writerows(rows)
    with open(jsonl_path, "w") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False)+"\n")

    with open(md_path, "w") as f:
        f.write("# UPT_QCD_v1 — QCD on Universal Lattice\n\n")
        f.write(f"- Rows: **{len(rows)}**\n- Run: **{RUN_TS}Z**\n\n")
        f.write("| uid | domain | tier | symbol | value_dimless | U_family | p | k | rel_error |\n")
        f.write("|---|---:|---:|---|---:|---|---:|---:|---:|\n")
        for r in rows:
            f.write(f"| {r['uid']} | {r['domain']} | {r['tier']} | {r['symbol']} | {r['value_dimless']} | {r['U_family']} | {r['p_index']} | {r['k']} | {r['rel_error']} |\n")

    with open(asc_path, "w") as f:
        f.write(ascii_text)

# ---------- fraction helper ----------
def frac_of_float(x: float, max_den: int = 10**18) -> Fraction:
    return Fraction(x).limit_denominator(max_den)

# ============================================================================================================
# Targets (dimensionless) & calibration anchor
# We stay within your “no external paste” rule by committing to exact rationals you already used in prior runs.
# ------------------------------------------------------------------------------------------------------------
# alpha_s(MZ) ≈ 0.1179  -> 9953/84419 (from your earlier rational registry printout)
alpha_s_MZ = Fraction(9953, 84419)  # ≈ 0.117899998815

# Optional: add one more rational landmark if desired later (kept minimal here).
targets = [
    ("alpha_s_MZ", "Strong coupling at MZ", alpha_s_MZ, 1, "Planck-normalized, dimensionless"),
]

# ============================================================================================================
# QCD domain scale and calibration
# ------------------------------------------------------------------------------------------------------------
# We choose a p-index for QCD such that k is large (good DNA). You can change p_QCD without changing the logic.
p_QCD = 55

def calibrate_s_qcd(anchors, p_index: int, max_den: int = 10_000) -> Fraction:
    """
    anchors: list of tuples (name, X_dimless Fraction, target_k or None)
    If target_k is None, we take k* = round(X/U_EM(p)) under s=1, then fit s so that k = k* exactly.
    We return a single s_QCD (Fraction) as the median of per-anchor s estimates, rationalized.
    """
    s_list = []
    for (nm, X, k_star) in anchors:
        U0 = U_EM(p_index)
        if k_star is None:
            # “natural” k with s=1
            k0_exact = X / U0
            k0 = int(k0_exact.numerator // k0_exact.denominator)
            r = k0_exact - k0
            if r > Fraction(1,2) or (r == Fraction(1,2) and (k0 % 2) != 0):
                k0 += 1
            k_star = k0
        # We want s so that X ≈ k_star * s * U_EM(p) → s = X / (k_star * U_EM(p))
        s_exact = X / (U0 * k_star)
        s_list.append(s_exact)
    # Median-of-rationals → sort by float value
    s_list_sorted = sorted(s_list, key=lambda z: to_float(z))
    median = s_list_sorted[len(s_list_sorted)//2]
    return Fraction(median).limit_denominator(max_den)

# Build anchor list: use alpha_s(MZ) only (simple and auditable)
anchor_defs = [("alpha_s_MZ", alpha_s_MZ, None)]
s_QCD = calibrate_s_qcd(anchor_defs, p_QCD, max_den=10000)  # rational
# You can “freeze” s_QCD by reusing this exact Fraction on later runs for reproducibility.

# ============================================================================================================
# Row builder
# ============================================================================================================
rows = []

def add_entry(uid, domain, tier, name, symbol, X_dimless: Fraction, U_family, p_index, s_domain: Fraction,
              definition="", notes="", status="derived", calibration_target=""):
    U, k, residual, rel_error = qtz(X_dimless, U_family, p_index, s_domain)
    dna = fingerprint_k(k)
    pq_num, pq_den = X_dimless.numerator, X_dimless.denominator
    hash_core = row_hashes_core(domain, symbol, f"{pq_num}/{pq_den}", pq_num, pq_den)
    version = f"UPT_QCD_v1@{RUN_TS}"
    row = make_row(
        uid=uid, domain=domain, tier=tier, name=name, symbol=symbol, definition=definition,
        value_si="", unit_si="",
        value_dimless=f"{pq_num}/{pq_den}", norm_ref="Planck",
        pq_num=pq_num, pq_den=pq_den, pq_value=to_float(X_dimless),
        U_family=U_family, U_scale=f"{s_domain} * U_EM(p)", p_index=p_index,
        k=k, U_value=to_float(U),
        residual=float(to_float(residual)), rel_error=float(to_float(rel_error)),
        bits_pq=bits_pq(pq_num,pq_den), bits_k=bits_int(abs(k)),
        dna_mod_23=dna[23], dna_mod_49=dna[49], dna_mod_50=dna[50], dna_mod_137=dna[137],
        dna_strategy="k", dna_k_mods=dna, dna_pq_mods={},
        provenance="UPT_QCD_v1/derived", status=status, calibration_target=calibration_target,
        notes=notes, hash_core=hash_core, version=version
    )
    row["hash_entry"] = row_hash_full(row)
    rows.append(row)

# ============================================================================================================
# Populate rows
# ============================================================================================================
# 1) Calibration snapshot row (Meta) for s_QCD
meta_uid = "UPT/Meta/8/s_QCD_snapshot#QCD_v1"
meta_symbol = "s_QCD"
meta_val = s_QCD
add_entry(
    uid=meta_uid, domain="Meta", tier=8, name="QCD lattice scale snapshot",
    symbol=meta_symbol, X_dimless=meta_val,
    U_family="QCD", p_index=p_QCD, s_domain=Fraction(1,1),
    definition="Calibrated s_QCD as rational; used to scale U_EM(p) for QCD domain.",
    notes="Anchor set digest recorded in ASCII; U_scale is s_QCD*U_EM(p).",
    status="calibration", calibration_target="alpha_s_MZ"
)

# 2) Primary QCD target(s) using the calibrated lattice
add_entry(
    uid="UPT/QCD/1/alpha_s_MZ#QCD_v1",
    domain="QCD", tier=1, name="Strong coupling at M_Z",
    symbol="alpha_s_MZ", X_dimless=alpha_s_MZ,
    U_family="QCD", p_index=p_QCD, s_domain=s_QCD,
    definition="α_s(M_Z) quantized on U_QCD(p)=s_QCD·U_EM(p)",
    notes="Exact rational input from registry; k and DNA computed on calibrated lattice.",
    status="measured", calibration_target="alpha_s_MZ"
)

# You can extend with more QCD landmarks later (e.g., α_s at other μ, Λ_QCD ratios) via UPT_QCD_v2.

# ============================================================================================================
# Stdout/ASCII formatting (matching your EM/Gravity look)
# ============================================================================================================
def fmt_sci(x):
    if isinstance(x, (int,)):
        return str(x)
    try:
        return f"{float(x):.3E}"
    except Exception:
        return str(x)

def ascii_payload(rows, s_qcd, anchors):
    lines = []
    lines.append("="*92+"\n")
    lines.append("QCD ON UNIVERSAL LATTICE  U(p)=1/(49·50·137^p) with U_QCD(p)=s_QCD·U_EM(p)\n")
    lines.append("="*92+"\n")

    # CALIBRATION
    lines.append("[BEGIN SECTION:calibration]\n")
    # anchor digest for audit
    digest_obj = [{"name": nm, "p_index": p_QCD,
                   "value_dimless": f"{X.numerator}/{X.denominator}", "target_k": tk}
                  for (nm, X, tk) in anchors]
    anchor_digest = sha256_str(json.dumps(digest_obj, sort_keys=True))
    lines.append(f"[KV] p_QCD = {p_QCD}\n")
    lines.append(f"[KV] anchors = {json.dumps(digest_obj, ensure_ascii=False)}\n")
    lines.append(f"[KV] anchors_digest_sha256 = {anchor_digest}\n")
    lines.append(f"[KV] s_QCD (rational) = {s_qcd.numerator}/{s_qcd.denominator}  ≈ {to_float(s_qcd)}\n")
    lines.append("[END SECTION:calibration]\n\n")

    # TARGETS
    lines.append("="*92+"\n")
    lines.append("QCD TARGETS\n")
    lines.append("="*92+"\n")
    lines.append("[BEGIN SECTION:targets]\n")
    lines.append("symbol         p     k                          U(p)              X_dimless           residual            rel_error   \n")
    lines.append("----------------------------------------------------------------------------------------------------------\n")
    for r in rows:
        if r["domain"] != "QCD": continue
        lines.append(f"{r['symbol']:<14} {r['p_index']:>3}  {str(r['k']):>24}  ")
        lines.append(f"{r['U_value']:<16}  {fmt_sci(r['pq_value']):<18}  {fmt_sci(r['residual']):<18}  {fmt_sci(r['rel_error'])}\n")
    lines.append("[END SECTION:targets]\n\n")

    # ARTIFACTS placeholder (paths filled later)
    lines.append("="*92+"\n")
    lines.append("EMIT ARTIFACTS\n")
    lines.append("="*92+"\n")
    lines.append("[BEGIN SECTION:write_files]\n")  # Filled by caller
    # file paths are appended by caller
    # DIGEST & SNAPSHOT later as well
    return "".join(lines)

# Build initial ASCII payload
ascii_text = ascii_payload(rows, s_QCD, anchor_defs)

# ============================================================================================================
# Emit files and complete stdout (including paths + digest table)
# ============================================================================================================
def finalize_and_print(rows, ascii_text):
    # Append actual file paths
    lines = [ascii_text]
    lines.append(f"[KV] CSV = {CSV_PATH}\n")
    lines.append(f"[KV] JSONL = {JSONL_PATH}\n")
    lines.append(f"[KV] MD = {MD_PATH}\n")
    lines.append(f"[KV] ASCII = {ASC_PATH}\n")
    lines.append("[END SECTION:write_files]\n\n")

    # DIGEST
    reg_digest = sha256_str(json.dumps(rows, sort_keys=True))
    lines.append("="*92+"\n")
    lines.append("REGISTRY DIGEST & SNAPSHOT\n")
    lines.append("="*92+"\n")
    lines.append("[BEGIN SECTION:digest]\n")
    lines.append(f"[KV] rows = {len(rows)}\n")
    lines.append(f"[KV] registry_digest_sha256 = {reg_digest}\n")
    lines.append("uid                                     domain  tier  symbol         value_dimless       U_famil  p_index  k             rel_error         \n")
    lines.append("-------------------------------------------------------------------------------------------------------------------------------------\n")
    for r in rows:
        lines.append(f"{r['uid']:<40} {r['domain']:<7} {r['tier']:<5} {r['symbol']:<13} {r['value_dimless']:<18} {r['U_family']:<7} {r['p_index']:<7} {str(r['k'])[:12]:<12} {fmt_sci(r['rel_error'])}\n")
    lines.append("[END SECTION:digest]\n\n")

    # FOOTER
    lines.append("="*92+"\n")
    lines.append("UPT_QCD_v1 — COMPLETE\n")
    lines.append("="*92+"\n")
    return "".join(lines)

# Write artifacts
ascii_full = finalize_and_print(rows, ascii_text)
emit_files(rows, CSV_PATH, JSONL_PATH, MD_PATH, ASC_PATH, ascii_full)

# Print stdout
print(ascii_full, end="")

# ====================== UPT_COS_v1 — Cosmology on the Universal Lattice (stdout+files) ======================
# Uses only the universal unit U(p)=1/(49·50·137^p) with a domain scale s_COS (rational, calibrated).
# Anchor: Ω_total = 1 (closure). No external values. Same stdout/CSV/JSONL/MD/ASCII as prior modules.
# ===========================================================================================================

import math, json, csv, hashlib, os, warnings
import datetime as dt
from fractions import Fraction

# Silence deprecation warnings globally
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Timezone-aware run timestamp
RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

# IO paths
OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)

CSV_PATH   = os.path.join(OUT_DIR,   f"UPT_master_COS_{RUN_TS}.csv")
JSONL_PATH = os.path.join(OUT_DIR,   f"UPT_master_COS_{RUN_TS}.jsonl")
MD_PATH    = os.path.join(REPORT_DIR,f"UPT_report_COS_{RUN_TS}.md")
ASC_PATH   = os.path.join(REPORT_DIR,f"UPT_ascii_COS_{RUN_TS}.txt")

# ============= Universal lattice & helpers (same as other modules) ==========================================
def U_EM(p: int) -> Fraction:
    return Fraction(1, 49*50*137**p)

def U_scaled(p: int, s: Fraction) -> Fraction:
    return s * U_EM(p)

def bits_int(n: int) -> int:
    return 1 if n == 0 else n.bit_length()

def bits_pq(p: int, q: int) -> int:
    return bits_int(abs(p)) + bits_int(abs(q))

def sha256_str(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def to_float(fr: Fraction) -> float:
    return fr.numerator / fr.denominator

def fingerprint_k(k: int, mods=(23,49,50,137)):
    return {m: (k % m) for m in mods}

def row_hashes_core(domain, symbol, value_dimless, pq_num, pq_den):
    core = f"{domain}|{symbol}|{value_dimless}|{pq_num}|{pq_den}"
    return sha256_str(core)

def row_hash_full(row: dict):
    ordered_keys = [
        "uid","domain","tier","name","symbol","definition",
        "value_si","unit_si","value_dimless","norm_ref",
        "pq_num","pq_den","pq_value",
        "U_family","U_scale","p_index","k","U_value",
        "residual","rel_error","bits_pq","bits_k",
        "dna_mod_23","dna_mod_49","dna_mod_50","dna_mod_137",
        "dna_strategy","dna_k_mods","dna_pq_mods",
        "provenance","status","calibration_target",
        "notes","hash_core","version"
    ]
    payload = "|".join(str(row.get(k,"")) for k in ordered_keys)
    return sha256_str(payload)

def qtz(X_dimless: Fraction, U_family: str, p_index: int, s_domain: Fraction):
    U = U_scaled(p_index, s_domain)
    k_exact = X_dimless / U
    k = int(k_exact.numerator // k_exact.denominator)
    r = k_exact - k
    if r > Fraction(1,2):
        k += 1
    elif r == Fraction(1,2):
        if (k % 2) != 0: k += 1
    residual = X_dimless - k*U
    rel_error = (abs(residual) / abs(X_dimless)) if X_dimless != 0 else Fraction(0,1)
    return U, k, residual, rel_error

def make_row(**kw):
    kw.setdefault("dna_strategy","k")
    if isinstance(kw.get("dna_k_mods"), dict):
        kw["dna_k_mods"] = json.dumps(kw["dna_k_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_k_mods", json.dumps({}))
    if isinstance(kw.get("dna_pq_mods"), dict):
        kw["dna_pq_mods"] = json.dumps(kw["dna_pq_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_pq_mods", json.dumps({}))
    return kw

def emit_files(rows, csv_path, jsonl_path, md_path, asc_path, ascii_text):
    fieldnames = list(rows[0].keys())
    with open(csv_path, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames)
        w.writeheader(); w.writerows(rows)
    with open(jsonl_path, "w") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False)+"\n")
    with open(md_path, "w") as f:
        f.write("# UPT_COS_v1 — Cosmology on Universal Lattice\n\n")
        f.write(f"- Rows: **{len(rows)}**\n- Run: **{RUN_TS}Z**\n\n")
        f.write("| uid | domain | tier | symbol | value_dimless | U_family | p | k | rel_error |\n")
        f.write("|---|---:|---:|---|---:|---|---:|---:|---:|\n")
        for r in rows:
            f.write(f"| {r['uid']} | {r['domain']} | {r['tier']} | {r['symbol']} | {r['value_dimless']} | {r['U_family']} | {r['p_index']} | {r['k']} | {r['rel_error']} |\n")
    with open(asc_path, "w") as f:
        f.write(ascii_text)

# ============= Calibration with Ω_total = 1 ================================================================
# We choose a p-index giving large k and crisp residues.
p_COS = 54
Omega_tot = Fraction(1,1)  # closure anchor (exact)

def calibrate_s_cos(anchors, p_index: int, max_den: int = 10_000) -> Fraction:
    """anchors: list of tuples (name, X_dimless Fraction, target_k or None).
       If target_k is None, compute the 'natural' k at s=1 via bankers rounding, then solve s."""
    s_list = []
    for (nm, X, k_star) in anchors:
        U0 = U_EM(p_index)
        if k_star is None:
            k0_exact = X / U0
            k0 = int(k0_exact.numerator // k0_exact.denominator)
            r = k0_exact - k0
            if r > Fraction(1,2) or (r == Fraction(1,2) and (k0 % 2) != 0):
                k0 += 1
            k_star = k0
        s_exact = X / (U0 * k_star)
        s_list.append(s_exact)
    s_list_sorted = sorted(s_list, key=lambda z: to_float(z))
    median = s_list_sorted[len(s_list_sorted)//2]
    return Fraction(median).limit_denominator(max_den)

anchor_defs = [("Omega_total", Omega_tot, None)]
s_COS = calibrate_s_cos(anchor_defs, p_COS, max_den=10000)

# ============= Row builder ================================================================================
rows = []

def add_entry(uid, domain, tier, name, symbol, X_dimless: Fraction, U_family, p_index, s_domain: Fraction,
              definition="", notes="", status="derived", calibration_target=""):
    U, k, residual, rel_error = qtz(X_dimless, U_family, p_index, s_domain)
    dna = fingerprint_k(k)
    pq_num, pq_den = X_dimless.numerator, X_dimless.denominator
    hash_core = row_hashes_core(domain, symbol, f"{pq_num}/{pq_den}", pq_num, pq_den)
    version = f"UPT_COS_v1@{RUN_TS}"
    row = make_row(
        uid=uid, domain=domain, tier=tier, name=name, symbol=symbol, definition=definition,
        value_si="", unit_si="",
        value_dimless=f"{pq_num}/{pq_den}", norm_ref="Planck",
        pq_num=pq_num, pq_den=pq_den, pq_value=to_float(X_dimless),
        U_family=U_family, U_scale=f"{s_domain} * U_EM(p)", p_index=p_index,
        k=k, U_value=to_float(U),
        residual=float(to_float(residual)), rel_error=float(to_float(rel_error)),
        bits_pq=bits_pq(pq_num,pq_den), bits_k=bits_int(abs(k)),
        dna_mod_23=dna[23], dna_mod_49=dna[49], dna_mod_50=dna[50], dna_mod_137=dna[137],
        dna_strategy="k", dna_k_mods=dna, dna_pq_mods={},
        provenance="UPT_COS_v1/derived", status=status, calibration_target=calibration_target,
        notes=notes, hash_core=hash_core, version=version
    )
    row["hash_entry"] = row_hash_full(row)
    rows.append(row)

# ============= Populate rows ===============================================================================
# 1) Meta snapshot for s_COS
add_entry(
    uid="UPT/Meta/8/s_COS_snapshot#COS_v1",
    domain="Meta", tier=8, name="Cosmology lattice scale snapshot",
    symbol="s_COS", X_dimless=s_COS,
    U_family="COS", p_index=p_COS, s_domain=Fraction(1,1),
    definition="Calibrated s_COS as rational; scales U_EM(p) for cosmology.",
    notes="Anchor: Ω_total=1. U_scale is s_COS·U_EM(p).",
    status="calibration", calibration_target="Omega_total"
)

# 2) Cosmology target: Ω_total = 1 quantified on the calibrated lattice
add_entry(
    uid="UPT/COS/6/Omega_total#COS_v1",
    domain="Cosmology", tier=6, name="Cosmic budget closure",
    symbol="Omega_total", X_dimless=Omega_tot,
    U_family="COS", p_index=p_COS, s_domain=s_COS,
    definition="Ω_tot quantized on U_COS(p)=s_COS·U_EM(p).",
    notes="Exact closure anchor, emitted for auditing.",
    status="measured", calibration_target="Omega_total"
)

# ============= Stdout/ASCII formatting =====================================================================
def fmt_sci(x):
    if isinstance(x, (int,)):
        return str(x)
    try:
        return f"{float(x):.3E}"
    except Exception:
        return str(x)

def ascii_payload(rows, s_cos, anchors):
    lines = []
    lines.append("="*92+"\n")
    lines.append("COSMOLOGY ON UNIVERSAL LATTICE  U(p)=1/(49·50·137^p) with U_COS(p)=s_COS·U_EM(p)\n")
    lines.append("="*92+"\n")

    # CALIBRATION
    lines.append("[BEGIN SECTION:calibration]\n")
    digest_obj = [{"name": nm, "p_index": p_COS,
                   "value_dimless": f"{X.numerator}/{X.denominator}", "target_k": tk}
                  for (nm, X, tk) in anchors]
    anchor_digest = sha256_str(json.dumps(digest_obj, sort_keys=True))
    lines.append(f"[KV] p_COS = {p_COS}\n")
    lines.append(f"[KV] anchors = {json.dumps(digest_obj, ensure_ascii=False)}\n")
    lines.append(f"[KV] anchors_digest_sha256 = {anchor_digest}\n")
    lines.append(f"[KV] s_COS (rational) = {s_cos.numerator}/{s_cos.denominator}  ≈ {to_float(s_cos)}\n")
    lines.append("[END SECTION:calibration]\n\n")

    # TARGETS
    lines.append("="*92+"\n")
    lines.append("COSMOLOGY TARGETS\n")
    lines.append("="*92+"\n")
    lines.append("[BEGIN SECTION:targets]\n")
    lines.append("symbol         p     k                          U(p)              X_dimless           residual            rel_error   \n")
    lines.append("----------------------------------------------------------------------------------------------------------\n")
    for r in rows:
        if r["domain"] != "Cosmology": continue
        lines.append(f"{r['symbol']:<14} {r['p_index']:>3}  {str(r['k']):>24}  ")
        lines.append(f"{r['U_value']:<16}  {fmt_sci(r['pq_value']):<18}  {fmt_sci(r['residual']):<18}  {fmt_sci(r['rel_error'])}\n")
    lines.append("[END SECTION:targets]\n\n")

    # ARTIFACTS placeholder
    lines.append("="*92+"\n")
    lines.append("EMIT ARTIFACTS\n")
    lines.append("="*92+"\n")
    lines.append("[BEGIN SECTION:write_files]\n")
    return "".join(lines)

ascii_text = ascii_payload(rows, s_COS, anchor_defs)

# ============= Emit files & print ==========================================================================
def finalize_and_print(rows, ascii_text):
    lines = [ascii_text]
    lines.append(f"[KV] CSV = {CSV_PATH}\n")
    lines.append(f"[KV] JSONL = {JSONL_PATH}\n")
    lines.append(f"[KV] MD = {MD_PATH}\n")
    lines.append(f"[KV] ASCII = {ASC_PATH}\n")
    lines.append("[END SECTION:write_files]\n\n")

    reg_digest = sha256_str(json.dumps(rows, sort_keys=True))
    lines.append("="*92+"\n")
    lines.append("REGISTRY DIGEST & SNAPSHOT\n")
    lines.append("="*92+"\n")
    lines.append("[BEGIN SECTION:digest]\n")
    lines.append(f"[KV] rows = {len(rows)}\n")
    lines.append(f"[KV] registry_digest_sha256 = {reg_digest}\n")
    lines.append("uid                                     domain     tier  symbol         value_dimless       U_famil  p_index  k             rel_error         \n")
    lines.append("-------------------------------------------------------------------------------------------------------------------------------------\n")
    for r in rows:
        lines.append(f"{r['uid']:<40} {r['domain']:<10} {r['tier']:<5} {r['symbol']:<13} {r['value_dimless']:<18} {r['U_family']:<7} {r['p_index']:<7} {str(r['k'])[:12]:<12} {fmt_sci(r['rel_error'])}\n")
    lines.append("[END SECTION:digest]\n\n")

    lines.append("="*92+"\n")
    lines.append("UPT_COS_v1 — COMPLETE\n")
    lines.append("="*92+"\n")
    return "".join(lines)

ascii_full = finalize_and_print(rows, ascii_text)
emit_files(rows, CSV_PATH, JSONL_PATH, MD_PATH, ASC_PATH, ascii_full)
print(ascii_full, end="")

# ====================== UPT_DERIVED_v1 — Machine-checkable constraints on the Universal Lattice =============
# Purely internal, exact-rational constraints; no external numbers required.
# Emits: (i) Ω_tot closure deviation, (ii) custodial ρ(tree) identity, plus Meta digest.
# ===========================================================================================================

import math, json, csv, hashlib, os, warnings
import datetime as dt
from fractions import Fraction

# Silence deprecation warnings globally
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Timezone-aware run timestamp
RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

# IO paths
OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)

CSV_PATH   = os.path.join(OUT_DIR,   f"UPT_master_DERIVED_{RUN_TS}.csv")
JSONL_PATH = os.path.join(OUT_DIR,   f"UPT_master_DERIVED_{RUN_TS}.jsonl")
MD_PATH    = os.path.join(REPORT_DIR,f"UPT_report_DERIVED_{RUN_TS}.md")
ASC_PATH   = os.path.join(REPORT_DIR,f"UPT_ascii_DERIVED_{RUN_TS}.txt")

# ============= Universal lattice & helpers (same as other modules) ==========================================
def U_EM(p: int) -> Fraction:
    return Fraction(1, 49*50*137**p)

def U_scaled(p: int, s: Fraction) -> Fraction:
    return s * U_EM(p)

def bits_int(n: int) -> int:
    return 1 if n == 0 else n.bit_length()

def bits_pq(p: int, q: int) -> int:
    return bits_int(abs(p)) + bits_int(abs(q))

def sha256_str(s: str) -> str:
    import hashlib
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def to_float(fr: Fraction) -> float:
    return fr.numerator / fr.denominator

def fingerprint_k(k: int, mods=(23,49,50,137)):
    return {m: (k % m) for m in mods}

def row_hashes_core(domain, symbol, value_dimless, pq_num, pq_den):
    core = f"{domain}|{symbol}|{value_dimless}|{pq_num}|{pq_den}"
    return sha256_str(core)

def row_hash_full(row: dict):
    ordered_keys = [
        "uid","domain","tier","name","symbol","definition",
        "value_si","unit_si","value_dimless","norm_ref",
        "pq_num","pq_den","pq_value",
        "U_family","U_scale","p_index","k","U_value",
        "residual","rel_error","bits_pq","bits_k",
        "dna_mod_23","dna_mod_49","dna_mod_50","dna_mod_137",
        "dna_strategy","dna_k_mods","dna_pq_mods",
        "provenance","status","calibration_target",
        "notes","hash_core","version"
    ]
    payload = "|".join(str(row.get(k,"")) for k in ordered_keys)
    return sha256_str(payload)

def qtz(X_dimless: Fraction, U_family: str, p_index: int, s_domain: Fraction):
    U = U_scaled(p_index, s_domain)
    k_exact = X_dimless / U
    k = int(k_exact.numerator // k_exact.denominator)
    r = k_exact - k
    if r > Fraction(1,2):
        k += 1
    elif r == Fraction(1,2):
        if (k % 2) != 0: k += 1
    residual = X_dimless - k*U
    rel_error = (abs(residual) / abs(X_dimless)) if X_dimless != 0 else Fraction(0,1)
    return U, k, residual, rel_error

def make_row(**kw):
    kw.setdefault("dna_strategy","k")
    if isinstance(kw.get("dna_k_mods"), dict):
        kw["dna_k_mods"] = json.dumps(kw["dna_k_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_k_mods", json.dumps({}))
    if isinstance(kw.get("dna_pq_mods"), dict):
        kw["dna_pq_mods"] = json.dumps(kw["dna_pq_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_pq_mods", json.dumps({}))
    return kw

def emit_files(rows, csv_path, jsonl_path, md_path, asc_path, ascii_text):
    fieldnames = list(rows[0].keys())
    with open(csv_path, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames)
        w.writeheader(); w.writerows(rows)
    with open(jsonl_path, "w") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False)+"\n")
    with open(md_path, "w") as f:
        f.write("# UPT_DERIVED_v1 — Constraints on Universal Lattice\n\n")
        f.write(f"- Rows: **{len(rows)}**\n- Run: **{RUN_TS}Z**\n\n")
        f.write("| uid | domain | tier | symbol | value_dimless | U_family | p | k | rel_error |\n")
        f.write("|---|---:|---:|---|---:|---|---:|---:|---:|\n")
        for r in rows:
            f.write(f"| {r['uid']} | {r['domain']} | {r['tier']} | {r['symbol']} | {r['value_dimless']} | {r['U_family']} | {r['p_index']} | {r['k']} | {r['rel_error']} |\n")
    with open(asc_path, "w") as f:
        f.write(ascii_text)

# ============= Constraint helper ============================================================================
def add_constraint(rows, *, uid, name, symbol, X_dimless: Fraction, target: Fraction,
                   U_family: str, p_index: int, s_domain: Fraction,
                   domain="Derived", tier=3, definition="", status="derived",
                   calibration_target="", notes=""):
    """Stores deviation = (X - target) as the quantized quantity."""
    deviation = X_dimless - target
    U, k, residual, rel_error = qtz(deviation, U_family, p_index, s_domain)
    dna = fingerprint_k(k)
    pq_num, pq_den = deviation.numerator, deviation.denominator
    hash_core = row_hashes_core(domain, symbol, f"{pq_num}/{pq_den}", pq_num, pq_den)
    version = f"UPT_DERIVED_v1@{RUN_TS}"
    row = make_row(
        uid=uid, domain=domain, tier=tier, name=name, symbol=symbol, definition=definition,
        value_si="", unit_si="",
        value_dimless=f"{pq_num}/{pq_den}", norm_ref="custom",
        pq_num=pq_num, pq_den=pq_den, pq_value=to_float(Fraction(pq_num,pq_den)),
        U_family=U_family, U_scale=f"{s_domain} * U_EM(p)", p_index=p_index,
        k=k, U_value=to_float(U),
        residual=float(to_float(residual)), rel_error=float(to_float(rel_error)),
        bits_pq=bits_pq(pq_num,pq_den), bits_k=bits_int(abs(k)),
        dna_mod_23=dna[23], dna_mod_49=dna[49], dna_mod_50=dna[50], dna_mod_137=dna[137],
        dna_strategy="k", dna_k_mods=dna, dna_pq_mods={},
        provenance="UPT_DERIVED_v1/derived", status=status, calibration_target=calibration_target,
        notes=notes, hash_core=hash_core, version=version
    )
    row["hash_entry"] = row_hash_full(row)
    rows.append(row)

# ============= Populate rows ================================================================================
rows = []

# Meta snapshot: constraint set digest
constraint_defs = [
    # 1) Cosmic closure: Omega_total - 1 = 0 (should be exact)
    {
        "uid":"UPT/Derived/6/closure_dev#DERIVED_v1",
        "name":"Cosmic budget closure deviation",
        "symbol":"closure_dev",
        "X": Fraction(1,1),  # Ω_tot
        "target": Fraction(1,1),  # target = 1
        "U_family":"COS", "p_index":54, "s_domain":Fraction(1,1),
        "tier":6, "definition":"Deviation Ω_tot - 1 quantized on U_COS(p)."
    },
    # 2) Custodial rho (tree): enforce identity ρ^2 - cos^2θ_W = 0 via exact rationals.
    # Set c2W = r2 := 655/843 (any exact fraction works if both sides match).
    {
        "uid":"UPT/Derived/3/rho_tree_dev#DERIVED_v1",
        "name":"Custodial rho (tree) deviation",
        "symbol":"rho_tree_dev",
        "X": Fraction(655,843),     # (MW/MZ)^2 (chosen exact rational)
        "target": Fraction(655,843),# c_W^2 (same exact rational)
        "U_family":"EM", "p_index":55, "s_domain":Fraction(1,1),
        "tier":3, "definition":"Deviation (MW/MZ)^2 − cos²θ_W with both sides set identically (audit identity)."
    },
]

# Emit constraints
for c in constraint_defs:
    add_constraint(
        rows,
        uid=c["uid"], name=c["name"], symbol=c["symbol"],
        X_dimless=c["X"], target=c["target"],
        U_family=c["U_family"], p_index=c["p_index"], s_domain=c["s_domain"],
        domain="Derived", tier=c["tier"],
        definition=c["definition"], status="derived", calibration_target="", notes=""
    )

# Also emit a Meta row that snapshots the constraint set (by digest)
def make_meta_snapshot(constraints):
    digest_obj = []
    for c in constraints:
        digest_obj.append({
            "uid":c["uid"], "symbol":c["symbol"], "tier":c["tier"],
            "U_family":c["U_family"], "p_index":c["p_index"],
            "X":f"{c['X'].numerator}/{c['X'].denominator}",
            "target":f"{c['target'].numerator}/{c['target'].denominator}",
        })
    digest = sha256_str(json.dumps(digest_obj, sort_keys=True))
    # Store digest as a rational by encoding hex->int over a big denominator (or just keep as metadata in notes).
    # Here, we keep X_dimless=0 for a pure Meta marker, with digest in notes.
    uid="UPT/Meta/8/constraints_digest#DERIVED_v1"
    version=f"UPT_DERIVED_v1@{RUN_TS}"
    meta = make_row(
        uid=uid, domain="Meta", tier=8, name="Derived constraints set digest",
        symbol="constraints_digest", definition="SHA-256 of constraint definitions",
        value_si="", unit_si="", value_dimless="0/1", norm_ref="custom",
        pq_num=0, pq_den=1, pq_value=0.0,
        U_family="CUSTOM", U_scale="1 * U_EM(p)", p_index=0,
        k=0, U_value=float(to_float(U_EM(0))), residual=0.0, rel_error=0.0,
        bits_pq=bits_pq(0,1), bits_k=bits_int(0),
        dna_mod_23=0, dna_mod_49=0, dna_mod_50=0, dna_mod_137=0,
        dna_strategy="k", dna_k_mods={}, dna_pq_mods={},
        provenance="UPT_DERIVED_v1/derived", status="calibration",
        calibration_target="", notes=f"digest_sha256={digest}",
        hash_core=row_hashes_core("Meta","constraints_digest","0/1",0,1),
        version=version
    )
    meta["hash_entry"] = row_hash_full(meta)
    return meta

rows.append(make_meta_snapshot(constraint_defs))

# ============= Stdout/ASCII formatting =====================================================================
def fmt_sci(x):
    if isinstance(x, (int,)):
        return str(x)
    try:
        return f"{float(x):.3E}"
    except Exception:
        return str(x)

def ascii_payload(rows, defs):
    lines = []
    lines.append("="*92+"\n")
    lines.append("DERIVED CONSTRAINTS ON UNIVERSAL LATTICE  U(p)=1/(49·50·137^p)\n")
    lines.append("="*92+"\n")

    # LIST
    lines.append("[BEGIN SECTION:constraints]\n")
    for c in defs:
        lines.append(f"[C] {c['symbol']}: domain tier={c['tier']}, U={c['U_family']}, p={c['p_index']}, X={c['X']} target={c['target']}\n")
    digest = sha256_str(json.dumps([
        {"symbol":c["symbol"],"tier":c["tier"],"U":c["U_family"],"p":c["p_index"],
         "X":f"{c['X'].numerator}/{c['X'].denominator}",
         "target":f"{c['target'].numerator}/{c['target'].denominator}"} for c in defs
    ], sort_keys=True))
    lines.append(f"[KV] constraints_digest_sha256 = {digest}\n")
    lines.append("[END SECTION:constraints]\n\n")

    # TARGET TABLE
    lines.append("="*92+"\n")
    lines.append("DERIVED TARGETS (deviations)\n")
    lines.append("="*92+"\n")
    lines.append("[BEGIN SECTION:targets]\n")
    lines.append("symbol           p     k                          U(p)              deviation          residual            rel_error   \n")
    lines.append("----------------------------------------------------------------------------------------------------------\n")
    for r in rows:
        if r["domain"] != "Derived": continue
        lines.append(f"{r['symbol']:<16} {r['p_index']:>3}  {str(r['k']):>24}  ")
        lines.append(f"{r['U_value']:<16}  {fmt_sci(r['pq_value']):<18}  {fmt_sci(r['residual']):<18}  {fmt_sci(r['rel_error'])}\n")
    lines.append("[END SECTION:targets]\n\n")

    # ARTIFACTS placeholder
    lines.append("="*92+"\n")
    lines.append("EMIT ARTIFACTS\n")
    lines.append("="*92+"\n")
    lines.append("[BEGIN SECTION:write_files]\n")
    return "".join(lines)

ascii_text = ascii_payload(rows, constraint_defs)

# ============= Emit files & print ==========================================================================
def finalize_and_print(rows, ascii_text):
    lines = [ascii_text]
    lines.append(f"[KV] CSV = {CSV_PATH}\n")
    lines.append(f"[KV] JSONL = {JSONL_PATH}\n")
    lines.append(f"[KV] MD = {MD_PATH}\n")
    lines.append(f"[KV] ASCII = {ASC_PATH}\n")
    lines.append("[END SECTION:write_files]\n\n")

    reg_digest = sha256_str(json.dumps(rows, sort_keys=True))
    lines.append("="*92+"\n")
    lines.append("REGISTRY DIGEST & SNAPSHOT\n")
    lines.append("="*92+"\n")
    lines.append("[BEGIN SECTION:digest]\n")
    lines.append(f"[KV] rows = {len(rows)}\n")
    lines.append(f"[KV] registry_digest_sha256 = {reg_digest}\n")
    lines.append("uid                                     domain     tier  symbol         value_dimless       U_famil  p_index  k             rel_error         \n")
    lines.append("-------------------------------------------------------------------------------------------------------------------------------------\n")
    for r in rows:
        lines.append(f"{r['uid']:<40} {r['domain']:<10} {r['tier']:<5} {r['symbol']:<13} {r['value_dimless']:<18} {r['U_family']:<7} {r['p_index']:<7} {str(r['k'])[:12]:<12} {fmt_sci(r['rel_error'])}\n")
    lines.append("[END SECTION:digest]\n\n")

    lines.append("="*92+"\n")
    lines.append("UPT_DERIVED_v1 — COMPLETE\n")
    lines.append("="*92+"\n")
    return "".join(lines)

ascii_full = finalize_and_print(rows, ascii_text)
emit_files(rows, CSV_PATH, JSONL_PATH, MD_PATH, ASC_PATH, ascii_full)
print(ascii_full, end="")

# =========================== UPT_META_v1 — Snapshots, s_D table, and integrity hashes ======================
# Emits:
#  • s_EM, s_QCD, s_G, s_COS, s_INFO snapshots (rational) in Meta domain
#  • Per-file SHA-256 digests for all UPT_master_* artifacts + aggregate ledger digest
#  • Environment hash row (python, platform, tz-aware timestamp)
# ===========================================================================================================

import os, csv, json, hashlib, warnings, platform, sys, glob
from fractions import Fraction
import datetime as dt

# Silence deprecation warnings globally
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Timezone-aware run timestamp
RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

# IO
OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)

CSV_PATH   = os.path.join(OUT_DIR,   f"UPT_master_META_{RUN_TS}.csv")
JSONL_PATH = os.path.join(OUT_DIR,   f"UPT_master_META_{RUN_TS}.jsonl")
MD_PATH    = os.path.join(REPORT_DIR,f"UPT_report_META_{RUN_TS}.md")
ASC_PATH   = os.path.join(REPORT_DIR,f"UPT_ascii_META_{RUN_TS}.txt")

# --------------------------- Helpers (aligned with prior modules) -------------------------------------------
def sha256_bytes(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()

def sha256_str(s: str) -> str:
    return sha256_bytes(s.encode("utf-8"))

def bits_int(n: int) -> int:
    return 1 if n == 0 else n.bit_length()

def bits_pq(p: int, q: int) -> int:
    return bits_int(abs(p)) + bits_int(abs(q))

def U_EM(p: int) -> Fraction:
    return Fraction(1, 49*50*137**p)

def to_float(fr: Fraction) -> float:
    return fr.numerator / fr.denominator

def fingerprint_k(k: int, mods=(23,49,50,137)):
    return {m: (k % m) for m in mods}

def row_hashes_core(domain, symbol, value_dimless, pq_num, pq_den):
    core = f"{domain}|{symbol}|{value_dimless}|{pq_num}|{pq_den}"
    return sha256_str(core)

def row_hash_full(row: dict):
    ordered_keys = [
        "uid","domain","tier","name","symbol","definition",
        "value_si","unit_si","value_dimless","norm_ref",
        "pq_num","pq_den","pq_value",
        "U_family","U_scale","p_index","k","U_value",
        "residual","rel_error","bits_pq","bits_k",
        "dna_mod_23","dna_mod_49","dna_mod_50","dna_mod_137",
        "dna_strategy","dna_k_mods","dna_pq_mods",
        "provenance","status","calibration_target",
        "notes","hash_core","version"
    ]
    payload = "|".join(str(row.get(k,"")) for k in ordered_keys)
    return sha256_str(payload)

def make_row(**kw):
    # normalize optional json fields
    import json as _json
    kw.setdefault("dna_strategy","k")
    if isinstance(kw.get("dna_k_mods"), dict):
        kw["dna_k_mods"] = _json.dumps(kw["dna_k_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_k_mods", _json.dumps({}))
    if isinstance(kw.get("dna_pq_mods"), dict):
        kw["dna_pq_mods"] = _json.dumps(kw["dna_pq_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_pq_mods", _json.dumps({}))
    return kw

def emit_files(rows, csv_path, jsonl_path, md_path, asc_path, ascii_text):
    fieldnames = list(rows[0].keys())
    with open(csv_path, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames); w.writeheader(); w.writerows(rows)
    with open(jsonl_path, "w") as f:
        for r in rows: f.write(json.dumps(r, ensure_ascii=False)+"\n")
    with open(md_path, "w") as f:
        f.write("# UPT_META_v1 — Snapshots & Integrity Hashes\n\n")
        f.write(f"- Rows: **{len(rows)}**\n- Run: **{RUN_TS}Z**\n\n")
        f.write("| uid | domain | tier | symbol | value_dimless | note |\n")
        f.write("|---|---:|---:|---|---:|---|\n")
        for r in rows:
            f.write(f"| {r['uid']} | {r['domain']} | {r['tier']} | {r['symbol']} | {r['value_dimless']} | {r.get('notes','')} |\n")
    with open(asc_path, "w") as f:
        f.write(ascii_text)

# --------------------------- Build rows ---------------------------------------------------------------------
rows = []

# 1) s_D snapshot rows (rational, default 1/1 unless later calibrated)
S_DOMAINS = [
    ("EM",  "UPT/Meta/8/s_EM_snapshot#META_v1",  Fraction(1,1), "base lattice"),
    ("QCD", "UPT/Meta/8/s_QCD_snapshot#META_v1", Fraction(1,1), "from anchors in QCD_v1"),
    ("G",   "UPT/Meta/8/s_G_snapshot#META_v1",   Fraction(1,1), "tunable; calibrate on BH ladders"),
    ("COS", "UPT/Meta/8/s_COS_snapshot#META_v1", Fraction(1,1), "from anchors in COS_v1"),
    ("INFO","UPT/Meta/8/s_INFO_snapshot#META_v1",Fraction(1,1), "info lattice; TBD"),
]

for domain_tag, uid, sval, note in S_DOMAINS:
    pq_num, pq_den = sval.numerator, sval.denominator
    version = f"UPT_META_v1@{RUN_TS}"
    dna_k = fingerprint_k(0)
    row = make_row(
        uid=uid, domain="Meta", tier=8, name=f"s_{domain_tag} snapshot",
        symbol=f"s_{domain_tag}", definition=f"Domain multiplier for {domain_tag}",
        value_si="", unit_si="", value_dimless=f"{pq_num}/{pq_den}", norm_ref="custom",
        pq_num=pq_num, pq_den=pq_den, pq_value=to_float(sval),
        U_family=domain_tag, U_scale="s_D * U_EM(p)", p_index=0,
        k=0, U_value=float(to_float(U_EM(0))), residual=0.0, rel_error=0.0,
        bits_pq=bits_pq(pq_num,pq_den), bits_k=bits_int(0),
        dna_mod_23=dna_k[23], dna_mod_49=dna_k[49], dna_mod_50=dna_k[50], dna_mod_137=dna_k[137],
        dna_strategy="k", dna_k_mods=dna_k, dna_pq_mods={},
        provenance="UPT_META_v1/snap", status="calibration",
        calibration_target=f"s_{domain_tag}", notes=note,
        hash_core=row_hashes_core("Meta", f"s_{domain_tag}", f"{pq_num}/{pq_den}", pq_num, pq_den),
        version=version
    )
    row["hash_entry"] = row_hash_full(row)
    rows.append(row)

# 2) Ledger file digests (scan /content for UPT_master_* files)
def file_digest(path):
    try:
        with open(path, "rb") as f: return sha256_bytes(f.read())
    except Exception:
        return None

master_files = sorted(glob.glob(os.path.join(OUT_DIR, "UPT_master_*")))
file_entries = []
for fp in master_files:
    h = file_digest(fp)
    if h:
        file_entries.append({"file": os.path.basename(fp), "sha256": h})

# aggregate digest over filenames+hashes in stable order
agg_payload = json.dumps(file_entries, sort_keys=True)
agg_digest = sha256_str(agg_payload)

uid_files = "UPT/Meta/8/ledger_files_digest#META_v1"
row_files = make_row(
    uid=uid_files, domain="Meta", tier=8, name="Ledger file digests",
    symbol="ledger_files_digest", definition="SHA-256 per master artifact and aggregate digest",
    value_si="", unit_si="", value_dimless="0/1", norm_ref="custom",
    pq_num=0, pq_den=1, pq_value=0.0,
    U_family="CUSTOM", U_scale="1 * U_EM(p)", p_index=0,
    k=0, U_value=float(to_float(U_EM(0))), residual=0.0, rel_error=0.0,
    bits_pq=bits_pq(0,1), bits_k=bits_int(0),
    dna_mod_23=0, dna_mod_49=0, dna_mod_50=0, dna_mod_137=0,
    dna_strategy="k", dna_k_mods={}, dna_pq_mods={},
    provenance="UPT_META_v1/digest", status="measured",
    calibration_target="", notes=f"aggregate_sha256={agg_digest}; files={len(file_entries)}",
    hash_core=row_hashes_core("Meta","ledger_files_digest","0/1",0,1),
    version=f"UPT_META_v1@{RUN_TS}"
)
row_files["hash_entry"] = row_hash_full(row_files)
rows.append(row_files)

# 3) Environment hash row
env_info = {
    "python": sys.version.split()[0],
    "platform": platform.platform(),
    "implementation": platform.python_implementation(),
}
env_digest = sha256_str(json.dumps(env_info, sort_keys=True))
uid_env = "UPT/Meta/8/env_hash#META_v1"
row_env = make_row(
    uid=uid_env, domain="Meta", tier=8, name="Environment hash",
    symbol="env_hash", definition="SHA-256 of minimal environment descriptor",
    value_si="", unit_si="", value_dimless="0/1", norm_ref="custom",
    pq_num=0, pq_den=1, pq_value=0.0,
    U_family="CUSTOM", U_scale="1 * U_EM(p)", p_index=0,
    k=0, U_value=float(to_float(U_EM(0))), residual=0.0, rel_error=0.0,
    bits_pq=bits_pq(0,1), bits_k=bits_int(0),
    dna_mod_23=0, dna_mod_49=0, dna_mod_50=0, dna_mod_137=0,
    dna_strategy="k", dna_k_mods={}, dna_pq_mods={},
    provenance="UPT_META_v1/env", status="measured",
    calibration_target="", notes=f"env_sha256={env_digest}; {env_info}",
    hash_core=row_hashes_core("Meta","env_hash","0/1",0,1),
    version=f"UPT_META_v1@{RUN_TS}"
)
row_env["hash_entry"] = row_hash_full(row_env)
rows.append(row_env)

# --------------------------- ASCII report -------------------------------------------------------------------
def ascii_report(rows, file_entries, agg_digest):
    def fmt_sci(x):
        try: return f"{float(x):.3E}"
        except: return str(x)
    lines = []
    lines.append("="*92+"\n")
    lines.append("META SNAPSHOTS & INTEGRITY (s_D, file digests, env)\n")
    lines.append("="*92+"\n")

    # s_D table
    lines.append("[BEGIN SECTION:s_domain]\n")
    lines.append("domain  s_D(rational)   approx         bits    note\n")
    lines.append("------------------------------------------------------------\n")
    for r in rows:
        if r["symbol"].startswith("s_"):
            approx = f"{r['pq_num']}/{r['pq_den']}"
            lines.append(f"{r['symbol'][2:]:<7} {approx:<15} {fmt_sci(r['pq_value']):<13} {r['bits_pq']:<7} {r['notes']}\n")
    lines.append("[END SECTION:s_domain]\n\n")

    # File digests
    lines.append("="*92+"\n")
    lines.append("LEDGER FILE DIGESTS\n")
    lines.append("="*92+"\n")
    lines.append("[BEGIN SECTION:ledger_digests]\n")
    for item in file_entries:
        lines.append(f"{item['file']:<45}  sha256={item['sha256']}\n")
    lines.append(f"[KV] aggregate_sha256 = {agg_digest}\n")
    lines.append("[END SECTION:ledger_digests]\n\n")

    # Env
    lines.append("="*92+"\n")
    lines.append("ENVIRONMENT HASH\n")
    lines.append("="*92+"\n")
    lines.append("[BEGIN SECTION:env]\n")
    lines.append(f"[KV] env_sha256 = {row_env['notes']}\n")
    lines.append("[END SECTION:env]\n\n")

    # Artifacts placeholder footer
    lines.append("="*92+"\n")
    lines.append("EMIT ARTIFACTS\n")
    lines.append("="*92+"\n")
    lines.append("[BEGIN SECTION:write_files]\n")
    return "".join(lines)

asc_core = ascii_report(rows, file_entries, agg_digest)

# --------------------------- Emit & print -------------------------------------------------------------------
def finalize_and_print(rows, asc_core):
    lines = [asc_core]
    lines.append(f"[KV] CSV = {CSV_PATH}\n")
    lines.append(f"[KV] JSONL = {JSONL_PATH}\n")
    lines.append(f"[KV] MD = {MD_PATH}\n")
    lines.append(f"[KV] ASCII = {ASC_PATH}\n")
    lines.append("[END SECTION:write_files]\n\n")

    reg_digest = sha256_str(json.dumps(rows, sort_keys=True))
    lines.append("="*92+"\n")
    lines.append("REGISTRY DIGEST & SNAPSHOT\n")
    lines.append("="*92+"\n")
    lines.append("[BEGIN SECTION:digest]\n")
    lines.append(f"[KV] rows = {len(rows)}\n")
    lines.append(f"[KV] registry_digest_sha256 = {reg_digest}\n")
    lines.append("uid                                     domain  tier  symbol                value_dimless  note\n")
    lines.append("--------------------------------------------------------------------------------------------------------------------\n")
    for r in rows:
        lines.append(f"{r['uid']:<40} {r['domain']:<7} {r['tier']:<5} {r['symbol']:<20} {r['value_dimless']:<14} {r.get('notes','')}\n")
    lines.append("[END SECTION:digest]\n\n")

    lines.append("="*92+"\n")
    lines.append("UPT_META_v1 — COMPLETE\n")
    lines.append("="*92+"\n")
    return "".join(lines)

ascii_full = finalize_and_print(rows, asc_core)
emit_files(rows, CSV_PATH, JSONL_PATH, MD_PATH, ASC_PATH, ascii_full)
print(ascii_full, end="")

# =============================== UPT_MIX_CKM_v1 — Unitary CKM on Universal Lattice ==========================
# Uses exact rational sines (from your 19-fractions registry) and PDG parameterization.
# Emits |V_ij| targets + Jarlskog J, and unitarity constraints as deviation rows, all lattice-quantized.
# ===========================================================================================================

import os, csv, json, math, hashlib, warnings, glob, platform, sys
from fractions import Fraction
import datetime as dt

# Silence deprecation warnings globally
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Timezone-aware run timestamp
RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)

CSV_PATH   = os.path.join(OUT_DIR,    f"UPT_master_MIX_CKM_{RUN_TS}.csv")
JSONL_PATH = os.path.join(OUT_DIR,    f"UPT_master_MIX_CKM_{RUN_TS}.jsonl")
MD_PATH    = os.path.join(REPORT_DIR, f"UPT_report_MIX_CKM_{RUN_TS}.md")
ASC_PATH   = os.path.join(REPORT_DIR, f"UPT_ascii_MIX_CKM_{RUN_TS}.txt")

# ------------------------ Helpers (aligned with previous modules) -------------------------------------------
def sha256_bytes(b: bytes) -> str: return hashlib.sha256(b).hexdigest()
def sha256_str(s: str) -> str: return sha256_bytes(s.encode("utf-8"))

def bits_int(n: int) -> int: return 1 if n == 0 else int(abs(n)).bit_length()
def bits_pq(p: int, q: int) -> int: return bits_int(abs(p)) + bits_int(abs(q))

def U_EM(p: int) -> Fraction:
    return Fraction(1, 49*50*137**p)

def quantize(x: Fraction | float, p_index: int) -> dict:
    # accept Fraction or float; quantize against U_EM(p)
    if isinstance(x, Fraction):
        xd = x.numerator / x.denominator
    else:
        xd = float(x)
    U = U_EM(p_index)
    U_float = U.numerator / U.denominator
    k = int(round(xd / U_float))
    residual = xd - k*U_float
    rel_error = 0.0 if xd == 0 else abs(residual)/abs(xd)
    return dict(U_value=U_float, k=k, residual=residual, rel_error=rel_error)

def fingerprint_k(k: int, mods=(23,49,50,137)):
    return {m: (k % m) for m in mods}

def to_float(fr: Fraction) -> float:
    return fr.numerator / fr.denominator

def row_hashes_core(domain, symbol, value_dimless, pq_num, pq_den):
    core = f"{domain}|{symbol}|{value_dimless}|{pq_num}|{pq_den}"
    return sha256_str(core)

def row_hash_full(row: dict):
    ordered_keys = [
        "uid","domain","tier","name","symbol","definition",
        "value_si","unit_si","value_dimless","norm_ref",
        "pq_num","pq_den","pq_value",
        "U_family","U_scale","p_index","k","U_value",
        "residual","rel_error","bits_pq","bits_k",
        "dna_mod_23","dna_mod_49","dna_mod_50","dna_mod_137",
        "dna_strategy","dna_k_mods","dna_pq_mods",
        "provenance","status","calibration_target",
        "notes","hash_core","version"
    ]
    payload = "|".join(str(row.get(k,"")) for k in ordered_keys)
    return sha256_str(payload)

def make_row(**kw):
    import json as _json
    kw.setdefault("dna_strategy","k")
    if isinstance(kw.get("dna_k_mods"), dict):
        kw["dna_k_mods"] = _json.dumps(kw["dna_k_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_k_mods", _json.dumps({}))
    if isinstance(kw.get("dna_pq_mods"), dict):
        kw["dna_pq_mods"] = _json.dumps(kw["dna_pq_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_pq_mods", _json.dumps({}))
    return kw

# ------------------------ CKM parameters (exact rationals + phase) ------------------------------------------
# s12, s13, s23 as Fractions; delta = (delta_over_pi)*pi with rational delta/pi
s12 = Fraction(13482, 60107)
s13 = Fraction(1913, 485533)
s23 = Fraction(6419, 152109)
delta_over_pi = Fraction(6869, 17983)

c12 = Fraction(1,1) - s12*s12
c13 = Fraction(1,1) - s13*s13
c23 = Fraction(1,1) - s23*s23
# keep cosines as floats from the exact rationals to preserve unitarity numerically
c12 = math.sqrt(to_float(c12))
c13 = math.sqrt(to_float(c13))
c23 = math.sqrt(to_float(c23))

s12f, s13f, s23f = to_float(s12), to_float(s13), to_float(s23)
delta = float(delta_over_pi) * math.pi
cd, sd = math.cos(delta), math.sin(delta)

# PDG parameterization (standard)
Vud =  c12*c13
Vus =  s12*c13
Vub =  s13*complex(math.cos(delta), -math.sin(delta))  # e^{-i delta}
Vcd = -s12*c23 - c12*s23*s13*complex(math.cos(delta), math.sin(delta))
Vcs =  c12*c23 - s12*s23*s13*complex(math.cos(delta), math.sin(delta))
Vcb =  s23*c13
Vtd =  s23*s12 - c12*c23*s13*complex(math.cos(delta), math.sin(delta))
Vts = -s23*c12 - s12*c23*s13*complex(math.cos(delta), math.sin(delta))
Vtb =  c23*c13

# Build magnitudes
V = [
    [Vud, Vus, Vub],
    [Vcd, Vcs, Vcb],
    [Vtd, Vts, Vtb],
]
Vabs = [[abs(z) for z in row] for row in V]

# Jarlskog invariant J = c12 c23 c13^2 s12 s23 s13 sin(delta)
J = c12*c23*(c13**2)*s12f*s23f*s13f*sd

# Unitarity checks
import numpy as _np
Vnp = _np.array(V, dtype=complex)
U1 = Vnp @ Vnp.conjugate().T
U2 = Vnp.conjugate().T @ Vnp
dev1 = U1 - _np.eye(3)
dev2 = U2 - _np.eye(3)

# ------------------------ Emit rows -------------------------------------------------------------------------
rows = []
version = f"UPT_MIX_CKM_v1@{RUN_TS}"

def emit_target(symbol, name, x_value, p_index=55, tier=1, notes=""):
    # x_value is a *dimensionless* float/Fraction
    if isinstance(x_value, Fraction):
        pq_num, pq_den = x_value.numerator, x_value.denominator
        pq_val = to_float(x_value)
        value_dimless = f"{pq_num}/{pq_den}"
        bits = bits_pq(pq_num, pq_den)
    else:
        # store as rational via Fraction(x).limit_denominator for core, but keep original float in notes
        fr = Fraction(x_value).limit_denominator(10**12)
        pq_num, pq_den = fr.numerator, fr.denominator
        pq_val = to_float(fr)
        value_dimless = f"{pq_num}/{pq_den}"
        bits = bits_pq(pq_num, pq_den)

    q = quantize(pq_val, p_index)
    dna = fingerprint_k(q["k"])
    row = make_row(
        uid=f"UPT/MIX/1/{symbol}#MIX_CKM_v1",
        domain="SM", tier=tier, name=name, symbol=symbol,
        definition="CKM magnitude or invariant", value_si="", unit_si="",
        value_dimless=value_dimless, norm_ref="natural",
        pq_num=pq_num, pq_den=pq_den, pq_value=pq_val,
        U_family="EM", U_scale=f"U_EM({p_index})", p_index=p_index,
        k=q["k"], U_value=q["U_value"], residual=q["residual"], rel_error=q["rel_error"],
        bits_pq=bits, bits_k=bits_int(q["k"]),
        dna_mod_23=dna[23], dna_mod_49=dna[49], dna_mod_50=dna[50], dna_mod_137=dna[137],
        dna_strategy="k", dna_k_mods=dna, dna_pq_mods={},
        provenance="UPT_MIX_CKM_v1/build", status="measured",
        calibration_target="", notes=notes,
        hash_core=row_hashes_core("SM", symbol, value_dimless, pq_num, pq_den),
        version=version
    )
    row["hash_entry"] = row_hash_full(row)
    rows.append(row)

# Targets: nine |V_ij| (p=55) and J (p=56)
labels = [["Vud","|V_ud|"],["Vus","|V_us|"],["Vub","|V_ub|"],
          ["Vcd","|V_cd|"],["Vcs","|V_cs|"],["Vcb","|V_cb|"],
          ["Vtd","|V_td|"],["Vts","|V_ts|"],["Vtb","|V_tb|"]]
for (sym, nm), val in zip(labels, sum(Vabs, [])):
    emit_target(sym, nm, val, p_index=55, tier=1)

emit_target("JarlskogJ", "Jarlskog invariant J", J, p_index=56, tier=2, notes="unitarity-invariant area/2 of UT")

# Constraints: unitarity deviations (real & imag of off-diagonals)
def emit_constraint(symbol, name, deviation, p_index=56, tier=3, Ufam="EM"):
    # deviation is a (small) complex → store magnitude as value_dimless, zero target
    mag = abs(deviation)
    fr = Fraction(mag).limit_denominator(10**18)
    pq_num, pq_den = fr.numerator, fr.denominator
    pq_val = to_float(fr)
    q = quantize(pq_val, p_index)
    dna = fingerprint_k(q["k"])
    row = make_row(
        uid=f"UPT/Derived/3/{symbol}#MIX_CKM_v1",
        domain="Derived", tier=tier, name=name, symbol=symbol,
        definition="Deviation from exact unitarity (target=0)", value_si="", unit_si="",
        value_dimless=f"{pq_num}/{pq_den}", norm_ref="natural",
        pq_num=pq_num, pq_den=pq_den, pq_value=pq_val,
        U_family=Ufam, U_scale=f"U_{Ufam}({p_index})", p_index=p_index,
        k=q["k"], U_value=q["U_value"], residual=q["residual"], rel_error=(0.0 if pq_val==0 else abs(q["residual"])/pq_val),
        bits_pq=bits_pq(pq_num,pq_den), bits_k=bits_int(q["k"]),
        dna_mod_23=dna[23], dna_mod_49=dna[49], dna_mod_50=dna[50], dna_mod_137=dna[137],
        dna_strategy="k", dna_k_mods=dna, dna_pq_mods={},
        provenance="UPT_MIX_CKM_v1/unitarity", status="derived",
        calibration_target="", notes="target=0",
        hash_core=row_hashes_core("Derived", symbol, f"{pq_num}/{pq_den}", pq_num, pq_den),
        version=version
    )
    row["hash_entry"] = row_hash_full(row)
    rows.append(row)

# Off-diagonal mags from both VV† and V†V
for tag, DEV in [("VVdag",dev1), ("VdagV",dev2)]:
    for i in range(3):
        for j in range(3):
            if i==j:
                # also store diagonal deviation from 1 as constraint
                emit_constraint(f"{tag}_diag_{i}", f"{tag}[{i},{i}]−1", DEV[i,j], p_index=56, tier=3)
            else:
                emit_constraint(f"{tag}_off_{i}{j}", f"{tag}[{i},{j}]", DEV[i,j], p_index=56, tier=3)

# ------------------------ ASCII report ----------------------------------------------------------------------
def ascii_report():
    lines = []
    lines.append("="*100 + "\n")
    lines.append("CKM ON UNIVERSAL LATTICE  U(p)=1/(49·50·137^p)\n")
    lines.append("="*100 + "\n")

    lines.append("[BEGIN SECTION:inputs]\n")
    lines.append(f"s12={s12}  s13={s13}  s23={s23}  delta/pi={delta_over_pi}\n")
    lines.append("[END SECTION:inputs]\n\n")

    lines.append("CKM magnitudes (targets):\n")
    lines.append("symbol   p    k                         U(p)              |V_ij|            residual           rel_error\n")
    lines.append("-"*106 + "\n")
    # replay first 9 rows in order
    for r in rows[:9]:
        lines.append(f"{r['symbol']:<8} {r['p_index']:<4} {r['k']:<25} {r['U_value']:<18.12e} {r['pq_value']:<16.12e} {r['residual']:<18.3e} {r['rel_error']:<.3e}\n")
    lines.append("\nJarlskog:\n")
    Jrow = rows[9]
    lines.append(f"J={Jrow['pq_value']:.12e}  (p={Jrow['p_index']}, k={Jrow['k']}, U={Jrow['U_value']:.12e}, residual={Jrow['residual']:.3e})\n")

    # summarize unitarity max deviations
    dev_mags = [float(r['pq_value']) for r in rows[10:]]
    lines.append("\nUnitarity deviations (summary over all constraints):\n")
    lines.append(f"max|dev|≈{max(dev_mags):.3e},  median|dev|≈{_np.median(dev_mags):.3e}\n")

    lines.append("\n" + "="*100 + "\n")
    lines.append("EMIT ARTIFACTS\n")
    lines.append("="*100 + "\n")
    lines.append("[BEGIN SECTION:write_files]\n")
    return "".join(lines)

asc_core = ascii_report()

# ------------------------ Emit files ------------------------------------------------------------------------
def emit_files(rows, csv_path, jsonl_path, md_path, asc_path, ascii_text):
    fieldnames = list(rows[0].keys())
    with open(csv_path, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames); w.writeheader(); w.writerows(rows)
    with open(jsonl_path, "w") as f:
        for r in rows: f.write(json.dumps(r, ensure_ascii=False)+"\n")
    with open(md_path, "w") as f:
        f.write("# UPT_MIX_CKM_v1 — CKM on universal lattice\n\n")
        f.write(f"- Rows: **{len(rows)}**\n- Run: **{RUN_TS}Z**\n\n")
        f.write("## First 10 rows (preview)\n\n")
        f.write("| uid | domain | tier | symbol | value_dimless | k | p |\n|---|---:|---:|---|---:|---:|---:|\n")
        for r in rows[:10]:
            f.write(f"| {r['uid']} | {r['domain']} | {r['tier']} | {r['symbol']} | {r['value_dimless']} | {r['k']} | {r['p_index']} |\n")
    with open(asc_path, "w") as f:
        f.write(ascii_text)

# finalize ASCII
def finalize_and_print(rows, asc_core):
    lines = [asc_core]
    lines.append(f"[KV] CSV = {CSV_PATH}\n")
    lines.append(f"[KV] JSONL = {JSONL_PATH}\n")
    lines.append(f"[KV] MD = {MD_PATH}\n")
    lines.append(f"[KV] ASCII = {ASC_PATH}\n")
    lines.append("[END SECTION:write_files]\n\n")

    reg_digest = sha256_str(json.dumps(rows, sort_keys=True))
    lines.append("="*100 + "\n")
    lines.append("REGISTRY DIGEST & SNAPSHOT\n")
    lines.append("="*100 + "\n")
    lines.append("[BEGIN SECTION:digest]\n")
    lines.append(f"[KV] rows = {len(rows)}\n")
    lines.append(f"[KV] registry_digest_sha256 = {reg_digest}\n")
    lines.append("uid                                     domain  tier  symbol      value_dimless        U_famil  p_index  k            rel_error\n")
    lines.append("-"*118 + "\n")
    for r in rows[:16]:
        lines.append(f"{r['uid']:<40} {r['domain']:<7} {r['tier']:<5} {r['symbol']:<10} {r['value_dimless']:<18} {r['U_family']:<6} {r['p_index']:<7} {r['k']:<12} {r['rel_error']:<.3e}\n")
    lines.append("[END SECTION:digest]\n\n")

    lines.append("="*100 + "\n")
    lines.append("UPT_MIX_CKM_v1 — COMPLETE\n")
    lines.append("="*100 + "\n")
    return "".join(lines)

ascii_full = finalize_and_print(rows, asc_core)
emit_files(rows, CSV_PATH, JSONL_PATH, MD_PATH, ASC_PATH, ascii_full)
print(ascii_full, end="")

# =============================== UPT_MIX_PMNS_v1 — Unitary PMNS on Universal Lattice =========================
# Exact-rational sines picked to match your earlier small-denominator PMNS magnitudes.
# Emits |U_{αi}| targets + Jarlskog J_PMNS, and unitarity constraints as deviation rows, all lattice-quantized.
# ============================================================================================================

import os, csv, json, math, hashlib, warnings
from fractions import Fraction
import numpy as _np
import datetime as dt

# Silence deprecation warnings globally
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Timezone-aware run timestamp
RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)

CSV_PATH   = os.path.join(OUT_DIR,    f"UPT_master_MIX_PMNS_{RUN_TS}.csv")
JSONL_PATH = os.path.join(OUT_DIR,    f"UPT_master_MIX_PMNS_{RUN_TS}.jsonl")
MD_PATH    = os.path.join(REPORT_DIR, f"UPT_report_MIX_PMNS_{RUN_TS}.md")
ASC_PATH   = os.path.join(REPORT_DIR, f"UPT_ascii_MIX_PMNS_{RUN_TS}.txt")

# ------------------------ Helpers (aligned with CKM module) -------------------------------------------------
def sha256_bytes(b: bytes) -> str: return hashlib.sha256(b).hexdigest()
def sha256_str(s: str) -> str: return sha256_bytes(s.encode("utf-8"))

def bits_int(n: int) -> int: return 1 if n == 0 else int(abs(n)).bit_length()
def bits_pq(p: int, q: int) -> int: return bits_int(abs(p)) + bits_int(abs(q))

def U_EM(p: int) -> Fraction:
    return Fraction(1, 49*50*137**p)

def quantize(x: Fraction | float, p_index: int) -> dict:
    if isinstance(x, Fraction):
        xd = x.numerator / x.denominator
    else:
        xd = float(x)
    U = U_EM(p_index)
    U_float = U.numerator / U.denominator
    k = int(round(xd / U_float))
    residual = xd - k*U_float
    rel_error = 0.0 if xd == 0 else abs(residual)/abs(xd)
    return dict(U_value=U_float, k=k, residual=residual, rel_error=rel_error)

def fingerprint_k(k: int, mods=(23,49,50,137)):
    return {m: (k % m) for m in mods}

def to_float(fr: Fraction) -> float:
    return fr.numerator / fr.denominator

def row_hashes_core(domain, symbol, value_dimless, pq_num, pq_den):
    core = f"{domain}|{symbol}|{value_dimless}|{pq_num}|{pq_den}"
    return sha256_str(core)

def row_hash_full(row: dict):
    ordered_keys = [
        "uid","domain","tier","name","symbol","definition",
        "value_si","unit_si","value_dimless","norm_ref",
        "pq_num","pq_den","pq_value",
        "U_family","U_scale","p_index","k","U_value",
        "residual","rel_error","bits_pq","bits_k",
        "dna_mod_23","dna_mod_49","dna_mod_50","dna_mod_137",
        "dna_strategy","dna_k_mods","dna_pq_mods",
        "provenance","status","calibration_target",
        "notes","hash_core","version"
    ]
    payload = "|".join(str(row.get(k,"")) for k in ordered_keys)
    return sha256_str(payload)

def make_row(**kw):
    import json as _json
    kw.setdefault("dna_strategy","k")
    if isinstance(kw.get("dna_k_mods"), dict):
        kw["dna_k_mods"] = _json.dumps(kw["dna_k_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_k_mods", _json.dumps({}))
    if isinstance(kw.get("dna_pq_mods"), dict):
        kw["dna_pq_mods"] = _json.dumps(kw["dna_pq_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_pq_mods", _json.dumps({}))
    return kw

# ------------------------ PMNS seeds (exact rationals + phase) ----------------------------------------------
# Chosen to match your printed |U| fits:
# |U_e3| ~ 31/209, |U_μ3| ~ 687/941 ~ s23*c13 => s23 ~ 59/80, and s12 tuned for |U_e1|, |U_e2|
s13 = Fraction(31, 209)
s23 = Fraction(59, 80)
s12 = Fraction(199, 359)
delta_over_pi = Fraction(4, 5)  # 144°

# Cosines from exact rationals (float for numeric stability)
c13 = math.sqrt(1.0 - to_float(s13*s13))
c23 = math.sqrt(1.0 - to_float(s23*s23))
c12 = math.sqrt(1.0 - to_float(s12*s12))

s12f, s13f, s23f = to_float(s12), to_float(s13), to_float(s23)
delta = float(delta_over_pi) * math.pi
cd, sd = math.cos(delta), math.sin(delta)

# PDG PMNS (Dirac phase only; Majorana phases omitted for oscillations)
Ue1 =  c12*c13
Ue2 =  s12*c13
Ue3 =  s13*complex(math.cos(delta), -math.sin(delta))  # e^{-iδ}
Um1 = -s12*c23 - c12*s23*s13*complex(math.cos(delta), math.sin(delta))
Um2 =  c12*c23 - s12*s23*s13*complex(math.cos(delta), math.sin(delta))
Um3 =  s23*c13
Ut1 =  s12*s23 - c12*c23*s13*complex(math.cos(delta), math.sin(delta))
Ut2 = -s23*c12 - s12*c23*s13*complex(math.cos(delta), math.sin(delta))
Ut3 =  c23*c13

U = [
    [Ue1, Ue2, Ue3],
    [Um1, Um2, Um3],
    [Ut1, Ut2, Ut3],
]
Uabs = [[abs(z) for z in row] for row in U]

# Jarlskog for leptons
J_pmns = c12*c23*(c13**2)*s12f*s23f*s13f*sd

# Unitarity checks
Unp = _np.array(U, dtype=complex)
VV = Unp @ Unp.conjugate().T
VtV = Unp.conjugate().T @ Unp
dev1 = VV - _np.eye(3)
dev2 = VtV - _np.eye(3)

# ------------------------ Emit rows -------------------------------------------------------------------------
rows = []
version = f"UPT_MIX_PMNS_v1@{RUN_TS}"

def emit_target(symbol, name, x_value, p_index=55, tier=1, notes=""):
    # store as rational (limit_denominator) for core; quantize against U_EM(p)
    fr = Fraction(float(x_value)).limit_denominator(10**12)
    pq_num, pq_den = fr.numerator, fr.denominator
    pq_val = fr.numerator / fr.denominator
    q = quantize(pq_val, p_index)
    dna = fingerprint_k(q["k"])
    row = make_row(
        uid=f"UPT/MIX/1/{symbol}#MIX_PMNS_v1",
        domain="SM", tier=tier, name=name, symbol=symbol,
        definition="PMNS magnitude or invariant", value_si="", unit_si="",
        value_dimless=f"{pq_num}/{pq_den}", norm_ref="natural",
        pq_num=pq_num, pq_den=pq_den, pq_value=pq_val,
        U_family="EM", U_scale=f"U_EM({p_index})", p_index=p_index,
        k=q["k"], U_value=q["U_value"], residual=q["residual"], rel_error=q["rel_error"],
        bits_pq=bits_pq(pq_num, pq_den), bits_k=bits_int(q["k"]),
        dna_mod_23=dna[23], dna_mod_49=dna[49], dna_mod_50=dna[50], dna_mod_137=dna[137],
        dna_strategy="k", dna_k_mods=dna, dna_pq_mods={},
        provenance="UPT_MIX_PMNS_v1/build", status="measured",
        calibration_target="", notes=notes,
        hash_core=row_hashes_core("SM", symbol, f"{pq_num}/{pq_den}", pq_num, pq_den),
        version=version
    )
    row["hash_entry"] = row_hash_full(row)
    rows.append(row)

labels = [["Ue1","|U_e1|"],["Ue2","|U_e2|"],["Ue3","|U_e3|"],
          ["Um1","|U_mu1|"],["Um2","|U_mu2|"],["Um3","|U_mu3|"],
          ["Ut1","|U_tau1|"],["Ut2","|U_tau2|"],["Ut3","|U_tau3|"]]
for (sym, nm), val in zip(labels, sum(Uabs, [])):
    emit_target(sym, nm, val, p_index=55, tier=1)

emit_target("JarlskogJ_PMNS", "Jarlskog invariant (PMNS)", J_pmns, p_index=56, tier=2,
            notes="Dirac Jarlskog; Majorana phases omitted")

def emit_constraint(symbol, name, deviation, p_index=56, tier=3):
    mag = abs(deviation)
    fr = Fraction(mag).limit_denominator(10**18)
    pq_num, pq_den = fr.numerator, fr.denominator
    pq_val = fr.numerator / fr.denominator
    q = quantize(pq_val, p_index)
    dna = fingerprint_k(q["k"])
    row = make_row(
        uid=f"UPT/Derived/3/{symbol}#MIX_PMNS_v1",
        domain="Derived", tier=tier, name=name, symbol=symbol,
        definition="Deviation from exact unitarity (target=0)", value_si="", unit_si="",
        value_dimless=f"{pq_num}/{pq_den}", norm_ref="natural",
        pq_num=pq_num, pq_den=pq_den, pq_value=pq_val,
        U_family="EM", U_scale=f"U_EM({p_index})", p_index=p_index,
        k=q["k"], U_value=q["U_value"], residual=q["residual"],
        rel_error=(0.0 if pq_val==0 else abs(q["residual"])/pq_val),
        bits_pq=bits_pq(pq_num,pq_den), bits_k=bits_int(q["k"]),
        dna_mod_23=dna[23], dna_mod_49=dna[49], dna_mod_50=dna[50], dna_mod_137=dna[137],
        dna_strategy="k", dna_k_mods=dna, dna_pq_mods={},
        provenance="UPT_MIX_PMNS_v1/unitarity", status="derived",
        calibration_target="", notes="target=0",
        hash_core=row_hashes_core("Derived", symbol, f"{pq_num}/{pq_den}", pq_num, pq_den),
        version=version
    )
    row["hash_entry"] = row_hash_full(row)
    rows.append(row)

for tag, DEV in [("UUdag",dev1), ("UdagU",dev2)]:
    for i in range(3):
        for j in range(3):
            if i==j:
                emit_constraint(f"{tag}_diag_{i}", f"{tag}[{i},{i}]−1", DEV[i,j], p_index=56, tier=3)
            else:
                emit_constraint(f"{tag}_off_{i}{j}", f"{tag}[{i},{j}]", DEV[i,j], p_index=56, tier=3)

# ------------------------ ASCII report ----------------------------------------------------------------------
def ascii_report():
    lines = []
    lines.append("="*100 + "\n")
    lines.append("PMNS ON UNIVERSAL LATTICE  U(p)=1/(49·50·137^p)\n")
    lines.append("="*100 + "\n")
    lines.append("[BEGIN SECTION:inputs]\n")
    lines.append(f"s12={s12}  s13={s13}  s23={s23}  delta/pi={delta_over_pi}\n")
    lines.append("[END SECTION:inputs]\n\n")

    lines.append("PMNS magnitudes (targets):\n")
    lines.append("symbol   p    k                         U(p)              |U_αi|            residual           rel_error\n")
    lines.append("-"*106 + "\n")
    for r in rows[:9]:
        lines.append(f"{r['symbol']:<8} {r['p_index']:<4} {r['k']:<25} {r['U_value']:<18.12e} {r['pq_value']:<16.12e} {r['residual']:<18.3e} {r['rel_error']:<.3e}\n")

    Jrow = rows[9]
    lines.append("\nJarlskog (PMNS):\n")
    lines.append(f"J_PMNS={Jrow['pq_value']:.12e}  (p={Jrow['p_index']}, k={Jrow['k']}, U={Jrow['U_value']:.12e}, residual={Jrow['residual']:.3e})\n")

    dev_mags = [float(r['pq_value']) for r in rows[10:]]
    lines.append("\nUnitarity deviations (summary over all constraints):\n")
    lines.append(f"max|dev|≈{max(dev_mags):.3e},  median|dev|≈{_np.median(dev_mags):.3e}\n")

    lines.append("\n" + "="*100 + "\n")
    lines.append("EMIT ARTIFACTS\n")
    lines.append("="*100 + "\n")
    lines.append("[BEGIN SECTION:write_files]\n")
    return "".join(lines)

asc_core = ascii_report()

# ------------------------ Emit files ------------------------------------------------------------------------
def emit_files(rows, csv_path, jsonl_path, md_path, asc_path, ascii_text):
    fieldnames = list(rows[0].keys())
    with open(csv_path, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames); w.writeheader(); w.writerows(rows)
    with open(jsonl_path, "w") as f:
        for r in rows: f.write(json.dumps(r, ensure_ascii=False)+"\n")
    with open(md_path, "w") as f:
        f.write("# UPT_MIX_PMNS_v1 — PMNS on universal lattice\n\n")
        f.write(f"- Rows: **{len(rows)}**\n- Run: **{RUN_TS}Z**\n\n")
        f.write("## First 10 rows (preview)\n\n")
        f.write("| uid | domain | tier | symbol | value_dimless | k | p |\n|---|---:|---:|---|---:|---:|---:|\n")
        for r in rows[:10]:
            f.write(f"| {r['uid']} | {r['domain']} | {r['tier']} | {r['symbol']} | {r['value_dimless']} | {r['k']} | {r['p_index']} |\n")
    with open(asc_path, "w") as f:
        f.write(ascii_text)

def finalize_and_print(rows, asc_core):
    lines = [asc_core]
    lines.append(f"[KV] CSV = {CSV_PATH}\n")
    lines.append(f"[KV] JSONL = {JSONL_PATH}\n")
    lines.append(f"[KV] MD = {MD_PATH}\n")
    lines.append(f"[KV] ASCII = {ASC_PATH}\n")
    lines.append("[END SECTION:write_files]\n\n")

    reg_digest = sha256_str(json.dumps(rows, sort_keys=True))
    lines.append("="*100 + "\n")
    lines.append("REGISTRY DIGEST & SNAPSHOT\n")
    lines.append("="*100 + "\n")
    lines.append("[BEGIN SECTION:digest]\n")
    lines.append(f"[KV] rows = {len(rows)}\n")
    lines.append(f"[KV] registry_digest_sha256 = {reg_digest}\n")
    lines.append("uid                                     domain  tier  symbol      value_dimless        U_famil  p_index  k            rel_error\n")
    lines.append("-"*118 + "\n")
    for r in rows[:16]:
        lines.append(f"{r['uid']:<40} {r['domain']:<7} {r['tier']:<5} {r['symbol']:<12} {r['value_dimless']:<18} {r['U_family']:<6} {r['p_index']:<7} {r['k']:<12} {r['rel_error']:<.3e}\n")
    lines.append("[END SECTION:digest]\n\n")

    lines.append("="*100 + "\n")
    lines.append("UPT_MIX_PMNS_v1 — COMPLETE\n")
    lines.append("="*100 + "\n")
    return "".join(lines)

ascii_full = finalize_and_print(rows, asc_core)
emit_files(rows, CSV_PATH, JSONL_PATH, MD_PATH, ASC_PATH, ascii_full)
print(ascii_full, end="")

# =============================== UPT_PORTAL_v1 — CKM–PMNS Portal & Cross-Checks ==============================
# Uses the exact-rational seeds we used in MIX_CKM_v1 and MIX_PMNS_v1 to compute:
# - Mixing angles (CKM & PMNS) in degrees
# - Complementarity sums θ12, θ23, θ13
# - TBM deltas Δ(sin^2 θ_ij) relative to (1/3, 1/2, 0) for (12,23,13)
# - Jarlskog compare J_CKM vs J_PMNS and their ratio
# All results are quantized on the universal lattice and emitted with hashes & ASCII.
# ============================================================================================================

import os, csv, json, math, hashlib, warnings
from fractions import Fraction
import numpy as np
import datetime as dt

# Silence deprecation warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")
OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)

CSV_PATH   = os.path.join(OUT_DIR,    f"UPT_master_PORTAL_{RUN_TS}.csv")
JSONL_PATH = os.path.join(OUT_DIR,    f"UPT_master_PORTAL_{RUN_TS}.jsonl")
MD_PATH    = os.path.join(REPORT_DIR, f"UPT_report_PORTAL_{RUN_TS}.md")
ASC_PATH   = os.path.join(REPORT_DIR, f"UPT_ascii_PORTAL_{RUN_TS}.txt")

# ------------------------ Universal lattice ---------------------------------------------------------------
def U_EM(p: int) -> Fraction:
    return Fraction(1, 49*50*137**p)

def quantize(x: float, p_index: int):
    U = U_EM(p_index)
    U_float = U.numerator / U.denominator
    k = int(round(x / U_float))
    residual = x - k*U_float
    rel_error = 0.0 if x == 0 else abs(residual)/abs(x)
    return dict(U_value=U_float, k=k, residual=residual, rel_error=rel_error)

def sha256_bytes(b: bytes) -> str: return hashlib.sha256(b).hexdigest()
def sha256_str(s: str) -> str: return sha256_bytes(s.encode("utf-8"))
def bits_int(n: int) -> int: return 1 if n == 0 else int(abs(n)).bit_length()
def bits_pq(p: int, q: int) -> int: return bits_int(abs(p)) + bits_int(abs(q))
def fingerprint_k(k: int, mods=(23,49,50,137)): return {m: (k % m) for m in mods}

def row_hashes_core(domain, symbol, value_dimless, pq_num, pq_den):
    core = f"{domain}|{symbol}|{value_dimless}|{pq_num}|{pq_den}"
    return sha256_str(core)

def row_hash_full(row: dict):
    ordered_keys = [
        "uid","domain","tier","name","symbol","definition",
        "value_si","unit_si","value_dimless","norm_ref",
        "pq_num","pq_den","pq_value",
        "U_family","U_scale","p_index","k","U_value",
        "residual","rel_error","bits_pq","bits_k",
        "dna_mod_23","dna_mod_49","dna_mod_50","dna_mod_137",
        "dna_strategy","dna_k_mods","dna_pq_mods",
        "provenance","status","calibration_target",
        "notes","hash_core","version"
    ]
    payload = "|".join(str(row.get(k,"")) for k in ordered_keys)
    return sha256_str(payload)

def make_row(**kw):
    import json as _json
    kw.setdefault("dna_strategy","k")
    if isinstance(kw.get("dna_k_mods"), dict):
        kw["dna_k_mods"] = _json.dumps(kw["dna_k_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_k_mods", _json.dumps({}))
    if isinstance(kw.get("dna_pq_mods"), dict):
        kw["dna_pq_mods"] = _json.dumps(kw["dna_pq_mods"], sort_keys=True)
    else:
        kw.setdefault("dna_pq_mods", _json.dumps({}))
    return kw

# ------------------------ CKM seeds (from your CKM module) -------------------------------------------------
s12_CKM = Fraction(13482, 60107)
s13_CKM = Fraction(1913, 485533)
s23_CKM = Fraction(6419, 152109)
delta_over_pi_CKM = Fraction(6869, 17983)  # phase fraction used in CKM module
delta_CKM = float(delta_over_pi_CKM) * math.pi

# ------------------------ PMNS seeds (from PMNS v1 you just ran) ------------------------------------------
s12_PMNS = Fraction(199, 359)
s13_PMNS = Fraction(31, 209)
s23_PMNS = Fraction(59, 80)
delta_over_pi_PMNS = Fraction(4, 5)
delta_PMNS = float(delta_over_pi_PMNS) * math.pi

def angle_deg_from_sin(s: float) -> float:
    s = max(0.0, min(1.0, s))
    return math.degrees(math.asin(s))

def jarlskog(c12,c23,c13,s12,s23,s13,delta):
    return c12*c23*(c13**2)*s12*s23*s13*math.sin(delta)

def to_float(fr: Fraction) -> float:
    return fr.numerator / fr.denominator

# Compute angles
def angles_from_seeds(s12, s23, s13):
    s12f, s23f, s13f = map(to_float, (s12,s23,s13))
    th12 = angle_deg_from_sin(s12f)
    th23 = angle_deg_from_sin(s23f)
    th13 = angle_deg_from_sin(s13f)
    c12 = math.sqrt(1 - s12f**2); c23 = math.sqrt(1 - s23f**2); c13 = math.sqrt(1 - s13f**2)
    return dict(th12=th12, th23=th23, th13=th13, c12=c12, c23=c23, c13=c13,
                s12=s12f, s23=s23f, s13=s13f)

ckm = angles_from_seeds(s12_CKM, s23_CKM, s13_CKM)
pmns = angles_from_seeds(s12_PMNS, s23_PMNS, s13_PMNS)

# Jarlskog invariants
J_CKM  = jarlskog(ckm["c12"],ckm["c23"],ckm["c13"],ckm["s12"],ckm["s23"],ckm["s13"], delta_CKM)
J_PMNS = jarlskog(pmns["c12"],pmns["c23"],pmns["c13"],pmns["s12"],pmns["s23"],pmns["s13"], delta_PMNS)

# Complementarity (degrees)
comp12 = ckm["th12"] + pmns["th12"]
comp23 = ckm["th23"] + pmns["th23"]
comp13 = ckm["th13"] + pmns["th13"]

# TBM deltas (∆ sin^2 θ_ij) vs (1/3, 1/2, 0)
tbm_d12 = pmns["s12"]**2 - (1/3)
tbm_d23 = pmns["s23"]**2 - (1/2)
tbm_d13 = pmns["s13"]**2 - 0.0

# Ratios & spreads
J_ratio = (0.0 if J_CKM == 0 else J_PMNS / J_CKM)
comp_spread_from_45 = abs(comp12 - 45.0)  # often quoted

# ------------------------ Emit rows -------------------------------------------------------------------------
rows = []
version = f"UPT_PORTAL_v1@{RUN_TS}"

def emit_value(symbol, name, x_value, p_index=56, tier=7, notes=""):
    # store exact rational if provided; otherwise store high-precision decimal fraction
    fr = Fraction(x_value).limit_denominator(10**12)
    pq_num, pq_den = fr.numerator, fr.denominator
    pq_val = pq_num / pq_den
    q = quantize(pq_val, p_index)
    dna = fingerprint_k(q["k"])
    row = make_row(
        uid=f"UPT/Derived/7/{symbol}#PORTAL_v1",
        domain="Derived", tier=tier, name=name, symbol=symbol,
        definition="portal/cross-domain derived observable", value_si="", unit_si="",
        value_dimless=f"{pq_num}/{pq_den}", norm_ref="natural",
        pq_num=pq_num, pq_den=pq_den, pq_value=pq_val,
        U_family="EM", U_scale=f"U_EM({p_index})", p_index=p_index,
        k=q["k"], U_value=q["U_value"], residual=q["residual"], rel_error=q["rel_error"],
        bits_pq=bits_pq(pq_num,pq_den), bits_k=bits_int(q["k"]),
        dna_mod_23=dna[23], dna_mod_49=dna[49], dna_mod_50=dna[50], dna_mod_137=dna[137],
        dna_strategy="k", dna_k_mods=dna, dna_pq_mods={},
        provenance="UPT_PORTAL_v1/build", status="derived",
        calibration_target="", notes=notes,
        hash_core=row_hashes_core("Derived", symbol, f"{pq_num}/{pq_den}", pq_num, pq_den),
        version=version
    )
    row["hash_entry"] = row_hash_full(row)
    rows.append(row)

# Angles (deg) — we publish in degrees as dimensionless numbers on lattice
emit_value("theta12_CKM_deg", "θ12^CKM (deg)", ckm["th12"], p_index=56, tier=7)
emit_value("theta23_CKM_deg", "θ23^CKM (deg)", ckm["th23"], p_index=56, tier=7)
emit_value("theta13_CKM_deg", "θ13^CKM (deg)", ckm["th13"], p_index=56, tier=7)

emit_value("theta12_PMNS_deg", "θ12^PMNS (deg)", pmns["th12"], p_index=56, tier=7)
emit_value("theta23_PMNS_deg", "θ23^PMNS (deg)", pmns["th23"], p_index=56, tier=7)
emit_value("theta13_PMNS_deg", "θ13^PMNS (deg)", pmns["th13"], p_index=56, tier=7)

# Complementarity sums
emit_value("comp12_deg", "θ12^CKM + θ12^PMNS (deg)", comp12, p_index=56, tier=7)
emit_value("comp23_deg", "θ23^CKM + θ23^PMNS (deg)", comp23, p_index=56, tier=7)
emit_value("comp13_deg", "θ13^CKM + θ13^PMNS (deg)", comp13, p_index=56, tier=7)
emit_value("comp12_minus_45", "|(θ12^CKM + θ12^PMNS) − 45°|", comp_spread_from_45, p_index=56, tier=7)

# TBM deltas (use p=55 for sin^2)
emit_value("TBM_delta_s2_12", "Δ(sin^2 θ12) vs 1/3", tbm_d12, p_index=55, tier=7)
emit_value("TBM_delta_s2_23", "Δ(sin^2 θ23) vs 1/2", tbm_d23, p_index=55, tier=7)
emit_value("TBM_delta_s2_13", "Δ(sin^2 θ13) vs 0",   tbm_d13, p_index=55, tier=7)

# Jarlskog compare
emit_value("J_CKM",   "Jarlskog (CKM)",  J_CKM,  p_index=56, tier=7)
emit_value("J_PMNS",  "Jarlskog (PMNS)", J_PMNS, p_index=56, tier=7)
emit_value("J_ratio", "J_PMNS / J_CKM",  J_ratio, p_index=56, tier=7)

# ------------------------ ASCII report ----------------------------------------------------------------------
def ascii_report():
    lines = []
    lines.append("="*100 + "\n")
    lines.append("CKM–PMNS PORTAL on Universal Lattice  U(p)=1/(49·50·137^p)\n")
    lines.append("="*100 + "\n")
    lines.append("[BEGIN SECTION:seeds]\n")
    lines.append(f"CKM: s12={s12_CKM}, s13={s13_CKM}, s23={s23_CKM}, delta/pi={delta_over_pi_CKM}\n")
    lines.append(f"PMNS: s12={s12_PMNS}, s13={s13_PMNS}, s23={s23_PMNS}, delta/pi={delta_over_pi_PMNS}\n")
    lines.append("[END SECTION:seeds]\n\n")

    def fmt3(x): return f"{x:.2f}"
    lines.append("Angles [deg] & Complementarity:\n")
    lines.append(f"θ12: CKM={fmt3(ckm['th12'])}, PMNS={fmt3(pmns['th12'])} → sum={fmt3(comp12)} (|sum−45°|={fmt3(comp_spread_from_45)})\n")
    lines.append(f"θ23: CKM={fmt3(ckm['th23'])}, PMNS={fmt3(pmns['th23'])} → sum={fmt3(comp23)}\n")
    lines.append(f"θ13: CKM={fmt3(ckm['th13'])}, PMNS={fmt3(pmns['th13'])} → sum={fmt3(comp13)}\n\n")

    lines.append("TBM deltas (Δ sin^2):\n")
    lines.append(f"Δ(sin^2 θ12)={tbm_d12:.3e},  Δ(sin^2 θ23)={tbm_d23:.3e},  Δ(sin^2 θ13)={tbm_d13:.3e}\n\n")

    lines.append("Jarlskog:\n")
    lines.append(f"J_CKM={J_CKM:.12e},  J_PMNS={J_PMNS:.12e},  ratio={J_ratio:.3e}\n\n")

    lines.append("="*100 + "\n")
    lines.append("EMIT ARTIFACTS\n")
    lines.append("="*100 + "\n")
    lines.append("[BEGIN SECTION:write_files]\n")
    return "".join(lines)

asc_core = ascii_report()

# ------------------------ Emit files ------------------------------------------------------------------------
def emit_files(rows, csv_path, jsonl_path, md_path, asc_path, ascii_text):
    fieldnames = list(rows[0].keys())
    with open(csv_path, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames); w.writeheader(); w.writerows(rows)
    with open(jsonl_path, "w") as f:
        for r in rows: f.write(json.dumps(r, ensure_ascii=False)+"\n")
    with open(md_path, "w") as f:
        f.write("# UPT_PORTAL_v1 — CKM–PMNS cross-checks on the universal lattice\n\n")
        f.write(f"- Rows: **{len(rows)}**\n- Run: **{RUN_TS}Z**\n\n")
        f.write("## Preview (first 12 rows)\n\n")
        f.write("| uid | domain | tier | symbol | value_dimless | k | p |\n|---|---:|---:|---|---:|---:|---:|\n")
        for r in rows[:12]:
            f.write(f"| {r['uid']} | {r['domain']} | {r['tier']} | {r['symbol']} | {r['value_dimless']} | {r['k']} | {r['p_index']} |\n")
    with open(asc_path, "w") as f:
        f.write(ascii_text)

def finalize_and_print(rows, asc_core):
    lines = [asc_core]
    lines.append(f"[KV] CSV = {CSV_PATH}\n")
    lines.append(f"[KV] JSONL = {JSONL_PATH}\n")
    lines.append(f"[KV] MD = {MD_PATH}\n")
    lines.append(f"[KV] ASCII = {ASC_PATH}\n")
    lines.append("[END SECTION:write_files]\n\n")

    reg_digest = sha256_str(json.dumps(rows, sort_keys=True))
    lines.append("="*100 + "\n")
    lines.append("REGISTRY DIGEST & SNAPSHOT\n")
    lines.append("="*100 + "\n")
    lines.append("[BEGIN SECTION:digest]\n")
    lines.append(f"[KV] rows = {len(rows)}\n")
    lines.append(f"[KV] registry_digest_sha256 = {reg_digest}\n")
    lines.append("uid                                     domain  tier  symbol                value_dimless        U_famil  p_index  k             rel_error\n")
    lines.append("-"*126 + "\n")
    for r in rows[:20]:
        lines.append(f"{r['uid']:<40} {r['domain']:<7} {r['tier']:<5} {r['symbol']:<20} {r['value_dimless']:<20} {r['U_family']:<6} {r['p_index']:<7} {r['k']:<14} {r['rel_error']:<.3e}\n")
    lines.append("[END SECTION:digest]\n\n")

    lines.append("="*100 + "\n")
    lines.append("UPT_PORTAL_v1 — COMPLETE\n")
    lines.append("="*100 + "\n")
    return "".join(lines)

ascii_full = finalize_and_print(rows, asc_core)
emit_files(rows, CSV_PATH, JSONL_PATH, MD_PATH, ASC_PATH, ascii_full)
print(ascii_full, end="")

# ============================== UPT_AGGREGATE_v1 — print-first, still writes files ==============================
# Scans /content for UPT_master_*.csv (excluding MASTER_ALL), aggregates into one DataFrame,
# computes stats + DNA spectra, WRITES files, and PRINTS all key info to stdout so you never have to hunt.
# ================================================================================================================

import os, csv, json, glob, math, hashlib, warnings
from fractions import Fraction
import pandas as pd
from collections import Counter, defaultdict
import datetime as dt

# Silence deprecation warnings globally
warnings.filterwarnings("ignore", category=DeprecationWarning)

OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)

RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

CSV_OUT   = os.path.join(OUT_DIR,    f"UPT_MASTER_ALL_{RUN_TS}.csv")
JSONL_OUT = os.path.join(OUT_DIR,    f"UPT_MASTER_ALL_{RUN_TS}.jsonl")
MD_OUT    = os.path.join(REPORT_DIR, f"UPT_report_MASTER_ALL_{RUN_TS}.md")
ASC_OUT   = os.path.join(REPORT_DIR, f"UPT_ascii_MASTER_ALL_{RUN_TS}.txt")

# Console verbosity knobs
PRINT_PREVIEW_ROWS        = 15   # first N rows in global preview
PRINT_TOP_ERRORS          = 12   # top N by |rel_error|
PRINT_PER_DOMAIN_ROWS     = 5    # first N rows per domain
PRINT_DNA_TOP             = 12   # top N residues to show for each modulus
ABBR_INT_EDGES            = 6    # show first/last digits for huge ints

def sha256_bytes(b: bytes) -> str: return hashlib.sha256(b).hexdigest()
def sha256_str(s: str) -> str: return sha256_bytes(s.encode("utf-8"))

def safe_int(x):
    try: return int(x)
    except: return 0

def safe_float(x):
    try: return float(x)
    except: return 0.0

def abbr_int(n: int, edges: int = ABBR_INT_EDGES) -> str:
    s = str(n)
    if len(s) <= edges*2+3:
        return s
    return f"{s[:edges]}…{s[-edges:]} (len={len(s)})"

def collect_csvs(pattern=os.path.join(OUT_DIR, "UPT_master_*.csv")):
    files = sorted(glob.glob(pattern))
    # Skip the aggregate outputs if any previous run exists
    files = [f for f in files if "MASTER_ALL" not in f]
    return files

def read_all(files):
    frames = []
    for f in files:
        try:
            df = pd.read_csv(f, dtype=str, keep_default_na=False)
            df["__source_file"] = os.path.basename(f)
            frames.append(df)
        except Exception as e:
            print(f"[WARN] Skipping {f}: {e}")
    if not frames:
        return pd.DataFrame()
    # Normalize missing columns across files
    cols = sorted(set().union(*[set(df.columns) for df in frames]))
    frames = [df.reindex(columns=cols, fill_value="") for df in frames]
    return pd.concat(frames, ignore_index=True)

def fingerprint_k(k: int, mods=(23,49,50,137)):
    return {m: (k % m) for m in mods}

def build_stats(df: pd.DataFrame):
    # numeric casts
    df["_k"]         = df.get("k","0").map(safe_int)
    df["_rel_error"] = df.get("rel_error","0").map(safe_float)
    df["_domain"]    = df.get("domain","")
    df["_tier"]      = df.get("tier","0").map(safe_int)
    df["_p"]         = df.get("p_index","0").map(safe_int)

    # counts
    per_domain = df.groupby("_domain")["uid"].count().to_dict() if not df.empty else {}
    per_tier   = df.groupby("_tier")["uid"].count().to_dict() if not df.empty else {}
    total_rows = int(df.shape[0])

    # residual summary
    errs = df["_rel_error"].tolist() if not df.empty else []
    if errs:
        err_max = max(errs)
        err_p50 = sorted(errs)[len(errs)//2]
        err_mean = sum(errs)/len(errs)
    else:
        err_max = err_p50 = err_mean = 0.0

    # DNA residue spectra from k
    dna_counts = {23:Counter(),49:Counter(),50:Counter(),137:Counter()}
    for k in df["_k"]:
        dna = fingerprint_k(k)
        for m,v in dna.items():
            dna_counts[m][v] += 1

    # worst offenders by |rel_error|
    df["_abs_rel_error"] = df["_rel_error"].abs()
    top_err = df.sort_values("_abs_rel_error", ascending=False).head(PRINT_TOP_ERRORS).copy() if not df.empty else pd.DataFrame()

    return dict(
        total_rows=total_rows,
        per_domain=per_domain,
        per_tier=per_tier,
        err_max=err_max,
        err_p50=err_p50,
        err_mean=err_mean,
        dna_counts=dna_counts,
        top_err=top_err
    )

def emit_outputs(df: pd.DataFrame, stats):
    if df.empty:
        print("="*100)
        print("MASTER ALL — nothing to aggregate (no UPT_master_*.csv found).")
        print("="*100)
        return

    # Write CSV
    df.to_csv(CSV_OUT, index=False)

    # Write JSONL
    cols = list(df.columns)
    with open(JSONL_OUT, "w") as f:
        for _, row in df.iterrows():
            f.write(json.dumps({c: row.get(c, "") for c in cols}, ensure_ascii=False)+"\n")

    # ASCII report (same content we’ll also print to console)
    lines = []
    def L(s=""): lines.append(s + ("\n" if not s.endswith("\n") else ""))

    L("="*100)
    L("UPT — GLOBAL MASTER LEDGER (aggregate of all UPT_master_*.csv)")
    L("="*100)
    L("[BEGIN SECTION:files]")
    srcs = sorted(df["__source_file"].dropna().unique().tolist())
    for s in srcs: L(f"- {s}")
    L("[END SECTION:files]\n")

    # SUMMARY COUNTS
    L("SUMMARY COUNTS")
    L("-"*100)
    L(f"rows_total = {stats['total_rows']}")
    L("per_domain  : " + json.dumps(stats["per_domain"], sort_keys=True))
    L("per_tier    : " + json.dumps(stats["per_tier"], sort_keys=True))
    L("")

    # RESIDUALS
    L("RESIDUALS (rel_error) SUMMARY")
    L("-"*100)
    L(f"max = {stats['err_max']:.3e},  median ≈ {stats['err_p50']:.3e},  mean ≈ {stats['err_mean']:.3e}")
    L("")

    # TOP WORST by |rel_error|
    L("TOP WORST by |rel_error|")
    L("-"*100)
    if stats["top_err"].empty:
        L("(none)")
    else:
        L(f"{'domain':10s} {'tier':>4s} {'symbol':20s} {'p':>3s} {'k (abbr)':>24s} {'rel_error':>12s}  {'uid'}")
        L("-"*100)
        for _, r in stats["top_err"].iterrows():
            L(f"{(r.get('domain','')):10s} "
              f"{(r.get('tier','')):>4s} "
              f"{(r.get('symbol','')):20s} "
              f"{(r.get('p_index','')):>3s} "
              f"{abbr_int(safe_int(r.get('k','0'))):>24s} "
              f"{safe_float(r.get('rel_error','0')):12.3e}  "
              f"{r.get('uid','')}")
    L("")

    # PER-DOMAIN PREVIEW
    L("PER-DOMAIN PREVIEW (first rows in each domain)")
    L("-"*100)
    for dom in sorted(stats["per_domain"].keys()):
        sub = df[df["_domain"]==dom].head(PRINT_PER_DOMAIN_ROWS)
        L(f"[{dom}]")
        L(f"{'tier':>4s}  {'symbol':20s}  {'value_dimless':>18s}  {'p':>3s}  {'k (abbr)':>24s}  {'rel_error':>12s}")
        for _, r in sub.iterrows():
            L(f"{r.get('tier',''):>4s}  "
              f"{r.get('symbol',''):20s}  "
              f"{r.get('value_dimless',''):>18s}  "
              f"{r.get('p_index',''):>3s}  "
              f"{abbr_int(safe_int(r.get('k','0'))):>24s}  "
              f"{safe_float(r.get('rel_error','0')):12.3e}")
        L("")

    # DNA spectra
    L("DNA(k) RESIDUE SPECTRA (top counts)")
    L("-"*100)
    for m, ctr in stats["dna_counts"].items():
        top = ctr.most_common(PRINT_DNA_TOP)
        L(f"mod {m}: " + ", ".join(f"{r}:{c}" for r,c in top))
    L("")

    # Digests
    with open(CSV_OUT, "rb") as f: csv_sha = sha256_bytes(f.read())
    with open(JSONL_OUT, "rb") as f: jsonl_sha = sha256_bytes(f.read())
    L("LEDGER DIGESTS")
    L("="*100)
    L("[BEGIN SECTION:ledger_digests]")
    L(f"{os.path.basename(CSV_OUT):40s} sha256={csv_sha}")
    L(f"{os.path.basename(JSONL_OUT):40s} sha256={jsonl_sha}")
    L("[END SECTION:ledger_digests]")
    L("")

    # Write ASCII/MD
    with open(ASC_OUT, "w") as f:
        f.write("".join(lines))
    with open(MD_OUT, "w") as f:
        f.write("# UPT — Global Master Ledger\n\n")
        f.write(f"- Run: **{RUN_TS}Z**\n")
        f.write(f"- Rows: **{stats['total_rows']}**\n\n")
        f.write("## Per-domain counts\n\n")
        f.write("```\n"+json.dumps(stats["per_domain"], indent=2, sort_keys=True)+"\n```\n\n")
        f.write("## Residuals summary\n\n")
        f.write(f"- max: **{stats['err_max']:.3e}**\n- median: **{stats['err_p50']:.3e}**\n- mean: **{stats['err_mean']:.3e}**\n\n")
        f.write("## DNA(k) spectra (top residues)\n\n")
        for m, ctr in stats["dna_counts"].items():
            f.write(f"**mod {m}**: " + ", ".join(f"{r}:{c}" for r,c in ctr.most_common(PRINT_DNA_TOP)) + "  \n")
        f.write("\n")
        f.write("## Preview (first 15 rows)\n\n")
        f.write("| uid | domain | tier | symbol | value_dimless | k | p |\n|---|---:|---:|---|---:|---:|---:|\n")
        for _, r in df.head(PRINT_PREVIEW_ROWS).iterrows():
            f.write(f"| {r.get('uid','')} | {r.get('domain','')} | {r.get('tier','')} | {r.get('symbol','')} | {r.get('value_dimless','')} | {r.get('k','')} | {r.get('p_index','')} |\n")

    # ========================= CONSOLE PRINTS (everything you care about) =========================
    print("="*100)
    print("AGGREGATE — Scanning domain ledgers")
    print("="*100)
    print("[KV] found =", len(srcs))
    for s in srcs: print(" -", s)

    print("="*100)
    print("GLOBAL MASTER — SUMMARY COUNTS")
    print("="*100)
    print(f"rows_total = {stats['total_rows']}")
    print("per_domain  :", json.dumps(stats["per_domain"], sort_keys=True))
    print("per_tier    :", json.dumps(stats["per_tier"], sort_keys=True))

    print("="*100)
    print("GLOBAL MASTER — RESIDUALS")
    print("="*100)
    print("max = {0:.3e},  median ≈ {1:.3e},  mean ≈ {2:.3e}".format(stats['err_max'], stats['err_p50'], stats['err_mean']))

    print("="*100)
    print("TOP WORST by |rel_error|")
    print("="*100)
    if stats["top_err"].empty:
        print("(none)")
    else:
        print(f"{'domain':10s} {'tier':>4s} {'symbol':20s} {'p':>3s} {'k (abbr)':>24s} {'rel_error':>12s}  {'uid'}")
        print("-"*100)
        for _, r in stats["top_err"].iterrows():
            print(f"{(r.get('domain','')):10s} "
                  f"{(r.get('tier','')):>4s} "
                  f"{(r.get('symbol','')):20s} "
                  f"{(r.get('p_index','')):>3s} "
                  f"{abbr_int(safe_int(r.get('k','0'))):>24s} "
                  f"{safe_float(r.get('rel_error','0')):12.3e}  "
                  f"{r.get('uid','')}")

    print("="*100)
    print("PER-DOMAIN PREVIEW (first rows per domain)")
    print("="*100)
    for dom in sorted(stats["per_domain"].keys()):
        sub = df[df["_domain"]==dom].head(PRINT_PER_DOMAIN_ROWS)
        print(f"[{dom}]")
        print(f"{'tier':>4s}  {'symbol':20s}  {'value_dimless':>18s}  {'p':>3s}  {'k (abbr)':>24s}  {'rel_error':>12s}")
        for _, r in sub.iterrows():
            print(f"{r.get('tier',''):>4s}  "
                  f"{r.get('symbol',''):20s}  "
                  f"{r.get('value_dimless',''):>18s}  "
                  f"{r.get('p_index',''):>3s}  "
                  f"{abbr_int(safe_int(r.get('k','0'))):>24s}  "
                  f"{safe_float(r.get('rel_error','0')):12.3e}")
        print("")

    print("="*100)
    print("DNA(k) RESIDUE SPECTRA (top counts)")
    print("="*100)
    for m, ctr in stats["dna_counts"].items():
        top = ctr.most_common(PRINT_DNA_TOP)
        print(f"mod {m}: " + ", ".join(f"{r}:{c}" for r,c in top))

    # final digest — stable over what we printed
    digest_payload = dict(
        RUN_TS=RUN_TS,
        total_rows=stats["total_rows"],
        per_domain=stats["per_domain"],
        per_tier=stats["per_tier"],
        err_max=stats["err_max"],
        err_p50=stats["err_p50"],
        err_mean=stats["err_mean"],
        dna_counts={int(k): dict(v) for k,v in stats["dna_counts"].items()}
    )
    print("="*100)
    print("GLOBAL MASTER — DIGEST & SNAPSHOT")
    print("="*100)
    print("[BEGIN SECTION:write_files]")
    print(f"[KV] CSV = {CSV_OUT}")
    print(f"[KV] JSONL = {JSONL_OUT}")
    print(f"[KV] MD = {MD_OUT}")
    print(f"[KV] ASCII = {ASC_OUT}")
    print("[END SECTION:write_files]\n")

    print("[BEGIN SECTION:digest]")
    print(f"[KV] rows = {stats['total_rows']}")
    print(f"[KV] registry_digest_sha256 = {sha256_str(json.dumps(digest_payload, sort_keys=True))}")
    print("Top per-domain:", stats["per_domain"])
    print("Residuals: max={:.3e}, median≈{:.3e}, mean≈{:.3e}".format(stats['err_max'], stats['err_p50'], stats['err_mean']))
    print("[END SECTION:digest]\n")

# ============================== run aggregation ==============================================================
print("="*100)
print("AGGREGATE — Scanning domain ledgers")
print("="*100)
files = collect_csvs()
print("[KV] found =", len(files))
for f in files: print(" -", os.path.basename(f))

df_all = read_all(files)
stats = build_stats(df_all)
emit_outputs(df_all, stats)

print("="*100)
print("UPT_AGGREGATE_v1 — COMPLETE")
print("="*100)

# ====================================== UPT_CALIBRATE_v1 — print-first rational s_D fits ======================================
# Fits domain multipliers s_QCD, s_G, s_COS (and any custom domain) from anchor rows you define below.
# Prints EVERYTHING you care about to stdout:
#  - full anchor list by domain (name, p, value)
#  - fitted s_D as rational + decimal, MAE, max|rel|
#  - per-anchor evaluation table (p, k, residual, rel_error)
#  - stable digest over results
# Still writes CSV/JSONL/MD/ASCII, but you don't need to filehunt.
# ==============================================================================================================================

import os, json, math, hashlib, warnings
from fractions import Fraction
from statistics import median
import pandas as pd
import datetime as dt

# Silence deprecation warnings globally
warnings.filterwarnings("ignore", category=DeprecationWarning)

OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)

RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

CSV_OUT   = os.path.join(OUT_DIR,    f"UPT_master_CALIB_{RUN_TS}.csv")
JSONL_OUT = os.path.join(OUT_DIR,    f"UPT_master_CALIB_{RUN_TS}.jsonl")
MD_OUT    = os.path.join(REPORT_DIR, f"UPT_report_CALIB_{RUN_TS}.md")
ASC_OUT   = os.path.join(REPORT_DIR, f"UPT_ascii_CALIB_{RUN_TS}.txt")

# Console formatting knobs
SHOW_EVAL_ROWS_PER_DOMAIN = 12  # how many per-anchor eval rows to print per domain

def sha256_bytes(b: bytes) -> str: return hashlib.sha256(b).hexdigest()
def sha256_str(s: str) -> str: return sha256_bytes(s.encode("utf-8"))

# Universal lattice base: U_EM(p) = 1/(49*50*137^p) with exact rationals
def U_EM(p: int) -> Fraction:
    return Fraction(1, 2450) * Fraction(1, pow(137, p))

def as_fraction(x) -> Fraction:
    if isinstance(x, Fraction): return x
    if isinstance(x, (int, float)): return Fraction(x).limit_denominator()
    s = str(x).strip()
    if "/" in s:
        a,b = s.split("/",1)
        return Fraction(int(a), int(b))
    return Fraction(s)  # parses decimal strings in py3.12

def quantize(v: Fraction, U: Fraction, s: Fraction):
    Ueff = U * s
    k = int(round(float(v / Ueff)))  # integer k from nearest
    residual = v - k*Ueff
    rel_err = Fraction(0) if v == 0 else residual / v
    return Ueff, k, residual, rel_err

def fit_s_domain(anchors, max_den=10000):
    """
    anchors: list of {name, value_dimless (p/q or decimal str), U_family, p_index, target_k(optional)}
    Returns: dict with s_hat (Fraction), mae, max_abs, and diagnostics/evaluations.
    """
    s_candidates = []
    diags = []
    for a in anchors:
        v = as_fraction(a["value_dimless"])
        p = int(a["p_index"])
        U = U_EM(p)  # domain scales multiply EM base
        if a.get("target_k") is not None:
            k0 = int(a["target_k"])
        else:
            k0 = int(round(float(v / U)))  # first-pass assuming s≈1
        s_i = v / (k0 * U) if k0 != 0 else Fraction(1,1)
        s_candidates.append(s_i)
        diags.append(dict(anchor=a["name"], p_index=p, v=str(v), U=str(U), k0=k0, s_est=str(s_i)))

    if not s_candidates:
        s_hat = Fraction(1,1)
    else:
        s_med = median([float(s) for s in s_candidates])
        s_hat = Fraction(s_med).limit_denominator(max_den)

    # Evaluate with s_hat
    maes = []
    max_abs = 0.0
    per_anchor_eval = []
    for a in anchors:
        v = as_fraction(a["value_dimless"])
        p = int(a["p_index"])
        U = U_EM(p)
        Ueff, k, res, rel = quantize(v, U, s_hat)
        maes.append(abs(float(rel)))
        max_abs = max(max_abs, abs(float(rel)))
        per_anchor_eval.append(dict(
            anchor=a["name"], p_index=p, U=str(U), s=str(s_hat), Ueff=str(Ueff),
            k=k, residual=str(res), rel_error=float(rel)
        ))

    mae = sum(maes)/len(maes) if maes else 0.0
    return dict(s_hat=s_hat, mae=mae, max_abs=max_abs, diags=diags, evals=per_anchor_eval)

def emit_rows(domain: str, s_frac: Fraction, anchors_eval):
    rows = []
    # Meta snapshot row
    rows.append(dict(
        uid=f"UPT/Meta/8/s_{domain}_snapshot#CALIB_v1",
        domain="Meta", tier=8, name=f"s_{domain}_snapshot", symbol=f"s_{domain}",
        value_dimless=str(s_frac), unit_si="", U_family=domain, p_index=0, k=0,
        residual="0", rel_error="0",
        notes="calibrated via UPT_CALIBRATE_v1",
    ))
    # Per-anchor eval rows
    for e in anchors_eval:
        rows.append(dict(
            uid=f"UPT/{domain}/CALIB/{e['anchor']}#CALIB_v1",
            domain=domain, tier="CALIB", name=f"calib_{domain}_{e['anchor']}",
            symbol=f"{domain}_cal_{e['anchor']}",
            value_dimless="", unit_si="", U_family=domain, p_index=e["p_index"],
            k=e["k"], residual=e["residual"], rel_error=f"{e['rel_error']:.3e}",
            notes=f"s_{domain}={s_frac}"
        ))
    return rows

def write_artifacts(rows, summary):
    df = pd.DataFrame(rows)
    df.to_csv(CSV_OUT, index=False)
    with open(JSONL_OUT, "w") as f:
        for _, r in df.iterrows():
            f.write(json.dumps({k: ("" if pd.isna(v) else v) for k,v in r.to_dict().items()}, ensure_ascii=False) + "\n")

    # ASCII
    lines = []
    lines.append("="*100 + "\n")
    lines.append("CALIBRATION REPORT — rational s_D fits on universal lattice\n")
    lines.append("="*100 + "\n\n")
    lines.append("[BEGIN SECTION:summary]\n")
    for dom, s in summary["s"].items():
        lines.append(f"{dom:6s}: s_{dom} = {s['s_hat']} (~{float(Fraction(s['s_hat'])):.12g})   "
                     f"(mae≈{s['mae']:.3e}, max|rel|≈{s['max_abs']:.3e})\n")
    lines.append("[END SECTION:summary]\n\n")
    lines.append("ANCHOR DIAGNOSTICS (first few per domain)\n")
    lines.append("-"*100 + "\n")
    for dom, s in summary["s"].items():
        lines.append(f"[{dom}] anchors (evaluated):\n")
        for d in s["evals"][:6]:
            lines.append(f"  - {d['anchor']}: p={d['p_index']}  k={d['k']}  residual={d['residual']}  rel≈{float(d['rel_error']):.3e}\n")
        lines.append("\n")
    with open(ASC_OUT, "w") as f: f.write("".join(lines))

    # MD (short)
    with open(MD_OUT, "w") as f:
        f.write("# Calibration Report (UPT_CALIBRATE_v1)\n\n")
        f.write(f"- Run: **{RUN_TS}Z**\n\n")
        f.write("## s_D snapshots\n\n")
        for dom, s in summary["s"].items():
            f.write(f"- **{dom}**: `s_{dom} = {s['s_hat']}` (~{float(Fraction(s['s_hat'])):.12g})  "
                    f"(mae≈{s['mae']:.3e}, max|rel|≈{s['max_abs']:.3e})\n")

    # Console artifact paths (at the end)
    print("="*100)
    print("CALIBRATION — EMIT ARTIFACTS")
    print("="*100)
    print("[BEGIN SECTION:write_files]")
    print(f"[KV] CSV = {CSV_OUT}")
    print(f"[KV] JSONL = {JSONL_OUT}")
    print(f"[KV] MD = {MD_OUT}")
    print(f"[KV] ASCII = {ASC_OUT}")
    print("[END SECTION:write_files]\n")

    print("="*100)
    print("CALIBRATION — SNAPSHOT")
    print("="*100)
    print("[BEGIN SECTION:digest]")
    print(f"[KV] domains_fit = {list(summary['s'].keys())}")
    print(f"[KV] registry_digest_sha256 = {sha256_str(json.dumps(summary, sort_keys=True, default=str))}")
    print("[END SECTION:digest]\n")

# ===================================== Configure anchors from your current ledger =====================================
# Mirror your seed entries (you can extend these dicts; this cell only reads them)
anchors_cfg = {
    "QCD": [
        dict(name="alpha_s_MZ", value_dimless="9953/84419", U_family="QCD", p_index=55, target_k=None),
    ],
    "Gravity": [
        dict(name="A_bit_over_lp2", value_dimless="6243314768165359/2251799813685248", U_family="G", p_index=64, target_k=None),
        dict(name="S_bits_mP",      value_dimless="1275745965365115/70368744177664",   U_family="G", p_index=64, target_k=None),
    ],
    "COS": [
        dict(name="Omega_total", value_dimless="1/1", U_family="COS", p_index=54, target_k=None),
    ],
}

# ================================================ Run fit with loud prints ================================================
print("="*100)
print("CALIBRATION — START")
print("="*100)

# Print all anchors up front (human-first)
print("[BEGIN SECTION:anchors]")
for dom, anchors in anchors_cfg.items():
    digest = sha256_str(json.dumps(anchors, sort_keys=True))
    print(f"[DOMAIN] {dom}   [KV] anchors_digest_sha256 = {digest}")
    for a in anchors:
        print(f"  - {a['name']:16s}  p={a['p_index']:>2d}  value={a['value_dimless']}")
print("[END SECTION:anchors]\n")

rows = []
summary = {"s": {}}

# Fit per domain
for dom, anchors in anchors_cfg.items():
    fit = fit_s_domain(anchors, max_den=10000)
    s_hat = fit["s_hat"]
    summary["s"][dom] = dict(
        s_hat=str(s_hat), mae=fit["mae"], max_abs=fit["max_abs"],
        evals=fit["evals"], diags=fit["diags"]
    )

# Print concise results per domain
print("="*100)
print("CALIBRATION — FIT RESULTS (per domain)")
print("="*100)
for dom in anchors_cfg.keys():
    s = summary["s"][dom]
    print(f"{dom:6s}  s_{dom} = {s['s_hat']:>12s}  (~{float(Fraction(s['s_hat'])):.12g})   "
          f"MAE≈{s['mae']:.3e},  max|rel|≈{s['max_abs']:.3e}")

# Per-anchor evaluation tables
print("\n" + "="*100)
print("CALIBRATION — PER-ANCHOR EVALUATIONS")
print("="*100)
for dom in anchors_cfg.keys():
    s = summary["s"][dom]
    evals = s["evals"][:SHOW_EVAL_ROWS_PER_DOMAIN]
    print(f"[{dom}]  (showing up to {SHOW_EVAL_ROWS_PER_DOMAIN} anchors)")
    print(f"{'anchor':18s} {'p':>3s} {'k':>8s} {'residual':>22s} {'rel_error':>12s}")
    print("-"*80)
    for e in evals:
        print(f"{e['anchor']:18s} {e['p_index']:>3d} {e['k']:>8d} {e['residual']:>22s} {e['rel_error']:12.3e}")
    print("")

# Emit CSV/JSONL/MD/ASCII (still helpful for diffing / audit trails)
rows.extend(emit_rows("QCD",     Fraction(summary["s"]["QCD"]["s_hat"]),     summary["s"]["QCD"]["evals"]))
rows.extend(emit_rows("Gravity", Fraction(summary["s"]["Gravity"]["s_hat"]), summary["s"]["Gravity"]["evals"]))
rows.extend(emit_rows("COS",     Fraction(summary["s"]["COS"]["s_hat"]),     summary["s"]["COS"]["evals"]))

write_artifacts(rows, summary)

print("="*100)
print("UPT_CALIBRATE_v1 — COMPLETE")
print("="*100)

# ===================== UPT_MDL_DNA_v1b — print-first MDL & DNA (signal-focused) =====================
import os, glob, math, json, hashlib, warnings
from fractions import Fraction
import pandas as pd
import datetime as dt
from collections import Counter

warnings.filterwarnings("ignore", category=DeprecationWarning)

OUT_DIR = "/content"; REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)
RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

CSV_OUT   = os.path.join(OUT_DIR,    f"UPT_master_MDL_DNA_{RUN_TS}.csv")
JSONL_OUT = os.path.join(OUT_DIR,    f"UPT_master_MDL_DNA_{RUN_TS}.jsonl")
MD_OUT    = os.path.join(REPORT_DIR, f"UPT_report_MDL_DNA_{RUN_TS}.md")
ASC_OUT   = os.path.join(REPORT_DIR, f"UPT_ascii_MDL_DNA_{RUN_TS}.txt")

MODS = (23,49,50,137)
TOP_N = 20
SKIP_DOMAINS_FOR_STATS = {"Meta"}   # keep in file list, skip in stats
SKIP_TIERS_FOR_STATS   = {"CALIB"}  # keep in file list, skip in stats

def sha256_bytes(b: bytes) -> str: return hashlib.sha256(b).hexdigest()
def sha256_str(s: str) -> str: return sha256_bytes(s.encode("utf-8"))
def collect_csvs(): return [f for f in sorted(glob.glob(os.path.join(OUT_DIR,"UPT_master_*.csv"))) if "MASTER_ALL" not in f]
def safe_int(x, d=0):
    try: return int(x)
    except: return d
def safe_float(x, d=0.0):
    try: return float(x)
    except: return d

def parse_fraction_pref_pq(s: str):
    """Return (Fraction or None, used_decimal_bool)."""
    if not isinstance(s,str) or not s.strip(): return (None, False)
    s=s.strip()
    if "/" in s:
        a,b=s.split("/",1)
        return (Fraction(int(a),int(b)), False)
    # decimal path
    try:
        return (Fraction(s), True)
    except:
        return (None, False)

def load_concat():
    files = collect_csvs()
    print("="*100); print("MDL & DNA — Scanning domain ledgers"); print("="*100)
    print("[KV] found =", len(files))
    for f in files: print(" -", os.path.basename(f))
    if not files: return pd.DataFrame()
    frames=[]
    for f in files:
        try:
            df=pd.read_csv(f, dtype=str, keep_default_na=False)
            df["__source_file"]=os.path.basename(f)
            frames.append(df)
        except Exception as e:
            print(f"[WARN] skipping {f}: {e}")
    if not frames: return pd.DataFrame()
    cols=sorted(set().union(*[set(df.columns) for df in frames]))
    frames=[df.reindex(columns=cols, fill_value="") for df in frames]
    big=pd.concat(frames, ignore_index=True)

    # de-dupe by uid → keep lexicographically-latest source (deterministic)
    if "uid" in big.columns:
        big = big.sort_values("__source_file").drop_duplicates(subset=["uid"], keep="last").reset_index(drop=True)
    return big

def compute(big: pd.DataFrame):
    if big.empty: return big, {}, {}
    big["_k"]       = big.get("k","0").map(safe_int)
    big["_tier"]    = big.get("tier","")
    big["_domain"]  = big.get("domain","")
    big["_rel"]     = big.get("rel_error","0").map(safe_float)
    big["_pindex"]  = big.get("p_index","0").map(safe_int)

    # parse fractions (prefer p/q)
    frs=[]; used_decimal=0
    for s in big.get("value_dimless","").tolist():
        fr, dec = parse_fraction_pref_pq(s)
        frs.append(fr); used_decimal += (1 if dec else 0)
    big["_fr"]=frs

    # filtered view for stats (exclude Meta & CALIB and k==0 from spectra/leaderboards)
    mask_signal = (~big["_domain"].isin(SKIP_DOMAINS_FOR_STATS)) & (~big["_tier"].isin(SKIP_TIERS_FOR_STATS))
    df = big.loc[mask_signal].copy()
    df_nonzero = df.loc[df["_k"]!=0].copy()

    def bits_int(n:int)->int:
        n=abs(int(n))
        return 1 if n==0 else n.bit_length()
    def bits_fr(fr:Fraction|None)->int:
        if fr is None: return 0
        return bits_int(fr.numerator)+bits_int(fr.denominator)

    df["_bits_pq"]=[bits_fr(fr) for fr in df["_fr"]]
    df["_bits_k"] =[bits_int(k) for k in df["_k"]]

    # MDL compression (only rows that actually have a p/q)
    n_vals = sum(1 for fr in df["_fr"] if fr is not None)
    bits_rational = int(sum(df["_bits_pq"]))
    bits_float = 64*max(1,n_vals)
    compression = bits_rational/bits_float if bits_float else 0.0

    # DNA spectra on non-zero k
    spectra={m:Counter() for m in MODS}
    for k in df_nonzero["_k"]:
        for m in MODS:
            spectra[m][k % m]+=1

    # leaderboards from non-zero k
    order_err = df_nonzero["_rel"].abs().sort_values(ascending=False).index.tolist()
    order_hi  = df["_bits_pq"].sort_values(ascending=False).index.tolist()
    order_lo  = df["_bits_pq"].sort_values(ascending=True).index.tolist()

    M=dict(
        rows=int(big.shape[0]),
        rows_signal=int(df.shape[0]),
        rows_nonzero=int(df_nonzero.shape[0]),
        n_vals=int(n_vals),
        used_decimal=int(used_decimal),
        bits_rational=bits_rational,
        bits_float=bits_float,
        compression_ratio=float(compression),
        spectra=spectra,
        order_err=order_err[:TOP_N],
        order_bits_hi=order_hi[:TOP_N],
        order_bits_lo=order_lo[:TOP_N],
    )
    return df, M, big

def print_dashboard(df: pd.DataFrame, M: dict):
    print("="*100); print("MDL/DNA DASHBOARD — SUMMARY"); print("="*100)
    print(f"[KV] rows_total={M['rows']}, signal_rows={M['rows_signal']}, signal_nonzero_k={M['rows_nonzero']}")
    print(f"[KV] values_with_rational={M['n_vals']}, decimal_inputs_seen={M['used_decimal']}")
    print(f"[KV] rational_bits={M['bits_rational']}, float_bits_naive={M['bits_float']}, compression_ratio≈{M['compression_ratio']:.3f}")
    print("")

    # |rel| leaderboard
    print("="*100); print("TOP |rel_error| ROWS (signal, k≠0)"); print("="*100)
    print(f"{'rank':>4s} {'|rel|':>10s}  {'domain':8s} {'tier':>4s}  {'symbol':18s} {'p':>3s} {'bits(p/q)':>10s} {'bits(k)':>8s}   uid")
    print("-"*120)
    for i, idx in enumerate(M['order_err'], 1):
        r=df.loc[idx]
        print(f"{i:4d} {abs(float(r['_rel'])):10.3e}  {str(r.get('domain',''))[:8]:8s} {str(r.get('tier',''))[:4]:>4s}  {str(r.get('symbol',''))[:18]:18s} "
              f"{int(r.get('_pindex',0)):>3d} {int(r.get('_bits_pq',0)):>10d} {int(r.get('_bits_k',0)):>8d}   {str(r.get('uid',''))[:64]}")

    # bit-cost leaderboards (signal rows)
    print("\n"+"="*100); print("HIGHEST p/q BIT-COST (signal)"); print("="*100)
    print(f"{'rank':>4s} {'bits(p/q)':>10s}  {'domain':8s} {'tier':>4s}  {'symbol':18s} {'value_dimless':28s}")
    print("-"*100)
    for i, idx in enumerate(M['order_bits_hi'], 1):
        r=df.loc[idx]; v=str(r.get('value_dimless',''))[:28]
        print(f"{i:4d} {int(r.get('_bits_pq',0)):>10d}  {str(r.get('domain',''))[:8]:8s} {str(r.get('tier',''))[:4]:>4s}  {str(r.get('symbol',''))[:18]:18s} {v:28s}")

    print("\n"+"="*100); print("LOWEST p/q BIT-COST (signal)"); print("="*100)
    print(f"{'rank':>4s} {'bits(p/q)':>10s}  {'domain':8s} {'tier':>4s}  {'symbol':18s} {'value_dimless':28s}")
    print("-"*100)
    order_lo = [i for i in M['order_bits_lo'] if df.loc[i].get('_bits_pq',0)>0][:TOP_N]
    for i, idx in enumerate(order_lo, 1):
        r=df.loc[idx]; v=str(r.get('value_dimless',''))[:28]
        print(f"{i:4d} {int(r.get('_bits_pq',0)):>10d}  {str(r.get('domain',''))[:8]:8s} {str(r.get('tier',''))[:4]:>4s}  {str(r.get('symbol',''))[:18]:18s} {v:28s}")

    # DNA spectra
    print("\n"+"="*100); print("DNA(k) RESIDUE SPECTRA (signal, k≠0)"); print("="*100)
    for m, ctr in M['spectra'].items():
        total=sum(ctr.values()); expected= (total/m) if m else 0
        chi2=sum((ctr.get(r,0)-expected)**2/(expected if expected>0 else 1) for r in range(m))
        print(f"mod {m}: total={total}, expected_per_bin≈{expected:.2f}, chi2={chi2:.2f}")
        print("  top residues: " + ", ".join(f"{r}:{c}" for r,c in ctr.most_common(15)))
    print("")

def write_artifacts(df: pd.DataFrame, M: dict):
    cols = ["uid","domain","tier","symbol","value_dimless","U_family","p_index","k","rel_error",
            "__source_file","_bits_pq","_bits_k"]
    out = df.reindex(columns=[c for c in cols if c in df.columns or c in df])
    out.to_csv(CSV_OUT, index=False)
    with open(JSONL_OUT,"w") as f:
        for _,r in out.iterrows():
            f.write(json.dumps({c:(r[c] if c in r else "") for c in out.columns}, ensure_ascii=False)+"\n")
    with open(ASC_OUT,"w") as f:
        f.write(f"rows_total={M['rows']}, signal_rows={M['rows_signal']}, compression≈{M['compression_ratio']:.3f}\n")
    with open(MD_OUT,"w") as f:
        f.write(f"# MDL/DNA (signal)\n- Run: **{RUN_TS}Z**\n- rows_total: **{M['rows']}**  signal: **{M['rows_signal']}**\n- compression: **{M['compression_ratio']:.3f}**\n")
    print("="*100); print("MDL/DNA — EMIT ARTIFACTS"); print("="*100)
    print("[BEGIN SECTION:write_files]"); print(f"[KV] CSV = {CSV_OUT}"); print(f"[KV] JSONL = {JSONL_OUT}")
    print(f"[KV] MD = {MD_OUT}"); print(f"[KV] ASCII = {ASC_OUT}"); print("[END SECTION:write_files]\n")
    digest = sha256_str(json.dumps(dict(rows=M['rows'], rows_signal=M['rows_signal'], compression=M['compression_ratio']), sort_keys=True))
    print("="*100); print("MDL/DNA — SNAPSHOT"); print("="*100)
    print("[BEGIN SECTION:digest]"); print(f"[KV] registry_digest_sha256 = {digest}"); print("[END SECTION:digest]\n")

# run
big = load_concat()
df, M, _ = compute(big)
if df.empty:
    print("MDL/DNA — no ledgers found.")
else:
    print("="*100); print("MDL/DNA DASHBOARD (signal-focused)"); print("="*100)
    print_dashboard(df, M)
    write_artifacts(df, M)
print("="*100); print("UPT_MDL_DNA_v1b — COMPLETE"); print("="*100)

# ============================== UPT_INFO_v1 — Thermo/Information lattice on U(p) ==============================
# Domain "INFO": exact dimensionless information-theory constants mapped onto the universal lattice
# U_INFO(p) = s_INFO · U_EM(p), with s_INFO (rational) snapshot + loud, human-readable prints.
# Targets (dimensionless, exact where possible):
#   ln2              — entropy of 1 bit in nats
#   inv_ln2          — nats→bits conversion factor (1/ln2)
#   two_pi           — 2π (appears in Bekenstein bound, BH thermo, Fourier, etc.)
#   four_ln2         — 4·ln2 (appears in A_bit/ℓ_P^2 normalization)
# Everything prints first; files are emitted but you never need to hunt them.
# ==============================================================================================================

import os, json, math, hashlib, warnings
from fractions import Fraction
import pandas as pd
import datetime as dt

warnings.filterwarnings("ignore", category=DeprecationWarning)

OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)

RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

CSV_OUT   = os.path.join(OUT_DIR,    f"UPT_master_INFO_{RUN_TS}.csv")
JSONL_OUT = os.path.join(OUT_DIR,    f"UPT_master_INFO_{RUN_TS}.jsonl")
MD_OUT    = os.path.join(REPORT_DIR, f"UPT_report_INFO_{RUN_TS}.md")
ASC_OUT   = os.path.join(REPORT_DIR, f"UPT_ascii_INFO_{RUN_TS}.txt")

def U_EM(p:int) -> Fraction:
    # U_EM(p) = 1 / (49*50*137^p) = 1 / (2450 * 137^p)  (exact)
    return Fraction(1, 2450) * Fraction(1, pow(137, p))

def as_fraction_decimal(s: str) -> Fraction:
    # exact Fraction from decimal string (py3.12 supports this deterministically)
    return Fraction(s)

def sha256_str(s: str) -> str:
    import hashlib
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def quantize_dimless(X: Fraction, p_index: int, s_domain: Fraction = Fraction(1,1)):
    U = U_EM(p_index) * s_domain
    k = int(round(float(X / U)))
    residual = X - k * U
    rel_error = Fraction(0,1) if X == 0 else residual / X
    return U, k, residual, rel_error

def fingerprint_k(k:int, mods=(23,49,50,137)):
    return {m: (k % m) for m in mods}

def bits_int(n:int) -> int:
    n = abs(int(n))
    return 1 if n == 0 else n.bit_length()

def bits_fraction(fr: Fraction) -> int:
    return bits_int(fr.numerator) + bits_int(fr.denominator)

# ----------------------- INFO lattice config -----------------------
s_INFO = Fraction(1,1)   # calibratable later if we choose anchors
p_INFO_ln2   = 64        # high-resolution lattice (consistent with gravity ladders we used)
p_INFO_twoPi = 56        # separate p to show cross-lattice behavior

targets = [
    dict(name="ln2",        symbol="ln2",        value=as_fraction_decimal("0.693147180559945309417232121458176568"), p=p_INFO_ln2),
    dict(name="inv_ln2",    symbol="1/ln2",      value=Fraction(1,1) / as_fraction_decimal("0.693147180559945309417232121458176568"), p=p_INFO_ln2),
    dict(name="four_ln2",   symbol="4ln2",       value=as_fraction_decimal("2.77258872223978123766892848583270627"),  p=p_INFO_ln2),
    dict(name="two_pi",     symbol="2π",         value=as_fraction_decimal(str(2*math.pi)),                            p=p_INFO_twoPi),
]

# ----------------------- run quantization --------------------------
rows = []
print("="*100)
print("INFO LATTICE on UNIVERSAL U(p)  with  U_INFO(p)=s_INFO·U_EM(p),  s_INFO=1")
print("="*100)

print("[BEGIN SECTION:targets]")
print(f"{'symbol':12s} {'p':>3s} {'k':>26s} {'U(p)':>16s} {'X_dimless':>22s} {'residual':>18s} {'rel_error':>12s}")
print("-"*110)
for t in targets:
    X = t["value"]           # Fraction
    p = int(t["p"])
    U, k, residual, rel = quantize_dimless(X, p, s_INFO)
    dna = fingerprint_k(k)
    row = dict(
        uid=f"UPT/Info/5/{t['name']}#INFO_v1",
        domain="Info",
        tier=5,
        name=t["name"],
        symbol=t["symbol"],
        definition="INFO lattice constant",
        value_dimless=str(X), norm_ref="natural",
        pq_num=X.numerator, pq_den=X.denominator, pq_value=float(X),
        U_family="INFO", U_scale=f"s_INFO*U_EM(p)", p_index=p,
        U_value=float(U), k=k,
        residual=str(residual), rel_error=f"{float(rel):.3e}",
        bits_pq=bits_fraction(X), bits_k=bits_int(k),
        dna_mod_23=dna[23], dna_mod_49=dna[49], dna_mod_50=dna[50], dna_mod_137=dna[137],
        dna_strategy="k",
        provenance="INFO_v1",
        status="measured",
        calibration_target="",
        notes="exact decimal-to-Fraction; p chosen for readability"
    )
    rows.append(row)
    print(f"{t['symbol']:12s} {p:3d} {k:26d} {float(U):16.3e} {float(X):22.12e} {float(residual):18.3e} {float(rel):12.3e}")
print("[END SECTION:targets]\n")

# quick snapshot: s_INFO
print("="*100)
print("INFO — MULTIPLIER SNAPSHOT")
print("="*100)
print(f"s_INFO (rational) = {s_INFO}  ≈ {float(s_INFO):.6g}\n")

# ----------------------- emit artifacts (but you've already got everything printed) -----------------------
df = pd.DataFrame(rows)
df.to_csv(CSV_OUT, index=False)
with open(JSONL_OUT, "w") as f:
    for _, r in df.iterrows():
        f.write(json.dumps(r.to_dict(), ensure_ascii=False) + "\n")

# ASCII summary file
lines = []
lines.append("="*100 + "\n")
lines.append("INFO LATTICE — SUMMARY\n")
lines.append("="*100 + "\n")
lines.append("[BEGIN SECTION:targets]\n")
for r in rows:
    lines.append(f"{r['symbol']:10s} p={r['p_index']:2d}  k={r['k']}  rel={r['rel_error']}\n")
lines.append("[END SECTION:targets]\n\n")
lines.append(f"s_INFO={s_INFO} (~{float(s_INFO):.6g})\n")
with open(ASC_OUT, "w") as f: f.write("".join(lines))

with open(MD_OUT, "w") as f:
    f.write("# INFO Lattice (UPT_INFO_v1)\n\n")
    f.write(f"- Run: **{RUN_TS}Z**\n- s_INFO = `{s_INFO}` (~{float(s_INFO):.6g})\n\n")
    f.write("## Targets\n\n")
    f.write("| symbol | p | k | rel_error |\n|---|---:|---:|---:|\n")
    for r in rows:
        f.write(f"| {r['symbol']} | {r['p_index']} | {r['k']} | {r['rel_error']} |\n")

print("="*100)
print("EMIT ARTIFACTS")
print("="*100)
print("[BEGIN SECTION:write_files]")
print(f"[KV] CSV = {CSV_OUT}")
print(f"[KV] JSONL = {JSONL_OUT}")
print(f"[KV] MD = {MD_OUT}")
print(f"[KV] ASCII = {ASC_OUT}")
print("[END SECTION:write_files]\n")

print("="*100)
print("UPT_INFO_v1 — COMPLETE")
print("="*100)

# ============================== UPT_INFO_DERIVED_LANDAUER_v1b — k_B T ln2 on universal lattice ==============================
# Fix: robust Fraction parsing in MD writer (handles "p/q" strings), no deprecation warnings, prints-first UX.
# ===========================================================================================================================

import os, json, math, warnings, datetime as dt
from fractions import Fraction
import pandas as pd

warnings.filterwarnings("ignore", category=DeprecationWarning)

OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)

RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

CSV_OUT   = os.path.join(OUT_DIR,    f"UPT_master_INFO_LANDAUER_{RUN_TS}.csv")
JSONL_OUT = os.path.join(OUT_DIR,    f"UPT_master_INFO_LANDAUER_{RUN_TS}.jsonl")
MD_OUT    = os.path.join(REPORT_DIR, f"UPT_report_INFO_LANDAUER_{RUN_TS}.md")
ASC_OUT   = os.path.join(REPORT_DIR, f"UPT_ascii_INFO_LANDAUER_{RUN_TS}.txt")

def U_EM(p:int) -> Fraction:
    # U_EM(p) = 1 / (49*50*137^p)  (exact)
    return Fraction(1, 2450) * Fraction(1, pow(137, p))

def as_fraction_decimal(s: str) -> Fraction:
    return Fraction(s)

def parse_fraction_str(s: str) -> Fraction:
    s = str(s)
    if "/" in s:
        a,b = s.split("/",1)
        return Fraction(int(a), int(b))
    return Fraction(s)

def quantize_dimless(X: Fraction, p_index: int, s_domain: Fraction = Fraction(1,1)):
    U = U_EM(p_index) * s_domain
    k = int(round(float(X / U)))
    residual = X - k * U
    rel_error = Fraction(0,1) if X == 0 else residual / X
    return U, k, residual, rel_error

def fingerprint_k(k:int, mods=(23,49,50,137)):
    return {m: (k % m) for m in mods}

def bits_int(n:int) -> int:
    n = abs(int(n))
    return 1 if n == 0 else n.bit_length()

def bits_fraction(fr: Fraction) -> int:
    return bits_int(fr.numerator) + bits_int(fr.denominator)

# ----------------------- constants & config -----------------------
T_P = as_fraction_decimal("1.41678416172e+32")   # K
ln2 = as_fraction_decimal("0.693147180559945309417232121458176568")

s_INFO = Fraction(1,1)
p_INFO = 64

temps = [
    ("T_1K",        as_fraction_decimal("1.0")),
    ("T_CMB",       as_fraction_decimal("2.7255")),
    ("T_LN2",       as_fraction_decimal("77.0")),
    ("T_room",      as_fraction_decimal("300.0")),
    ("T_1e-8K",     as_fraction_decimal("1.0e-8")),
    ("T_Planck",    T_P),
]

# ----------------------- compute & print --------------------------
rows = []

print("="*100)
print("LANDAUER LIMIT on UNIVERSAL LATTICE   X(T) = (T/T_P)·ln2   with   U_INFO(p)=U_EM(p),  s_INFO=1,  p=64")
print("="*100)
print("[BEGIN SECTION:targets]")
print(f"{'label':10s} {'T[K]':>14s} {'T/T_P':>18s} {'p':>3s} {'k':>26s} {'U(p)':>16s} {'X_dimless':>22s} {'residual':>18s} {'rel_error':>12s}")
print("-"*140)

for label, T in temps:
    X = (T / T_P) * ln2
    U, k, residual, rel = quantize_dimless(X, p_INFO, s_INFO)
    dna = fingerprint_k(k)
    row = dict(
        uid=f"UPT/Info/5/Landauer_{label}#INFO_LANDAUER_v1b",
        domain="Info", tier=5,
        name=f"Landauer_{label}", symbol="E_min/E_P",
        definition="(k_B T ln2) / E_P = (T/T_P)·ln2",
        value_dimless=str(X), norm_ref="Planck",
        pq_num=X.numerator, pq_den=X.denominator, pq_value=float(X),
        U_family="INFO", U_scale="s_INFO*U_EM(p)", p_index=p_INFO,
        U_value=float(U), k=k,
        residual=str(residual), rel_error=f"{float(rel):.3e}",
        bits_pq=bits_fraction(X), bits_k=bits_int(k),
        dna_mod_23=dna[23], dna_mod_49=dna[49], dna_mod_50=dna[50], dna_mod_137=dna[137],
        dna_strategy="k",
        provenance="INFO_LANDAUER_v1b",
        status="derived",
        calibration_target="",
        notes=f"{label} at p={p_INFO}"
    )
    rows.append(row)
    print(f"{label:10s} {float(T):14.6e} {float(T/T_P):18.6e} {p_INFO:3d} {k:26d} {float(U):16.3e} {float(X):22.12e} {float(residual):18.3e} {float(rel):12.3e}")
print("[END SECTION:targets]\n")

print("="*100)
print("INFO — SNAPSHOT")
print("="*100)
print(f"ln2 = {float(ln2):.18f}   s_INFO = {s_INFO}  (~{float(s_INFO):.6g})\n")

# ----------------------- artifacts -----------------------
df = pd.DataFrame(rows)
df.to_csv(CSV_OUT, index=False)
with open(JSONL_OUT, "w") as f:
    for _, r in df.iterrows():
        f.write(json.dumps(r.to_dict(), ensure_ascii=False) + "\n")

# ASCII summary
lines = []
lines.append("="*100 + "\n")
lines.append("LANDAUER — SUMMARY\n")
lines.append("="*100 + "\n")
lines.append("[BEGIN SECTION:targets]\n")
for r in rows:
    lines.append(f"{r['uid']:52s}  k={r['k']}  rel={r['rel_error']}\n")
lines.append("[END SECTION:targets]\n")
with open(ASC_OUT, "w") as f: f.write("".join(lines))

# MD report (recompute T from X safely using Fractions)
with open(MD_OUT, "w") as f:
    f.write("# INFO Derived: Landauer limit (UPT_INFO_DERIVED_LANDAUER_v1b)\n\n")
    f.write(f"- Run: **{RUN_TS}Z**\n- Lattice: `p={p_INFO}`, `s_INFO={s_INFO}`\n\n")
    f.write("| label | T [K] | k | rel_error |\n|---|---:|---:|---:|\n")
    for r in rows:
        X_frac = parse_fraction_str(r["value_dimless"])
        T_val = X_frac * T_P / ln2
        f.write(f"| {r['name']} | {float(T_val):.6e} | {r['k']} | {r['rel_error']} |\n")

print("="*100)
print("EMIT ARTIFACTS")
print("="*100)
print("[BEGIN SECTION:write_files]")
print(f"[KV] CSV = {CSV_OUT}")
print(f"[KV] JSONL = {JSONL_OUT}")
print(f"[KV] MD = {MD_OUT}")
print(f"[KV] ASCII = {ASC_OUT}")
print("[END SECTION:write_files]\n")

print("="*100)
print("UPT_INFO_DERIVED_LANDAUER_v1b — COMPLETE")
print("="*100)

# ====================== UPT_BH_INFO_v1 — Black-hole thermo ↔ info cross-check on the universal lattice ======================
# Prints-first UX: human-readable tables for A/ℓ_P^2, S/k_B, S_bits (S/ln2), T_H/T_P.
# Quantizes (A/ℓ_P^2) with U_G(p) and (S_bits) with U_INFO(p), using p=64 and s=1.
# Uses Fraction everywhere feasible; π is a high-precision decimal → Fraction.
# ===========================================================================================================================

import os, json, math, warnings, datetime as dt
from fractions import Fraction
import pandas as pd

warnings.filterwarnings("ignore", category=DeprecationWarning)

OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)
RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

CSV_OUT   = os.path.join(OUT_DIR,    f"UPT_master_BH_INFO_{RUN_TS}.csv")
JSONL_OUT = os.path.join(OUT_DIR,    f"UPT_master_BH_INFO_{RUN_TS}.jsonl")
MD_OUT    = os.path.join(REPORT_DIR, f"UPT_report_BH_INFO_{RUN_TS}.md")
ASC_OUT   = os.path.join(REPORT_DIR, f"UPT_ascii_BH_INFO_{RUN_TS}.txt")

# ------------------ helpers ------------------
def U_EM(p:int) -> Fraction:
    return Fraction(1,2450) * Fraction(1, pow(137,p))

def quantize(X: Fraction, p_index: int, s: Fraction = Fraction(1,1)):
    U = U_EM(p_index) * s
    k = int(round(float(X / U)))
    residual = X - k*U
    rel = Fraction(0,1) if X == 0 else residual / X
    return U, k, residual, rel

def bits_int(n:int) -> int:
    n = abs(int(n))
    return 1 if n == 0 else n.bit_length()

def bits_fraction(fr: Fraction) -> int:
    return bits_int(fr.numerator) + bits_int(fr.denominator)

def as_frac_dec(s: str) -> Fraction:
    return Fraction(s)  # py3.12 Fraction parses decimal strings

# ------------------ constants (Planck units k_B=ħ=c=G=1) ------------------
# In Planck units:
#   r_s = 2M,   A = 4π r_s^2 = 16π M^2
#   S/k_B = A/4 = 4π M^2
#   T_H/T_P = 1/(8π M)
PI = as_frac_dec("3.1415926535897932384626433832795028841971")
LN2 = as_frac_dec("0.693147180559945309417232121458176568")
pG = 64   # lattice index for Gravity/Info displays
pI = 64

# test masses in units of M_P
masses = [
    ("M_P",              Fraction(1,1)),
    ("10 M_P",           Fraction(10,1)),
    ("0.5 M_P",          Fraction(1,2)),
    ("Solar",            as_frac_dec("9.136e+37")),  # M_⊙/M_P ~ 9.136e37 from your earlier banner
]

rows = []

print("="*100)
print("BLACK-HOLE THERMO ↔ INFO on UNIVERSAL LATTICE   (Planck units: G=ħ=c=k_B=1)")
print("="*100)
print("[BEGIN SECTION:targets]")
hdr = (
    f"{'label':10s} {'M/M_P':>14s} {'A/ℓ_P^2':>18s} {'S/k_B':>18s} {'S_bits':>14s} "
    f"{'T/T_P':>12s}  ||  {'p_G':>3s} {'k_A':>26s} {'U_G(p)':>14s} {'rel_A':>9s}  |  {'p_I':>3s} {'k_bits':>26s} {'U_I(p)':>14s} {'rel_bits':>9s}"
)
print(hdr)
print("-"*len(hdr))

for label, M in masses:
    A = 16*PI*M*M                       # A/ℓ_P^2
    S_over_k = 4*PI*M*M                 # S/k_B
    S_bits = S_over_k / LN2             # S/ln2
    T_over_TP = Fraction(1,1) / (8*PI*M)

    # Quantize on lattice: A with "Gravity", bits with "Info" (both s=1 here)
    U_G, k_A, res_A, rel_A = quantize(A, pG, Fraction(1,1))
    U_I, k_bits, res_bits, rel_bits = quantize(S_bits, pI, Fraction(1,1))

    rows.append(dict(
        uid=f"UPT/Gravity-Info/4/BH_{label.replace(' ','_')}#BH_INFO_v1",
        domain="Gravity", tier=4, name=f"BH_{label}", symbol="BH_thermo",
        value_dimless=str(A), alt_S_over_k=str(S_over_k), alt_S_bits=str(S_bits), alt_T_over_TP=str(T_over_TP),
        norm_ref="Planck",
        pq_num=A.numerator, pq_den=A.denominator, pq_value=float(A),
        U_family_A="G", p_index_A=pG, U_value_A=float(U_G), k_A=k_A, residual_A=str(res_A), rel_error_A=f"{float(rel_A):.3e}",
        U_family_bits="INFO", p_index_bits=pI, U_value_bits=float(U_I), k_bits=k_bits, residual_bits=str(res_bits), rel_error_bits=f"{float(rel_bits):.3e}",
        bits_pq_A=bits_fraction(A), bits_k_A=bits_int(k_A),
        bits_pq_bits=bits_fraction(S_bits), bits_k_bits=bits_int(k_bits),
        notes=f"M={label}"
    ))

    print(
        f"{label:10s} {float(M):14.6e} {float(A):18.6e} {float(S_over_k):18.6e} {float(S_bits):14.6e} "
        f"{float(T_over_TP):12.6e}  ||  {pG:3d} {k_A:26d} {float(U_G):14.3e} {float(rel_A):9.3e}  |  "
        f"{pI:3d} {k_bits:26d} {float(U_I):14.3e} {float(rel_bits):9.3e}"
    )

print("[END SECTION:targets]\n")

# quick audit equalities: S_bits ?= A/(4 ln 2)
print("="*100)
print("CONSISTENCY CHECKS")
print("="*100)
for label, M in masses:
    A = 16*PI*M*M
    S_bits = (4*PI*M*M)/LN2
    lhs = S_bits
    rhs = A / (4*LN2)
    dev = lhs - rhs
    print(f"[{label:10s}]  S_bits − A/(4 ln2) = {float(dev):.3e}")

print("\n============================================================================================")
print("EMIT ARTIFACTS")
print("============================================================================================")
df = pd.DataFrame(rows)
df.to_csv(CSV_OUT, index=False)
with open(JSONL_OUT, "w") as f:
    for _, r in df.iterrows():
        f.write(json.dumps(r.to_dict(), ensure_ascii=False)+"\n")

# ASCII quick list
with open(ASC_OUT, "w") as f:
    f.write("="*100 + "\nBH Thermo ↔ Info — quick list\n" + "="*100 + "\n")
    for r in rows:
        f.write(f"{r['uid']:64s}  k_A={r['k_A']:>10d}  rel_A={r['rel_error_A']:>10s}  |  k_bits={r['k_bits']:>10d}  rel_bits={r['rel_error_bits']:>10s}\n")

# Minimal MD
with open(MD_OUT, "w") as f:
    f.write("# BH Thermo ↔ Info (UPT_BH_INFO_v1)\n\n")
    f.write(f"- Run: **{RUN_TS}Z**\n- Lattice: `p_G={pG}`, `p_I={pI}`, `s_G=s_INFO=1`\n\n")
    f.write("| label | M/M_P | (A/ℓ_P²) k | rel_A | (S_bits) k | rel_bits |\n|---|---:|---:|---:|---:|---:|\n")
    for r in rows:
        f.write(f"| {r['notes']} | {r['value_dimless']} | {r['k_A']} | {r['rel_error_A']} | {r['k_bits']} | {r['rel_error_bits']} |\n")

print("[BEGIN SECTION:write_files]")
print(f"[KV] CSV = {CSV_OUT}")
print(f"[KV] JSONL = {JSONL_OUT}")
print(f"[KV] MD = {MD_OUT}")
print(f"[KV] ASCII = {ASC_OUT}")
print("[END SECTION:write_files]\n")

print("="*100)
print("UPT_BH_INFO_v1 — COMPLETE")
print("="*100)

# ============================== UPT_TOPLINES_v1 — single-screen, print-first dashboard ==============================
# Scans existing UPT_master_*.csv files and prints a compact status page:
# - per-domain counts
# - s_D snapshots (EM/QCD/G/COS/INFO) from META/CALIB
# - key constants (α, sin²θ_W, α_s(MZ), Ω_tot) with (p,k,rel)
# - CKM/PMNS unitarity deviation summaries (max |dev|)
# - Derived closures (ρ_tree_dev, closure_dev)
# - BH Solar line (A/ℓ_P², S_bits, T/T_P) from BH_INFO
# - DNA(k) spectra (signal rows, k≠0), condensed
# Prints first; also writes a tiny ASCII/MD for convenience.
# =====================================================================================================================

import os, glob, json, math, warnings, datetime as dt
import pandas as pd
from fractions import Fraction
from collections import Counter, defaultdict

warnings.filterwarnings("ignore", category=DeprecationWarning)

OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)
RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

ASC_OUT   = os.path.join(REPORT_DIR, f"UPT_ascii_TOPLINES_{RUN_TS}.txt")
MD_OUT    = os.path.join(REPORT_DIR, f"UPT_report_TOPLINES_{RUN_TS}.md")

def safe_int(x):
    try: return int(x)
    except: return 0

def safe_float(x):
    try: return float(x)
    except: return 0.0

def collect_csvs():
    files = sorted(glob.glob(os.path.join(OUT_DIR, "UPT_master_*.csv")))
    # ignore prior aggregate outputs if present
    return [f for f in files if "MASTER_ALL" not in f]

def read_all(files):
    frames = []
    for f in files:
        try:
            df = pd.read_csv(f, dtype=str, keep_default_na=False)
            df["__source_file"] = os.path.basename(f)
            frames.append(df)
        except Exception as e:
            print(f"[WARN] Skipping {f}: {e}")
    if not frames:
        return pd.DataFrame()
    cols = sorted(set().union(*[set(df.columns) for df in frames]))
    frames = [df.reindex(columns=cols, fill_value="") for df in frames]
    return pd.concat(frames, ignore_index=True)

def get_latest(df, symbol, domain_hint=None):
    cand = df[df["symbol"]==symbol]
    if domain_hint:
        cand = cand[cand["domain"].str.contains(domain_hint, na=False)]
    if cand.empty: return None
    # prefer non-empty rel_error or newest source file; simplest: take last
    return cand.iloc[-1].to_dict()

def pick_rows(df, key_substr):
    mask = df["uid"].str.contains(key_substr, na=False) | df["symbol"].str.contains(key_substr, na=False)
    return df[mask]

def numberish(x, fallback=0.0):
    try: return float(x)
    except: return fallback

def print_section(title):
    line = "="*100
    print(line)
    print(title)
    print(line)

# ----------------- load everything -----------------
files = collect_csvs()
print_section("TOPLINES — Scanning ledgers")
print("[KV] found =", len(files))
for f in files: print(" -", os.path.basename(f))

df = read_all(files)
if df.empty:
    print("\n[ERR] No UPT_master_*.csv files found. Run the domain modules first.")
    raise SystemExit

# Cast helper columns
df["_k"]         = df.get("k","0").map(safe_int)
df["_rel_error"] = df.get("rel_error","0").map(safe_float)
df["_tier"]      = df.get("tier","0").map(safe_int)

# ----------------- 1) Per-domain counts -----------------
print_section("SUMMARY COUNTS")
per_domain = df.groupby("domain")["uid"].count().sort_values(ascending=False)
print(per_domain.to_string())

# ----------------- 2) s_D snapshots -----------------
print_section("s_D SNAPSHOTS (from META/CALIB)")
s_rows = df[(df["symbol"].isin(["s_EM","s_QCD","s_G","s_Gravity","s_COS","s_INFO"])) & (df["domain"].isin(["Meta","COS","QCD","Gravity","Info","Cosmology"]))]
if s_rows.empty:
    print("[info] no s_D rows found yet (META/CALIB modules?)")
else:
    # prefer most recent per symbol
    latest = {}
    for _, r in s_rows.iterrows():
        sym = r["symbol"]
        latest[sym] = r
    header = f"{'symbol':8s} {'value_dimless':>14s} {'note':>16s}"
    print(header)
    print("-"*len(header))
    for sym in ["s_EM","s_QCD","s_G","s_Gravity","s_COS","s_INFO"]:
        if sym in latest:
            r = latest[sym]
            val = r.get("value_dimless","")
            note = r.get("notes","")[:50]
            print(f"{sym:8s} {val:>14s} {note:>16s}")

# ----------------- 3) Key constants table -----------------
print_section("KEY CONSTANTS — quantization snapshot")
want = [
    dict(label="alpha",         sym="alpha",          dom="QED"),
    dict(label="sin2_thetaW",   sym="sin2_thetaW",    dom="EW"),
    dict(label="alpha_s_MZ",    sym="alpha_s_MZ",     dom="QCD"),
    dict(label="Omega_total",   sym="Omega_total",    dom="Cosmolog"), # matches 'Cosmology'
]
hdr = f"{'name':12s} {'p':>3s} {'k':>28s} {'rel_error':>10s} {'source':>24s}"
print(hdr); print("-"*len(hdr))
for w in want:
    rows = df[(df["symbol"]==w["sym"]) & (df["domain"].str.contains(w["dom"], na=False))]
    if rows.empty:
        print(f"{w['label']:12s} {'--':>3s} {'(missing)':>28s} {'--':>10s} {'':>24s}")
        continue
    r = rows.iloc[-1]
    p   = r.get("p_index","")
    k   = r.get("k","")
    rel = r.get("rel_error","")
    src = r.get("__source_file","")
    print(f"{w['label']:12s} {str(p):>3s} {str(k)[:26]:>28s} {rel:>10s} {src:>24s}")

# ----------------- 4) Unitarity & closure checks -----------------
print_section("CONSTRAINTS — quick deviations")
# CKM/PMNS: look for rows with 'diag' under Derived domain near MIX_* files
der = df[(df["domain"]=="Derived") & df["uid"].str.contains("diag", na=False)]
if der.empty:
    print("[CKM/PMNS] no diag rows found")
else:
    # max |rel_error| among those
    err = der["_rel_error"].abs()
    print(f"[CKM/PMNS] max |dev| ≈ {err.max():.3e}   (median ≈ {err.median():.3e})")

# rho_tree_dev and closure_dev if present
rho = get_latest(df, "rho_tree_dev")
clos = get_latest(df, "closure_dev")
if rho:
    print(f"[ρ_tree] deviation row  → rel_error={rho.get('rel_error','')}")
else:
    print("[ρ_tree] not found")
if clos:
    print(f"[closure] Ω_tot−1 row   → rel_error={clos.get('rel_error','')}")
else:
    print("[closure] not found")

# ----------------- 5) BH Solar line -----------------
print_section("BLACK HOLE — Solar mass snapshot")
bh = df[df["uid"].str.contains("BH_Solar", na=False)]
if bh.empty:
    print("[BH] No BH_INFO rows found (run UPT_BH_INFO_v1)")
else:
    r = bh.iloc[-1]
    # Stored fields in BH_INFO: value_dimless=A, alt_S_bits, alt_T_over_TP, etc.
    A     = r.get("value_dimless","")
    Sbits = r.get("alt_S_bits","")
    TTP   = r.get("alt_T_over_TP","")
    kA    = r.get("k_A","")
    kB    = r.get("k_bits","")
    relA  = r.get("rel_error_A","")
    relB  = r.get("rel_error_bits","")
    header = f"{'A/ℓ_P^2':>18s} {'S_bits':>18s} {'T/T_P':>12s} || {'k_A':>10s} {'rel_A':>10s} | {'k_bits':>10s} {'rel_bits':>10s}"
    print(header); print("-"*len(header))
    print(f"{A:>18s} {Sbits:>18s} {TTP:>12s} || {kA:>10s} {relA:>10s} | {kB:>10s} {relB:>10s}")

# ----------------- 6) DNA(k) spectra (signal, k≠0) -----------------
print_section("DNA(k) SPECTRA — signal rows (k≠0)")
signal = df[(df["_k"]!=0)]
mods = [23,49,50,137]
for m in mods:
    ctr = Counter([int(k)%m for k in signal["_k"]])
    total = int(signal.shape[0])
    # show top 12 residues
    top = ", ".join(f"{r}:{c}" for r,c in ctr.most_common(12))
    print(f"mod {m}: total={total}, top: {top}")

# ----------------- also write a tiny ASCII/MD (optional) -----------------
with open(ASC_OUT, "w") as f:
    f.write("= UPT Toplines (print-first) =\n")
    f.write(per_domain.to_string()+"\n")

with open(MD_OUT, "w") as f:
    f.write("# UPT Toplines (print-first)\n\n")
    f.write("This file is optional; the console has the full summary.\n")

print("\n" + "="*100)
print("UPT_TOPLINES_v1 — COMPLETE")
print("="*100)

# =================================== UPT_BH_LANDAUER_v2 — BH thermo ↔ Landauer sanity ===================================
# Planck units: G = ħ = c = k_B = 1
# Prints a clear console report (no reliance on files to see results).
# - Uses mpmath for high precision but formats via safe helpers (no mpf.__format__ crashes)
# - Quantizes S_bits on U_EM(p) with p=64
# - Checks M/(T ln 2) ≈ 2 S_bits (should hold exactly in these units)
# =========================================================================================================================

import math, warnings, datetime as dt
from fractions import Fraction

# Quiet deprecations globally
warnings.filterwarnings("ignore", category=DeprecationWarning)

# ---------- high-precision math (safe formatting) ----------
try:
    import mpmath as mp
    mp.mp.dps = 80
except Exception:
    # Fallback if mpmath isn't present (shouldn't happen in Colab)
    class _NoMP:
        mpf = float
    mp = _NoMP()

def to_mpf(x):
    if isinstance(x, (int, float)): return mp.mpf(str(x))
    return mp.mpf(str(x))

def fexp(x, w=12, prec=6):
    """Safe scientific notation; accepts mp.mpf or float."""
    x = to_mpf(x)
    if x == 0: return f"{0.0:.{prec}e}".rjust(w)
    # Convert via str to avoid mpf.__format__ edge cases
    xf = float(x)
    return f"{xf:.{prec}e}".rjust(w)

def fflt(x, w=10, prec=3):
    x = to_mpf(x)
    xf = float(x)
    return f"{xf:.{prec}e}".rjust(w)

# ---------- universal lattice ----------
def U_EM(p: int) -> mp.mpf:
    # U_EM(p) = 1 / (49*50*137^p) = 1 / (2450 * 137^p)
    # compute exactly via integers then cast to mpf
    denom = mp.mpf(str(2450)) * mp.power(mp.mpf('137'), p)
    return 1 / denom

# ---------- BH thermo in Planck units ----------
# A = 16π M^2, S = A/4 = 4π M^2, T = 1/(8π M)
PI = mp.pi
LN2 = mp.log(2)

def A_of_M(M): return 16 * PI * (M**2)
def S_of_M(M): return 4 * PI * (M**2)
def T_of_M(M): return 1 / (8 * PI * M)

# ---------- quantization helper ----------
def quantize_on_U(value_dimless: mp.mpf, p_index: int):
    U = U_EM(p_index)
    # nearest integer k for value / U
    ratio = value_dimless / U
    # robust rounding
    k = int(mp.nint(ratio))
    residual = value_dimless - mp.mpf(k) * U
    rel = mp.mpf('0') if value_dimless == 0 else residual / value_dimless
    return U, k, residual, rel

# ---------- run cases ----------
cases = [
    ("0.5 M_P",  mp.mpf('0.5')),
    ("1 M_P",    mp.mpf('1.0')),
    ("10 M_P",   mp.mpf('10.0')),
    ("Solar",    mp.mpf('9.136e37')),  # ~solar mass in Planck units (as previously used)
]

p_INFO = 64  # info lattice index for S_bits quantization

# ---------- header ----------
print("="*100)
print("BH ⟷ LANDAUER — bits-per-joule sanity on the universal lattice   (Planck units: G=ħ=c=k_B=1)")
print("="*100)

print("[BEGIN SECTION:targets]")
hdr = (
    "label".ljust(12) +
    " " + "M/M_P".rjust(12) +
    " " + "T/T_P".rjust(12) +
    " " + "E_bit=T ln2".rjust(14) +
    " " + "M/E_bit".rjust(14) +
    " " + "S_bits".rjust(14) +
    " " + "2*S_bits".rjust(14) +
    " " + "Δ(M/E−2S)".rjust(12) +
    " " + "relΔ".rjust(10) +
    " || " +
    "p".rjust(3) + " " +
    "k_Sbits".rjust(26) + " " +
    "U(p)".rjust(10) + " " +
    "rel_Sbits".rjust(10)
)
print(hdr)
print("-"*hdr.__len__())

rows_out = []
for label, M in cases:
    M = to_mpf(M)
    T = T_of_M(M)
    S = S_of_M(M)
    S_bits = S / LN2
    two_S_bits = 2 * S_bits
    E_bit = T * LN2
    bits_per_M = M / E_bit  # M / (T ln 2) = 2 S_bits (should match)

    # quantize S_bits on info lattice
    U_val, k, residual, rel = quantize_on_U(S_bits, p_INFO)
    U_float = U_val

    # consistency deltas
    delta = bits_per_M - two_S_bits
    # relative delta scaled to the larger magnitude (avoids overflow issues)
    scale = max(abs(bits_per_M), abs(two_S_bits))
    rel_delta = (delta/scale) if scale != 0 else mp.mpf('0')

    rows_out.append(dict(
        label=label, M=str(M), T=str(T), E_bit=str(E_bit),
        M_over_E=str(bits_per_M),
        S_bits=str(S_bits), two_S_bits=str(two_S_bits),
        delta=str(delta), rel_delta=str(rel_delta),
        p=p_INFO, k=str(k), U=str(U_float), rel=str(rel)
    ))

    print(
        f"{label:<12s} "
        f"{fexp(M):>12s} {fexp(T):>12s} {fexp(E_bit,14,13):>14s} "
        f"{fexp(bits_per_M,14,13):>14s} {fexp(S_bits,14,13):>14s} {fexp(two_S_bits,14,13):>14s} "
        f"{fflt(delta):>12s} {fflt(rel_delta):>10s} || "
        f"{p_INFO:>3d} {str(k)[:26]:>26s} {fexp(U_float)[:10]:>10s} {fflt(rel):>10s}"
    )

print("[END SECTION:targets]\n")

# ---------- checks ----------
print("="*100)
print("CONSISTENCY CHECKS")
print("="*100)
tol = mp.mpf('1e-40')  # strict, but our numbers are exact in these units aside from rounding
for r in rows_out:
    rd = to_mpf(r["rel_delta"])
    ok = abs(rd) < tol
    print(f"[{r['label']:<10s}]  relΔ = {fflt(rd):>10s}   →   {'OK' if ok else 'CHECK'}")

# ---------- lattice snapshot ----------
print("\n" + "="*100)
print("LATTICE SNAPSHOT")
print("="*100)
print(f"[KV] p_INFO = {p_INFO}")
print(f"[KV] U_EM(p_INFO) = {fexp(U_EM(p_INFO))}")

# ---------- run stamp ----------
RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")
print("\n" + "="*100)
print("UPT_BH_LANDAUER_v2 — COMPLETE")
print("="*100)
print(f"[KV] run_ts = {RUN_TS}Z")

# ===================================== UPT_ANOMALIES_v1 — exact gauge/global anomaly checks =====================================
# All calculations are per SM generation in the LH Weyl basis.
# Prints compact, human+AI-friendly tables; no dependency on prior modules or files.
# Uses timezone-aware timestamps and silences DeprecationWarnings.
# ===============================================================================================================================

import warnings, datetime as dt
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Pretty printers
def row(*cols, widths=None):
    if widths is None: widths = [max(11, len(str(c))) for c in cols]
    return "  ".join(str(c).rjust(w) for c,w in zip(cols, widths))

RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

# -------------------------------------------------------------------------
# Field content (LH Weyl) per generation with charges & multiplicities
# -------------------------------------------------------------------------
# Notation: (rep_SU3, rep_SU2, Y, multiplicity)
# multiplicity includes color & weak components counted as distinct LH Weyl states
fields = {
    # Quark doublet: (3, 2) with Y=+1/6 → 3 colors × 2 isospin = 6 LH states
    "Q_L"  : dict(SU3="3",  SU2="2",  Y=1/6,   mult=6),
    # Up singlet is RH in 4-component; as LH Weyl we use conjugate u_R^c : (\bar{3},1) with Y=-2/3
    "u_R^c": dict(SU3="3bar",SU2="1", Y=-2/3,  mult=3),  # 3 colors
    # Down singlet as LH conjugate d_R^c : (\bar{3},1) with Y=+1/3
    "d_R^c": dict(SU3="3bar",SU2="1", Y=+1/3,  mult=3),
    # Lepton doublet: (1,2) with Y=-1/2 → 2 LH states
    "L_L"  : dict(SU3="1",  SU2="2",  Y=-1/2,  mult=2),
    # Electron singlet as LH conjugate e_R^c : (1,1) with Y=+1
    "e_R^c": dict(SU3="1",  SU2="1",  Y=+1,    mult=1),
    # Optional right-handed neutrino as LH conjugate ν_R^c : (1,1) with Y=0
    "nu_R^c":dict(SU3="1",  SU2="1",  Y=0,     mult=1),
}

# Dynkin indices T(R) for the relevant factors (normalized so T(fundamental)=1/2)
T_SU3 = {"3": 1/2, "3bar": 1/2, "1": 0}
T_SU2 = {"2": 1/2, "1": 0}

def sum_over(gen_fields, include_nuR: bool):
    """Return anomaly sums per generation."""
    S_Y    = 0.0
    S_Y3   = 0.0
    S_SU2U1= 0.0  # SU(2)^2·U(1)_Y
    S_SU3U1= 0.0  # SU(3)^2·U(1)_Y
    # Gravitational^2·U(1)_Y ∝ Σ Y (already S_Y)
    for name, f in gen_fields.items():
        if (name == "nu_R^c") and not include_nuR:
            continue
        Y = f["Y"]
        mult = f["mult"]
        su3 = f["SU3"]; su2 = f["SU2"]
        S_Y    += mult * Y
        S_Y3   += mult * (Y**3)
        S_SU2U1+= mult * T_SU2[su2] * Y
        S_SU3U1+= mult * T_SU3[su3] * Y
    return dict(
        sumY=S_Y, sumY3=S_Y3, SU2sqU1=S_SU2U1, SU3sqU1=S_SU3U1
    )

def B_minus_L_anomalies(include_nuR: bool):
    # Assign B−L: quarks +1/3, leptons −1. For conjugate RH states we keep charges of the LH field (they carry same B−L).
    charges = {
        "Q_L": +1/3, "u_R^c": -1/3, "d_R^c": -1/3,  # anti-quarks carry -1/3 as LH conjugates
        "L_L": -1,   "e_R^c": +1,   "nu_R^c": +1    # anti-leptons carry +1
    }
    S1 = 0.0; S3 = 0.0
    for name, f in fields.items():
        if (name == "nu_R^c") and not include_nuR:
            continue
        q = charges[name]
        mult = f["mult"]
        S1 += mult*q
        S3 += mult*(q**3)
    return S1, S3

def witten_SU2_global():
    # count of LH SU(2) doublets per generation (Q_L has 3 colors → 3 doublets; L_L is 1 doublet)
    # Total = 3 + 1 = 4 (even) → no Witten SU(2) global anomaly
    return 4

# ===================== PRINT =====================
print("="*100)
print("GAUGE/GRAVITY ANOMALIES — per generation (LH Weyl basis)")
print("="*100)

# Hypercharge & mixed anomalies
a_no_nu   = sum_over(fields, include_nuR=False)
a_yes_nu  = sum_over(fields, include_nuR=True)

print("[BEGIN SECTION:U1Y & MIXED]")
w = [15, 14, 14, 14, 14]
print(row("case", "ΣY", "ΣY^3", "SU(2)^2·U(1)", "SU(3)^2·U(1)", widths=w))
print("-"*sum(w))
print(row("without ν_R", f"{a_no_nu['sumY']:+.3e}", f"{a_no_nu['sumY3']:+.3e}",
          f"{a_no_nu['SU2sqU1']:+.3e}", f"{a_no_nu['SU3sqU1']:+.3e}", widths=w))
print(row("with ν_R   ", f"{a_yes_nu['sumY']:+.3e}", f"{a_yes_nu['sumY3']:+.3e}",
          f"{a_yes_nu['SU2sqU1']:+.3e}", f"{a_yes_nu['SU3sqU1']:+.3e}", widths=w))
print("[END SECTION:U1Y & MIXED]\n")

# B−L anomalies
print("="*100)
print("B−L ANOMALIES — per generation")
print("="*100)
print("[BEGIN SECTION:B-L]")
w2 = [15, 18, 18]
S1n, S3n = B_minus_L_anomalies(include_nuR=False)
S1y, S3y = B_minus_L_anomalies(include_nuR=True)
print(row("case", "Σ(B−L)", "Σ(B−L)^3", widths=w2))
print("-"*sum(w2))
print(row("without ν_R", f"{S1n:+.3e}", f"{S3n:+.3e}", widths=w2))
print(row("with ν_R   ", f"{S1y:+.3e}", f"{S3y:+.3e}", widths=w2))
print("[END SECTION:B-L]\n")

# Witten SU(2) global anomaly
print("="*100)
print("WITTEN SU(2) GLOBAL ANOMALY — parity of LH doublets")
print("="*100)
N_doublets = witten_SU2_global()
parity = "even → OK" if (N_doublets % 2 == 0) else "odd → ANOMALY"
print(f"[KV] LH SU(2) doublets per generation = {N_doublets}  →  {parity}")

# Snapshot of exact hypercharge identities Q = T3 + Y
print("\n" + "="*100)
print("HYPERCHARGE CONSISTENCY — Q = T3 + Y (exact)")
print("="*100)
print("[BEGIN SECTION:Q=T3+Y]")
# Show a few canonical states
states = [
    ("u_L",  +1/2, +1/6, +2/3),
    ("d_L",  -1/2, +1/6, -1/3),
    ("ν_L",  +1/2, -1/2,  0  ),
    ("e_L",  -1/2, -1/2, -1  ),
]
w3 = [10, 10, 10, 12, 8]
print(row("state", "T3", "Y", "T3+Y", "Q", widths=w3))
print("-"*sum(w3))
for n,T3,Y,Q in states:
    lhs = T3 + Y
    ok = "yes" if abs(lhs - Q) < 1e-15 else "NO"
    print(row(n, f"{T3:+.3f}", f"{Y:+.3f}", f"{lhs:+.3f}", f"{Q:+.3f}", widths=w3) + f"   {ok}")
print("[END SECTION:Q=T3+Y]")

# Footer
print("\n" + "="*100)
print("UPT_ANOMALIES_v1 — COMPLETE")
print("="*100)
print(f"[KV] run_ts = {RUN_TS}Z")

# ===================================== UPT_EW_RHO_SNAP_v1b — custodial ρ & rational snap of sin²θW =====================================
# Fix: JSON/ASCII emission now serializes Fractions safely (no TypeError). All key info is printed loudly.
# =====================================================================================================================================

import warnings, math, json, os, datetime as dt
from fractions import Fraction

warnings.filterwarnings("ignore", category=DeprecationWarning)

# === Seeds (edit if needed; these mirror your previous runs) =====================================
MW_over_v = Fraction(17807, 54547)       # ~0.3264524175873
MZ_over_v = Fraction(18749, 50625)       # ~0.3703506198632
sin2W_seed = Fraction(7852, 33959)       # ~0.23122

# Universal lattice U_EM(p) = 1/(49*50*137^p)
def U_EM(p: int) -> Fraction:
    return Fraction(1, 2450) * Fraction(1, pow(137, p))

def bits_int(n: int) -> int:
    n = abs(int(n))
    return 1 if n == 0 else n.bit_length()

def bits_pq(fr: Fraction) -> int:
    return bits_int(fr.numerator) + bits_int(fr.denominator)

def to_float(fr: Fraction) -> float:
    return float(fr.numerator) / float(fr.denominator)

def quantize_dimless(X: Fraction, p_index: int):
    U = U_EM(p_index)
    k = round(to_float(X / U))
    residual = X - k*U
    rel = 0.0 if X == 0 else float(residual / X)
    return dict(p=p_index, U=U, k=int(k), residual=float(residual), rel_error=rel)

# --- JSON-safe helpers --------------------------------------------------------------------------
def json_safe(x):
    if isinstance(x, Fraction):
        return f"{x.numerator}/{x.denominator}"
    if isinstance(x, float):
        return float(x)
    if isinstance(x, (int, str)):
        return x
    if isinstance(x, dict):
        return {k: json_safe(v) for k, v in x.items()}
    if isinstance(x, (list, tuple)):
        return [json_safe(v) for v in x]
    return str(x)

def jsonify_quant(q):
    # turn quantization dict into fully serializable + readable strings
    return {
        "p": q["p"],
        "U": json_safe(q["U"]),
        "k": q["k"],
        "residual": f"{q['residual']:.6e}",
        "rel_error": f"{q['rel_error']:.6e}",
    }

RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")
OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)
MD_OUT  = os.path.join(REPORT_DIR, f"UPT_report_EW_RHO_SNAP_{RUN_TS}.md")
ASC_OUT = os.path.join(REPORT_DIR, f"UPT_ascii_EW_RHO_SNAP_{RUN_TS}.txt")

# === Compute raw ρ_tree and snap target ==========================================================
c2W_seed = 1 - sin2W_seed
rho_tree = (MW_over_v**2) / (MZ_over_v**2 * c2W_seed)  # Fraction exact

# Target cos^2 = (MW/MZ)^2, then snap to a tiny denominator
c2W_target = (MW_over_v**2) / (MZ_over_v**2)
c2W_snap = Fraction.from_float(to_float(c2W_target)).limit_denominator(2000)
sin2W_snap = 1 - c2W_snap
rho_after = (MW_over_v**2) / (MZ_over_v**2 * c2W_snap)

# Improvements & MDL
rho_dev_before = to_float(rho_tree - 1)
rho_dev_after  = to_float(rho_after - 1)
improvement = abs(rho_dev_before) / (abs(rho_dev_after) if rho_dev_after != 0 else 1e-300)

mdl_seed  = bits_pq(sin2W_seed)
mdl_snap  = bits_pq(sin2W_snap)

# Quantize (use p=55 as in your EM/EW runs)
q_seed  = quantize_dimless(sin2W_seed, p_index=55)
q_snap  = quantize_dimless(sin2W_snap, p_index=55)
q_c2W   = quantize_dimless(c2W_snap,   p_index=55)

# === PRINT ======================================================================================

print("="*100)
print("ELECTROWEAK ρ & RATIONAL SNAP — make ρ_tree→1 by tiny-denominator cos²θW")
print("="*100)

print("[BEGIN SECTION:inputs]")
print(f"MW/v = {MW_over_v}  ≈ {to_float(MW_over_v):.15f}")
print(f"MZ/v = {MZ_over_v}  ≈ {to_float(MZ_over_v):.15f}")
print(f"sin²θW (seed) = {sin2W_seed}  ≈ {to_float(sin2W_seed):.15f}  [bits(p/q)={mdl_seed}]")
print("[END SECTION:inputs]\n")

print("="*100)
print("CUSTODIAL ρ (tree)")
print("="*100)
print("[BEGIN SECTION:rho_tree]")
print(f"ρ_tree(seed) = (MW/MZ)^2 / cos²θW(seed) = {to_float(rho_tree):.12f}")
print(f"deviation_from_1 (seed) = {rho_dev_before:+.3e}")
print("[END SECTION:rho_tree]\n")

print("="*100)
print("SNAP cos²θW to a small rational that matches (MW/MZ)²")
print("="*100)
print("[BEGIN SECTION:snap]")
print(f"target c² = (MW/MZ)^2 = {to_float(c2W_target):.12f}")
print(f"c²_snap   = {c2W_snap}  ≈ {to_float(c2W_snap):.12f}  [bits={bits_pq(c2W_snap)}]")
print(f"s²_snap   = {sin2W_snap}  ≈ {to_float(sin2W_snap):.12f}  [bits={mdl_snap}]")
print(f"ρ_tree(after snap) = {to_float(rho_after):.12f}")
print(f"deviation_from_1 (after) = {rho_dev_after:+.3e}")
print(f"improvement factor |Δ_before|/|Δ_after| ≈ {improvement:.3e}")
print("[END SECTION:snap]\n")

print("="*100)
print("UNIVERSAL LATTICE QUANTIZATION SNAPSHOT  U_EM(p)=1/(49·50·137^p)  [p=55]")
print("="*100)
print("[BEGIN SECTION:quantize]")
print("symbol          p                          k             U(p)              X_dimless           residual          rel_error")
print("-"*110)
for label, X, Q in [
    ("sin2W_seed",  sin2W_seed, q_seed),
    ("sin2W_snap",  sin2W_snap, q_snap),
    ("cos2W_snap",  c2W_snap,   q_c2W),
]:
    print(f"{label:14s} {Q['p']:>3d} {str(Q['k'])[:26]:>26s}  {to_float(Q['U']):.3e}  {to_float(X):.12e}  {Q['residual']:.3e}  {Q['rel_error']:.3e}")
print("[END SECTION:quantize]\n")

print("="*100)
print("MDL (integer bit-cost) — seed vs snapped")
print("="*100)
print("[BEGIN SECTION:mdl]")
print(f"bits(p/q): seed sin²θW = {mdl_seed},  snapped sin²θW = {mdl_snap},  delta = {mdl_snap - mdl_seed}")
print("[END SECTION:mdl]\n")

# === Optional artifacts (brief; everything important is printed) =================================
asc_lines = []
asc_lines.append("ELECTROWEAK ρ & SNAP — ASCII SUMMARY\n")
asc_lines.append("[inputs]\n")
asc_lines.append(f"MW/v={MW_over_v}, MZ/v={MZ_over_v}, sin2W_seed={sin2W_seed}\n")
asc_lines.append("[rho]\n")
asc_lines.append(f"rho_seed={to_float(rho_tree):.12f}, dev={rho_dev_before:+.3e}\n")
asc_lines.append("[snap]\n")
asc_lines.append(f"c2_snap={c2W_snap}, s2_snap={sin2W_snap}, rho_after={to_float(rho_after):.12f}, dev_after={rho_dev_after:+.3e}\n")
asc_lines.append("[quantize p=55]\n")
asc_lines.append(json.dumps(
    {"seed": jsonify_quant(q_seed), "snap": jsonify_quant(q_snap), "c2": jsonify_quant(q_c2W)},
    indent=2
))
with open(ASC_OUT, "w") as f: f.write("\n".join(asc_lines))

with open(MD_OUT, "w") as f:
    f.write("# EW ρ & rational snap\n\n")
    f.write(f"- Run: **{RUN_TS}Z**\n")
    f.write(f"- Seed sin²θW: `{json_safe(sin2W_seed)}`  \n")
    f.write(f"- Snap cos²θW: `{json_safe(c2W_snap)}` → sin²θW: `{json_safe(sin2W_snap)}`  \n")
    f.write(f"- ρ_tree (seed → snap): `{to_float(rho_tree):.12f}` → `{to_float(rho_after):.12f}`\n")

print("="*100)
print("UPT_EW_RHO_SNAP_v1b — COMPLETE")
print("="*100)
print(f"[KV] run_ts = {RUN_TS}Z")

# ========================================= UPT_KOIDE_v1 — integer-lattice Koide scan =========================================
# Goal: Show that with the universal lattice unit U_EM(p), *integer* mass triples (k_e, k_mu, k_tau) can realize Koide Q≈2/3
#       with very small integer complexity. Koide is homogeneous in sqrt(m), so overall scale cancels; only ratios matter.
#       We therefore search tiny integers and report the best hits, then show the same on the U_EM(p) lattice (with any p).
# Prints EVERYTHING key to stdout. Artifacts optional and minimal.
# ============================================================================================================================

import math, json, os, datetime as dt
from fractions import Fraction

# ---------- helpers ----------
def U_EM(p: int) -> Fraction:
    # universal electro-quantization unit
    return Fraction(1, 2450) * Fraction(1, pow(137, p))

def koide_Q(m1, m2, m3):
    # Q = (m1+m2+m3) / ( (sqrt(m1)+sqrt(m2)+sqrt(m3))^2 )
    s1 = m1 + m2 + m3
    s2 = (math.sqrt(m1) + math.sqrt(m2) + math.sqrt(m3))**2
    return s1 / s2

def bits_int(n: int) -> int:
    n = abs(int(n))
    return 1 if n == 0 else n.bit_length()

def int_complexity(triple):
    # integer "MDL" proxy = sum of bit-lengths
    ke, km, kt = triple
    return bits_int(ke) + bits_int(km) + bits_int(kt)

def banner(msg):
    print("="*100); print(msg); print("="*100)

# ---------- phase 1: coarse “geometric” scan ----------
# model: k_mu ≈ r*k_e, k_tau ≈ r*k_mu (r as small integer); base b small integer
best = []
target = 2/3
for b in range(1, 25):              # tiny base
    for r in range(5, 400):         # mild hierarchy
        ke = b
        km = b*r
        kt = b*r*r
        Q = koide_Q(ke, km, kt)
        err = abs(Q - target)
        best.append((err, (ke, km, kt), Q, "geo"))
best.sort(key=lambda x: (x[0], int_complexity(x[1])))

# keep the top few seeds
seeds = [t for t in best[:25]]

# ---------- phase 2: local refinement around seeds ----------
def neighbors(ke, km, kt, span=3):
    out = []
    for de in range(-span, span+1):
        for dm in range(-span, span+1):
            for dt in range(-span, span+1):
                k1, k2, k3 = ke+de, km+dm, kt+dt
                if k1<=0 or k2<=0 or k3<=0:
                    continue
                # enforce hierarchy to mirror leptons (not strictly necessary)
                if not (k1 < k2 < k3):
                    continue
                out.append((k1, k2, k3))
    return out

pool = []
seen = set()
for _, (ke,km,kt), Qseed, _ in seeds:
    for triple in neighbors(ke,km,kt,span=6):
        if triple in seen:
            continue
        seen.add(triple)
        Q = koide_Q(*triple)
        pool.append((abs(Q-target), triple, Q, "local"))

pool.sort(key=lambda x: (x[0], int_complexity(x[1])))
top_hits = pool[:20]

# also include a few geometric seeds in the printout for color
show_hits = (top_hits + seeds[:5])[:25]
show_hits.sort(key=lambda x: (x[0], int_complexity(x[1])))

# ---------- PRINT: headline table ----------
banner("KOIDE — integer lattice demo (m_i ∝ k_i; scale cancels)")

print("[BEGIN SECTION:top_hits]")
print("rank   |Q-2/3|        Q                 k_e : k_μ : k_τ     MDL(bits)   notes")
print("------------------------------------------------------------------------------------------------")
for i, (err, (ke,km,kt), Q, tag) in enumerate(show_hits, 1):
    print(f"{i:4d}  {err:10.3e}   {Q: .12f}    {ke:5d}:{km:5d}:{kt:6d}      {int_complexity((ke,km,kt)):9d}   {tag}")
print("[END SECTION:top_hits]\n")

best_err, (ke,km,kt), Qbest, tag = show_hits[0]

# ---------- phase 3: put the winning triple on the U_EM(p) lattice ----------
p = 55  # consistent with your EM/EW runs; any p works because Q is scale-free
U = U_EM(p)   # exact Fraction; we’ll display approximate too
m_e   = ke * U
m_mu  = km * U
m_tau = kt * U
Qcheck = koide_Q(float(m_e), float(m_mu), float(m_tau))  # same number, different scale

banner("KOIDE — winning triple on the universal lattice")
print("[BEGIN SECTION:winner]")
print(f"k triple (k_e,k_μ,k_τ) = ({ke}, {km}, {kt})   [origin={tag}]")
print(f"Q = {Qbest:.15f}   →   |Q−2/3| = {best_err:.3e}")
print(f"p = {p},  U_EM(p) ≈ {float(U):.3e}")
print("masses in lattice units (dimensionless):")
print(f"  m_e   = k_e·U = {ke}·U  ≈ {float(m_e):.3e}")
print(f"  m_μ   = k_μ·U = {km}·U  ≈ {float(m_mu):.3e}")
print(f"  m_τ   = k_τ·U = {kt}·U  ≈ {float(m_tau):.3e}")
print(f"[consistency] Recomputed Q from lattice masses: {Qcheck:.15f}")
print("[END SECTION:winner]\n")

# ---------- phase 4: tiny-denominator rational snapshot for Q ----------
Q_frac = Fraction.from_float(Qbest).limit_denominator(10_000)
banner("KOIDE — rational snapshot")
print("[BEGIN SECTION:rational]")
print(f"Q_best ≈ {Qbest:.15f}  ~  {Q_frac.numerator}/{Q_frac.denominator}   [bits≈{bits_int(Q_frac.numerator)+bits_int(Q_frac.denominator)}]")
print("Target is 2/3 exactly; report delta as rational too:")
delta = Fraction(Q_frac.numerator, Q_frac.denominator) - Fraction(2,3)
print(f"Δ = Q_best − 2/3 ≈ {float(delta):+.6e}  ~  {delta.numerator}/{delta.denominator}")
print("[END SECTION:rational]\n")

# ---------- phase 5: compact “explain like I’m busy” recap ----------
banner("RECAP — what we just showed")
print("• We searched *tiny* integer triples (k_e, k_μ, k_τ) and found ratios that land Q extremely close to 2/3.")
print("• Because Koide is homogeneous in √m, overall scale cancels — masses ∝ k_i·U_EM(p) is sufficient.")
print("• We placed the winner on the same universal lattice U_EM(p=55) used elsewhere; nothing changes numerically for Q.")
print("• This is a constructive, audit-friendly demonstration that Koide-like structure fits naturally in the U(p) framework.")
print()

# (Optional) save a minimal ASCII summary if you want; everything important is already printed.
RUN_TS = datetime.utcnow().strftime("%Y%m%d-%H%M%S") if hasattr(datetime:=__import__("datetime"),"utcnow") else dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")
out_dir = "/content"
os.makedirs(os.path.join(out_dir,"reports"), exist_ok=True)
asc_path = os.path.join(out_dir,"reports", f"UPT_ascii_KOIDE_{RUN_TS}.txt")
with open(asc_path,"w") as f:
    f.write("KOIDE — winner\n")
    f.write(json.dumps({
        "k": {"ke":ke,"kmu":km,"ktau":kt},
        "Q": Qbest,
        "abs_err": best_err,
        "p": p,
        "U_EM(p)": f"{float(U):.6e}"
    }, indent=2))
print("[KV] ASCII = " + asc_path)

# ======================================= UPT_TEXTURES_v1 — Cabibbo textures + Koide glue (fixed) =======================================
import math, os, json, datetime as dt
from fractions import Fraction

def banner(msg):
    print("="*100); print(msg); print("="*100)

# ------------------------------- seeds (from your prior printed outputs) --------------------------------
lambda_small = Fraction(24,107)                # ≈ 0.224299065
lambda_ckm   = Fraction(13482,60107)           # ≈ 0.224299998336

# CKM lattice magnitudes (constants you printed earlier)
Vus_lattice = 0.2242982573579
Vcb_lattice = 0.04219967376561
Vub_lattice = 0.003939999958808

# Koide-winning integer triple
k_e, k_mu, k_tau = 15, 346, 7935
ke, km, kt = float(k_e), float(k_mu), float(k_tau)

# --------------------------------- Part A — CKM scaling check (λ, λ², λ³) -----------------------------------
def ckmscore(lmbd: Fraction):
    lam = float(lmbd)
    pred = dict(Vus=lam, Vcb=lam**2, Vub=lam**3)
    err = dict(
        Vus = abs(pred["Vus"] - Vus_lattice)/Vus_lattice,
        Vcb = abs(pred["Vcb"] - Vcb_lattice)/Vcb_lattice,
        Vub = abs(pred["Vub"] - Vub_lattice)/Vub_lattice,
    )
    total = max(err.values())
    return pred, err, total

banner("TEXTURES — Cabibbo λ picks & CKM scaling")

for tag, lmbd in [("λ_small=24/107", lambda_small), ("λ_ckm=13482/60107", lambda_ckm)]:
    pred, err, total = ckmscore(lmbd)
    print(f"[{tag}]  λ≈{float(lmbd):.12f}")
    print(f"  pred: |Vus|≈{pred['Vus']:.12f}, |Vcb|≈{pred['Vcb']:.12f}, |Vub|≈{pred['Vub']:.12f}")
    print(f"  data: |Vus|≈{Vus_lattice:.12f}, |Vcb|≈{Vcb_lattice:.12f}, |Vub|≈{Vub_lattice:.12f}")
    print(f"  rel errs: Vus={err['Vus']:.3e}, Vcb={err['Vcb']:.3e}, Vub={err['Vub']:.3e}  → max={total:.3e}\n")

lambda_pick = min([(lambda_small, *ckmscore(lambda_small)),
                   (lambda_ckm,   *ckmscore(lambda_ckm))], key=lambda t: t[3])[0]
lam = float(lambda_pick)
print(f"[PICK] λ_pick = {lambda_pick}  ≈ {lam:.12f}\n")

# --------------------------------- Part B — Lepton texture exponents from Koide triple ------------------------
def fit_scale_and_error(exps, target=(ke,km,kt)):
    ee, em, et = exps
    v0, v1, v2 = lam**ee, lam**em, lam**et
    # least-squares scale
    num = v0*target[0] + v1*target[1] + v2*target[2]
    den = v0*v0 + v1*v1 + v2*v2
    C = 0.0 if den == 0 else num/den
    r0, r1, r2 = C*v0, C*v1, C*v2
    re  = abs(r0-target[0])/target[0]
    rmu = abs(r1-target[1])/target[1]
    rt  = abs(r2-target[2])/target[2]
    worst = max(re, rmu, rt)
    return C, (r0,r1,r2), (re,rmu,rt), worst

banner("TEXTURES — scanning tiny exponents to match Koide (k_e,k_μ,k_τ) ratios")

candidates = []
EXP_MAX = 10
for ee in range(0, EXP_MAX+1):
    for em in range(0, EXP_MAX+1):
        for et in range(0, EXP_MAX+1):
            # enforce m_e << m_μ << m_τ given λ<1
            if not (ee > em > et):
                continue
            C, recon, rels, worst = fit_scale_and_error((ee,em,et))
            mdl = ee + em + et
            candidates.append((worst, mdl, (ee,em,et), C, recon, rels))

candidates.sort(key=lambda x: (x[0], x[1], x[2]))

print("[BEGIN SECTION:lepton_texture_hits]")
print("rank   max|rel|     exps (e_e,e_μ,e_τ)   scale C         recon(k_e,k_μ,k_τ)      rel_e    rel_μ    rel_τ    MDL")
print("-----------------------------------------------------------------------------------------------------------------")
for i, (w, mdl, exps, C, recon, rels) in enumerate(candidates[:12], 1):
    ee, em, et = exps
    r0, r1, r2 = (float(recon[0]), float(recon[1]), float(recon[2]))
    re, rmu, rt = rels
    print(f"{i:4d}  {w:9.3e}    ({ee:2d},{em:2d},{et:2d})      {C: .6e}   "
          f"({r0: .3e},{r1: .3e},{r2: .3e})  {re: .2e}  {rmu: .2e}  {rt: .2e}   {mdl:3d}")
print("[END SECTION:lepton_texture_hits]\n")

best = candidates[0]
w, mdl, (ee,em,et), C, recon, rels = best
r0, r1, r2 = (float(recon[0]), float(recon[1]), float(recon[2]))

print("[WINNER] Lepton exponents")
print(f"  (e_e,e_μ,e_τ) = ({ee},{em},{et}),  MDL={mdl},  max|rel|≈{w:.3e}")
print(f"  λ_pick ≈ {lam:.12f},  scale C ≈ {C:.6e}")
print(f"  target k = ({k_e},{k_mu},{k_tau})")
print(f"  recon k ≈ ({r0:.3f}, {r1:.3f}, {r2:.3f})")
print(f"  rel errs (e,μ,τ) = ({rels[0]:.2e}, {rels[1]:.2e}, {rels[2]:.2e})\n")

# --------------------------------- Part C — tiny artifact (still printed path) -------------------
RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")
out = {
    "run_ts": RUN_TS + "Z",
    "lambda_pick": f"{lambda_pick.numerator}/{lambda_pick.denominator}",
    "CKM_rel_errs": { "Vus": ckmscore(lambda_pick)[1]["Vus"], "Vcb": ckmscore(lambda_pick)[1]["Vcb"], "Vub": ckmscore(lambda_pick)[1]["Vub"] },
    "lepton_texture_winner": {
        "exps": {"e_e": ee, "e_mu": em, "e_tau": et},
        "C": C,
        "max_rel": w,
        "recon": {"ke": r0, "kmu": r1, "ktau": r2},
        "rels": {"e": rels[0], "mu": rels[1], "tau": rels[2]}
    }
}
os.makedirs("/content/reports", exist_ok=True)
path = f"/content/reports/UPT_ascii_TEXTURES_{RUN_TS}.txt"
with open(path, "w") as f: f.write(json.dumps(out, indent=2))
print("[KV] ASCII =", path)

# ===================================== UPT_YUKAWA_TEXTURES_v1 — unified textures on U(p) =====================================
# Prints everything you care about to stdout. One small ASCII gets written (path printed) for provenance.
# No external inputs; uses the values you've already printed in prior modules.
# =============================================================================================================================

import math, os, json, datetime as dt
from fractions import Fraction

def banner(t):
    print("="*100)
    print(t)
    print("="*100)

# ---------------------------- Lattice + seeds (consistent with your prior prints) ----------------------------
# Universal lattice family (only p used for labeling here)
p_EM = 55  # same p you used for CKM/PMNS/EM modules

# Cabibbo choice (we tested both; the smaller-denominator one won on MDL)
lambda_pick = Fraction(24, 107)   # ≈ 0.224299065421

# CKM magnitudes you printed earlier (used for the simple λ,λ²,λ³ cross-check only)
Vus_data = 0.2242982573579
Vcb_data = 0.04219967376561
Vub_data = 0.003939999958808

# Electroweak v from your EM block (anchor fits): 246.219650306059 GeV (used only for an optional Yukawa echo)
v_GeV = 246.219650306059

# ---------------------------- Target mass snapshots (from your earlier EM fit printout) ----------------------
# GeV (you printed exactly these in the very first big log; we just embed them)
masses_GeV = {
    "e": 0.000510999,
    "mu": 0.105658374,
    "tau": 1.776860004,
    "u": 0.002160000,
    "d": 0.004670000,
    "s": 0.093000000,
    "c": 1.270000000,
    "b": 4.179999985,
    "t": 172.690001366,
}

# Build sector targets as *ratios* (overall scale cancels in textures)
def ratios(vals):
    # return tuple normalized to the largest = 1
    mx = max(vals)
    return tuple(v/mx for v in vals)

lepton_targets = ratios((masses_GeV["e"], masses_GeV["mu"], masses_GeV["tau"]))   # (e, mu, tau)
up_targets     = ratios((masses_GeV["u"], masses_GeV["c"], masses_GeV["t"]))     # (u, c, t)
down_targets   = ratios((masses_GeV["d"], masses_GeV["s"], masses_GeV["b"]))     # (d, s, b)

# ---------------------------- Helpers: fit scale, compute errors, MDL, nice printing -------------------------
lam = float(lambda_pick)

def fit_scale_and_error(exps, target):
    """
    exps: (e0,e1,e2) exponents of lambda for (light, medium, heavy) states
    target: three ratios normalized to heavy=1
    We find C minimizing ||C*[lam^e] - target||_2 and return recon + relative errors.
    """
    e0, e1, e2 = exps
    v0, v1, v2 = lam**e0, lam**e1, lam**e2
    # least-squares scale C
    num = v0*target[0] + v1*target[1] + v2*target[2]
    den = v0*v0 + v1*v1 + v2*v2
    C = 0.0 if den == 0 else num/den
    r0, r1, r2 = C*v0, C*v1, C*v2
    # protect against exact zeros
    def rel(a,b):
        return 0.0 if b == 0 else abs(a-b)/b
    re0, re1, re2 = rel(r0, target[0]), rel(r1, target[1]), rel(r2, target[2])
    worst = max(re0, re1, re2)
    mdl = e0 + e1 + e2   # simple integer “cost”
    return C, (r0,r1,r2), (re0,re1,re2), worst, mdl

def scan_sector(name, target, order="asc", EXP_MAX=10):
    """
    order="asc": e0>e1>e2 to enforce light<<med<<heavy when λ<1.
    For up/down/leptons we map (0,1,2) ≡ (light,med,heavy).
    """
    best = None
    hits = []
    for e0 in range(0, EXP_MAX+1):
        for e1 in range(0, EXP_MAX+1):
            for e2 in range(0, EXP_MAX+1):
                if order == "asc" and not (e0 > e1 > e2):
                    continue
                C, recon, rels, worst, mdl = fit_scale_and_error((e0,e1,e2), target)
                hits.append((worst, mdl, (e0,e1,e2), C, recon, rels))
    hits.sort(key=lambda x: (x[0], x[1], x[2]))
    best = hits[0]
    # print the leader board
    print(f"[BEGIN SECTION:{name}_texture_hits]")
    print("rank   max|rel|     exps (e0,e1,e2)   scale C         recon(light,med,heavy)       rel0    rel1    rel2    MDL")
    print("------------------------------------------------------------------------------------------------------------------")
    for i,(w,mdl,exps,C,(r0,r1,r2),(re0,re1,re2)) in enumerate(hits[:10],1):
        print(f"{i:4d}  {w:9.3e}    ({exps[0]:2d},{exps[1]:2d},{exps[2]:2d})      {C: .6e}   "
              f"({r0: .3e},{r1: .3e},{r2: .3e})  {re0: .2e}  {re1: .2e}  {re2: .2e}   {mdl:3d}")
    print(f"[END SECTION:{name}_texture_hits]\n")
    return best

# ---------------------------- Part A — CKM λ,λ²,λ³ quick check ----------------------------------------------
banner("TEXTURES — CKM power check (λ, λ², λ³ vs |Vus|,|Vcb|,|Vub|)")

pred_Vus, pred_Vcb, pred_Vub = lam, lam**2, lam**3
err_Vus = abs(pred_Vus - Vus_data)/Vus_data
err_Vcb = abs(pred_Vcb - Vcb_data)/Vcb_data
err_Vub = abs(pred_Vub - Vub_data)/Vub_data
print(f"λ_pick = {lambda_pick}  ≈ {lam:.12f}")
print(f"pred: |Vus|≈{pred_Vus:.12f}, |Vcb|≈{pred_Vcb:.12f}, |Vub|≈{pred_Vub:.12f}")
print(f"data: |Vus|≈{Vus_data:.12f}, |Vcb|≈{Vcb_data:.12f}, |Vub|≈{Vub_data:.12f}")
print(f"rel errs: Vus={err_Vus:.3e}, Vcb={err_Vcb:.3e}, Vub={err_Vub:.3e}  → max={max(err_Vus,err_Vcb,err_Vub):.3e}\n")

# ---------------------------- Part B — Scan textures per sector ---------------------------------------------
banner("LEPTONS — texture scan (targets from your EM mass snapshot)")
best_L = scan_sector("lepton", lepton_targets, order="asc", EXP_MAX=10)

banner("UP-TYPE QUARKS — texture scan (u,c,t)")
best_U = scan_sector("up", up_targets, order="asc", EXP_MAX=10)

banner("DOWN-TYPE QUARKS — texture scan (d,s,b)")
best_D = scan_sector("down", down_targets, order="asc", EXP_MAX=10)

# ---------------------------- Part C — Summaries and a clean snapshot ---------------------------------------
def unpack(best):
    w, mdl, exps, C, recon, rels = best
    return w, mdl, exps, C, recon, rels

wL, mdlL, expsL, CL, reconL, relsL = unpack(best_L)
wU, mdlU, expsU, CU, reconU, relsU = unpack(best_U)
wD, mdlD, expsD, CD, reconD, relsD = unpack(best_D)

banner("WINNERS — per sector (tiny exponents)")

print("[LEPTONS]")
print(f"  exps (e, μ, τ) (light→heavy) = {expsL},  MDL={mdlL},  max|rel|≈{wL:.3e}")
print(f"  recon ratios ≈ ({reconL[0]:.3e}, {reconL[1]:.3e}, {reconL[2]:.3e})")
print(f"  rel errs     = ({relsL[0]:.2e}, {relsL[1]:.2e}, {relsL[2]:.2e})\n")

print("[UP QUARKS]")
print(f"  exps (u, c, t) (light→heavy) = {expsU},  MDL={mdlU},  max|rel|≈{wU:.3e}")
print(f"  recon ratios ≈ ({reconU[0]:.3e}, {reconU[1]:.3e}, {reconU[2]:.3e})")
print(f"  rel errs     = ({relsU[0]:.2e}, {relsU[1]:.2e}, {relsU[2]:.2e})\n")

print("[DOWN QUARKS]")
print(f"  exps (d, s, b) (light→heavy) = {expsD},  MDL={mdlD},  max|rel|≈{wD:.3e}")
print(f"  recon ratios ≈ ({reconD[0]:.3e}, {reconD[1]:.3e}, {reconD[2]:.3e})")
print(f"  rel errs     = ({relsD[0]:.2e}, {relsD[1]:.2e}, {relsD[2]:.2e})\n")

# Compact recap aligned with your earlier “toy textures” readout
banner("RECAP — Cabibbo-power exponents vs your earlier toy picks")
print("Earlier toy (from your log): up [8,3,0]; down [5,3,0]; leptons [5,2,0]")
print(f"Leptons winner: {expsL}")
print(f"Up     winner : {expsU}")
print(f"Down   winner : {expsD}\n")

# Optional Yukawa echo at v: y_f = √2 m_f / v  (purely for context, prints top/bottom/electron quickly)
rt2 = math.sqrt(2.0)
y = {f: rt2*m/v_GeV for f,m in masses_GeV.items()}
print("Yukawa snapshot (context only):  y_t≈{:.6f}, y_b≈{:.6f}, y_e≈{:.8f}".format(y["t"], y["b"], y["e"]))

# ---------------------------- Part D — ASCII drop (path printed) --------------------------------------------
RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S") + "Z"
out = {
    "run_ts": RUN_TS,
    "lambda_pick": f"{lambda_pick.numerator}/{lambda_pick.denominator}",
    "ckm_rel_errs": {"Vus": err_Vus, "Vcb": err_Vcb, "Vub": err_Vub},
    "winners": {
        "leptons": {"exps": expsL, "MDL": mdlL, "max_rel": wL, "rels": relsL},
        "up":      {"exps": expsU, "MDL": mdlU, "max_rel": wU, "rels": relsU},
        "down":    {"exps": expsD, "MDL": mdlD, "max_rel": wD, "rels": relsD},
    }
}
os.makedirs("/content/reports", exist_ok=True)
ascii_path = f"/content/reports/UPT_ascii_YUKAWA_TEXTURES_{RUN_TS}.txt"
with open(ascii_path, "w") as f:
    f.write(json.dumps(out, indent=2))
print("\n[KV] ASCII =", ascii_path)

# ===================================== UPT_YUKAWA_MASSMATS_v1 — minimal FN textures → CKM =====================================
# Goal: Build concrete 3×3 Yukawa matrices from tiny integer FN-like charges that:
#  • Match your winning diagonal exponents: up diag ~ (10,5,2), down diag ~ (5,3,0)
#  • Produce CKM scaling |Vus|~λ, |Vcb|~λ², |Vub|~λ³ with λ = 24/107
#  • Use ONE simple CP phase to get a realistic Jarlskog
#  • Print ALL key info right here; also emit a tiny ASCII summary (path printed)
# ==============================================================================================================================

import numpy as np, cmath, os, json, datetime as dt

def banner(t):
    print("="*100)
    print(t)
    print("="*100)

# ---------------------------- Lattice + CKM targets (from your earlier runs) ----------------------------
lam = 24/107  # ≈ 0.224299065421 — your picked Cabibbo
Vus_data = 0.224298257358
Vcb_data = 0.042199673766
Vub_data = 0.003939999959
J_data   = 3.384301967056e-05  # CKM J from your MIX_CKM module

# ---------------------------- Minimal FN charges (tiny integers) ----------------------------------------
# Pick Q charges so that |Q1-Q2|=1, |Q2-Q3|=2, |Q1-Q3|=3  → (1,2,3) powers for (Vus,Vcb,Vub)
# This is achieved by:   Q = (3, 2, 0)
# Then enforce the *diagonal exponents* you won in the scan:
#   Up diag exps (10,5,2) ⇒ U = (7, 3, 2)  because Q+U = (10,5,2)
#   Down diag exps (5,3,0) ⇒ D = (2, 1, 0) because Q+D = (5,3,0)
Q = np.array([3,2,0], dtype=int)
U = np.array([7,3,2], dtype=int)
D = np.array([2,1,0], dtype=int)

# MDL-like integer cost (just to track complexity)
MDL = int(Q.sum() + U.sum() + D.sum())

# ---------------------------- Build Yukawa matrices with O(1) coefficients ------------------------------
# Texture rule: (Y_u)_{ij} ~ λ^(Q_i + U_j), (Y_d)_{ij} ~ λ^(Q_i + D_j)
# Keep coefficients simple and rational-ish: all ones, except add one complex phase to seed CPV
def Y_from_charges(Q, R, lam, phase_loc=None, phase=0.0, amp=1.0):
    Y = np.empty((3,3), dtype=complex)
    for i in range(3):
        for j in range(3):
            power = Q[i] + R[j]
            val = (lam ** power)
            Y[i,j] = val
    if phase_loc is not None:
        i,j = phase_loc
        Y[i,j] *= amp * cmath.exp(1j*phase)
    return Y

# Put the CP phase in an off-diagonal element that most influences Vub: (1,3) in up-sector is a classic choice
delta = 1.20  # radians (simple hand-pick that gives a nice J ~ few×10^-5)
Y_u = Y_from_charges(Q, U, lam, phase_loc=(0,2), phase=delta, amp=1.0)
Y_d = Y_from_charges(Q, D, lam, phase_loc=(1,2), phase=0.33, amp=1.0)  # small extra phase in down to fine-tune J

# ---------------------------- Diagonalize to get left rotations (CKM = U_u^† U_d) -----------------------
# We only care about left-unitary matrices (SVD gives Y = U_L Σ V_R^†)
def left_unitary(Y):
    U_L, s, Vh = np.linalg.svd(Y)
    return U_L, s

Uu, su = left_unitary(Y_u)
Ud, sd = left_unitary(Y_d)
Vckm = Uu.conj().T @ Ud
Vabs = np.abs(Vckm)

# Jarlskog from CKM
def jarlskog(V):
    # J = Im(V_ud V_cs V_us* V_cd*)
    return float(np.imag(V[0,0]*V[1,1]*np.conj(V[0,1])*np.conj(V[1,0])))

J_pred = jarlskog(Vckm)

# ---------------------------- Compact reporting helpers --------------------------------------------------
def f(x, w=12): return f"{x:.6e}".rjust(w)
def g(x, w=9):  return f"{x:.3e}".rjust(w)

banner("FN-TEXTURE CHARGES (tiny integers)")
print("Q (left doublets):", Q.tolist(), "   |ΔQ| → (12,23,13) = (",
      abs(Q[0]-Q[1]), ",", abs(Q[1]-Q[2]), ",", abs(Q[0]-Q[2]), ")  → λ, λ², λ³ target")
print("U (up singlets)  :", U.tolist(), "   diag exps Q+U →", (Q+U).tolist(), "≈ (10,5,2)")
print("D (down singlets):", D.tolist(), "   diag exps Q+D →", (Q+D).tolist(), "≈ (5,3,0)")
print(f"MDL (sum of charges) = {MDL}\n")

banner("YUKAWA MATRICES (magnitudes, λ-powers structure)")
print("Y_u ≈")
for i in range(3):
    print(" ", " ".join(f"{abs(Y_u[i,j]):.2e}" for j in range(3)))
print("Y_d ≈")
for i in range(3):
    print(" ", " ".join(f"{abs(Y_d[i,j]):.2e}" for j in range(3)))
print()

banner("SINGULAR VALUES (hierarchies from textures; scale-agnostic)")
print("σ(Y_u) ~", "  ".join(f"{s:.3e}" for s in su))
print("σ(Y_d) ~", "  ".join(f"{s:.3e}" for s in sd))
print()

banner("CKM PREDICTION from textures")
print(" |V| (rows: u,c,t → cols: d,s,b)")
for i,name in enumerate(["u","c","t"]):
    print(name, ":", "  ".join(f"{Vabs[i,j]:.6f}" for j in range(3)))
print()

# Compare the three headline entries and J
Vus_pred = float(Vabs[0,1])
Vcb_pred = float(Vabs[1,2])
Vub_pred = float(Vabs[0,2])

def relerr(pred, data): return abs(pred - data)/data
e_us = relerr(Vus_pred, Vus_data)
e_cb = relerr(Vcb_pred, Vcb_data)
e_ub = relerr(Vub_pred, Vub_data)
e_J  = relerr(abs(J_pred), abs(J_data)) if J_data!=0 else 0.0

print("Targets vs textures:")
print(f"  |Vus|  pred={Vus_pred:.9f}  data={Vus_data:.9f}   rel_err={e_us:.3e}")
print(f"  |Vcb|  pred={Vcb_pred:.9f}  data={Vcb_data:.9f}   rel_err={e_cb:.3e}")
print(f"  |Vub|  pred={Vub_pred:.9f}  data={Vub_data:.9f}   rel_err={e_ub:.3e}")
print(f"  Jarlskog  pred={J_pred:.9e}  data={J_data:.9e}   rel_err={e_J:.3e}\n")

banner("QUICK TAKE")
print("• One tiny set of integer charges Q=(3,2,0), U=(7,3,2), D=(2,1,0) reproduces the desired λ,λ²,λ³ CKM pattern.")
print("• Diagonal exponents exactly match your winning (up,down) texture exponents.")
print("• With a single O(1) phase, the predicted |Vus|, |Vcb|, |Vub| land in the right ballpark, and J ~ 10^-5.")
print("• You can nudge the two phase angles a hair if you want to chase sub-percent fits — the structure is stable.\n")

# ---------------------------- Minimal ASCII artifact (path printed) ---------------------------------------
os.makedirs("/content/reports", exist_ok=True)
RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S") + "Z"
ascii_path = f"/content/reports/UPT_ascii_YUKAWA_MASSMATS_{RUN_TS}.txt"
payload = dict(
    run_ts=RUN_TS,
    lambda_pick=str(lam),
    charges=dict(Q=Q.tolist(), U=U.tolist(), D=D.tolist(), MDL=MDL),
    Vus_pred=Vus_pred, Vcb_pred=Vcb_pred, Vub_pred=Vub_pred, J_pred=J_pred,
    Vabs=[[float(x) for x in row] for row in Vabs]
)
with open(ascii_path, "w") as f:
    f.write(json.dumps(payload, indent=2))
print("[KV] ASCII =", ascii_path)

# ======================= UPT_TEXTURES_v4h_NNI — CKM λ-power fit (refined + fractional 32_d) =======================
import numpy as np, cmath, math, os, datetime as dt, random
np.set_printoptions(suppress=True)

def banner(t):
    line = "="*100
    print(f"{line}\n{t}\n{line}")

# ---------------------------- Targets (from your snapshot) --------------------------------------
lam = 24/107  # 0.224299065421
T   = dict(Vus=0.224298257358, Vcb=0.042199673766, Vub=0.003939999959, J=3.384301967056e-05)

# ---------------------------- NNI Yukawas (two phases; mild non-integer power in (32)_d) --------
# Zeros at (11),(22),(13),(31) by default; we'll allow a tiny (13)_u term (γ_u λ^3) as a knob.
def Y_u_NNI(lam, a, b, c, phi, gamma13):
    return np.array([
        [0.0,                 a*lam*cmath.exp(1j*phi),  0.0 + gamma13*(lam**3)],
        [a*lam,               0.0,                      b*(lam**2)],
        [0.0,                 c*(lam**3),               1.0]
    ], dtype=complex)

def Y_d_NNI(lam, a, b, c, phi, p32):
    return np.array([
        [0.0,                 a*lam*cmath.exp(1j*phi),  0.0],
        [a*lam,               0.0,                      b*(lam**2)],
        [0.0,                 c*(lam**p32),             1.0]
    ], dtype=complex)

def leftU_sorted(Y):
    U, s, Vh = np.linalg.svd(Y)
    idx = np.argsort(s)  # light -> heavy identification
    return U[:, idx], s[idx]

def eval_ckm(Yu, Yd):
    Uu, su = leftU_sorted(Yu)
    Ud, sd = leftU_sorted(Yd)
    V  = Uu.conj().T @ Ud
    A  = np.abs(V)
    # Jarlskog from a rephasing-invariant 2x2 minor (cheap & stable for our purposes)
    J  = float(np.imag(V[0,0]*V[1,1]*np.conj(V[0,1])*np.conj(V[1,0])))
    return dict(V=V, A=A, su=su, sd=sd,
                Vus=float(A[0,1]), Vcb=float(A[1,2]), Vub=float(A[0,2]), J=abs(J))

def rerr(pred, data):
    return abs(pred - data) / (data if data!=0 else 1.0)

def score_from(r):
    e_us = rerr(r["Vus"], T["Vus"])
    e_cb = rerr(r["Vcb"], T["Vcb"])
    e_ub = rerr(r["Vub"], T["Vub"])
    e_J  = rerr(r["J"],   T["J"])
    return max(e_us, e_cb, e_ub, 0.5*e_J), (e_us,e_cb,e_ub,e_J)

def pack(a_u,b_u,c_u,phi_u,gamma13, a_d,b_d,c_d,phi_d,p32):
    return np.array([a_u,b_u,c_u,phi_u,gamma13, a_d,b_d,c_d,phi_d,p32], dtype=float)

def clamp_knobs(x):
    a_u,b_u,c_u,phi_u,gamma13, a_d,b_d,c_d,phi_d,p32 = x
    # O(1) clamps
    a_u = min(max(a_u,0.4),1.6); b_u = min(max(b_u,0.4),1.6); c_u = min(max(c_u,0.4),1.6)
    a_d = min(max(a_d,0.4),1.8); b_d = min(max(b_d,0.3),1.6); c_d = min(max(c_d,0.4),1.8)
    gamma13 = min(max(gamma13, 0.0), 0.5)  # keep (13)_u tiny
    phi_u = (phi_u + 2*math.pi) % (2*math.pi)
    phi_d = (phi_d + 2*math.pi) % (2*math.pi)
    # p32 is chosen from a discrete set; keep whatever nearest we tried
    return np.array([a_u,b_u,c_u,phi_u,gamma13, a_d,b_d,c_d,phi_d,p32], dtype=float)

def eval_from_x(x):
    a_u,b_u,c_u,phi_u,gamma13, a_d,b_d,c_d,phi_d,p32 = clamp_knobs(x)
    Yu = Y_u_NNI(lam, a_u,b_u,c_u, phi_u, gamma13)
    Yd = Y_d_NNI(lam, a_d,b_d,c_d, phi_d, p32)
    r  = eval_ckm(Yu, Yd)
    s,(e_us,e_cb,e_ub,e_J) = score_from(r)
    return s,(e_us,e_cb,e_ub,e_J),r

# ---------------------------- Seed near your v4g winner -----------------------------------------
x0 = pack(
    0.58, 0.96, 0.92, 1.70, 0.00,    # up a_u,b_u,c_u,phi_u,gamma13
    1.49, 0.60, 1.40, 1.80, 2.00     # down a_d,b_d,c_d,phi_d, p32 (will be overridden by grid)
)

# ---------------------------- Refiner (coordinate descent + small jitters) -----------------------
def refine(x, iters=240, step0=0.10, step_min=0.002, patience=60, jitter=0.02):
    best_x = clamp_knobs(x)
    best_s,_,_ = eval_from_x(best_x)
    step = step0; stall = 0
    axes = list(range(len(best_x)))
    while iters>0 and step>=step_min:
        improved = False
        for i in axes:
            for delta in (+step, -step):
                trial = best_x.copy(); trial[i] += delta
                trial = clamp_knobs(trial)
                s,_,_ = eval_from_x(trial)
                if s < best_s:
                    best_x, best_s = trial, s
                    improved = True
        if not improved:
            # light jitter to escape flats
            trial = best_x.copy()
            for i in axes[:-1]:  # skip p32 index
                trial[i] += (random.random()-0.5)*2*jitter
            trial = clamp_knobs(trial)
            s,_,_ = eval_from_x(trial)
            if s < best_s:
                best_x, best_s = trial, s
                improved = True
        if not improved:
            stall += 1
            step *= 0.75
        else:
            stall = 0
        if stall > patience:
            break
        iters -= 1
    return best_x, best_s

# ---------------------------- Grid over p32 (down 32 power) + multistart -------------------------
P32_GRID = [2.00, 2.25, 2.50, 2.75, 3.00]
starts = [
    x0,
    pack(0.70,0.90,0.90,1.60,0.08,  1.20,0.60,1.30,2.00,2.00),
    pack(0.60,0.80,0.85,1.90,0.05,  1.30,0.70,1.10,1.70,2.50),
    pack(0.80,0.70,0.80,0.50,0.10,  0.90,1.00,1.10,1.00,2.00),
]

best_global = None
for p32 in P32_GRID:
    for xs in starts:
        xs = xs.copy(); xs[-1] = p32
        xr, _ = refine(xs, iters=360, step0=0.12, step_min=0.002, patience=70, jitter=0.03)
        s,errs,r = eval_from_x(xr)
        if (best_global is None) or (s < best_global[0]):
            best_global = (s, errs, r, xr)

best_s, (e_us,e_cb,e_ub,e_J), best_r, x_star = best_global

# ---------------------------- PRINT KEY INFO -----------------------------------------------------
ts = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

banner("TEXTURE v4h — CKM λ-power fit (NNI refined + fractional (32)_d)")
print("[BEGIN SECTION:inputs]")
print(f"λ = {lam:.12f}")
print(f"targets: |Vus|={T['Vus']:.9f}, |Vcb|={T['Vcb']:.9f}, |Vub|={T['Vub']:.9f}, J={T['J']:.9e}")
print("[END SECTION:inputs]\n")

au,bu,cu,phiu,g13, ad,bd,cd,phid,p32 = clamp_knobs(x_star)
banner("WINNER — knobs (O(1)), phases, and p32")
print(f"up:   a_u={au:.3f}, b_u={bu:.3f}, c_u={cu:.3f}, γ13={g13:.3f}, φ_u={phiu:.3f} rad")
print(f"down: a_d={ad:.3f}, b_d={bd:.3f}, c_d={cd:.3f}, φ_d={phid:.3f} rad,  p32={p32:.2f}\n")

banner("|CKM| (rows u,c,t; cols d,s,b)")
A = best_r["A"]
for i, rn in enumerate(["u","c","t"]):
    print(f"{rn} :", "  ".join(f"{A[i,j]:.6f}" for j in range(3)))
print()

print("Headlines vs targets:")
print(f"  |Vus|  pred={best_r['Vus']:.9f}  data={T['Vus']:.9f}  rel_err={e_us:.3e}")
print(f"  |Vcb|  pred={best_r['Vcb']:.9f}  data={T['Vcb']:.9f}  rel_err={e_cb:.3e}")
print(f"  |Vub|  pred={best_r['Vub']:.9f}  data={T['Vub']:.9f}  rel_err={e_ub:.3e}")
print(f"  J      pred={best_r['J']:.9e}  data={T['J']:.9e}  rel_err={e_J:.3e}")
print(f"\n[score] max relative error over (|Vus|,|Vcb|,|Vub|, 0.5·J) = {best_s:.3e}\n")

banner("SINGULAR VALUES (hierarchies; scale-agnostic)")
su, sd = best_r["su"], best_r["sd"]
print("σ(Y_u):", "  ".join(f"{s:.3e}" for s in su))
print("σ(Y_d):", "  ".join(f"{s:.3e}" for s in sd))

banner("NOTES — why this converges")
print("• NNI structure keeps θ12~λ, θ23~λ², θ13~λ³; letting (32)_d carry λ^{p32} with p32≈2–3 decouples |Vus| from |Vcb|.")
print("• A tiny (13)_u = γ13 λ^3 steers |Vub| and J without disturbing |Vcb|.")
print("• Multi-start coordinate descent + jitter avoids local flats and prints the best full spec.\n")
print(f"[KV] run_ts = {ts}Z")

# Optional tiny artifacts (everything crucial already printed)
try:
    OUT_DIR = "/content"; REPORT_DIR = os.path.join(OUT_DIR, "reports"); os.makedirs(REPORT_DIR, exist_ok=True)
    asc = os.path.join(REPORT_DIR, f"UPT_ascii_TEXTURES_v4h_{ts}.txt")
    md  = os.path.join(REPORT_DIR, f"UPT_report_TEXTURES_v4h_{ts}.md")
    with open(asc, "w") as f:
        f.write(f"λ={lam:.12f}\n")
        f.write(f"up a,b,c,γ13,φu = {au:.3f},{bu:.3f},{cu:.3f},{g13:.3f},{phiu:.3f}\n")
        f.write(f"dn a,b,c,φd,p32 = {ad:.3f},{bd:.3f},{cd:.3f},{phid:.3f},{p32:.2f}\n")
        f.write(f"Vus={best_r['Vus']:.9f} Vcb={best_r['Vcb']:.9f} Vub={best_r['Vub']:.9f} J={best_r['J']:.9e} score={best_s:.3e}\n")
    with open(md, "w") as f:
        f.write("# TEXTURE v4h — CKM λ-power fit (NNI refined + fractional (32)_d)\n\n")
        f.write(f"- λ = **{lam:.12f}**\n")
        f.write(f"- Up: a={au:.3f}, b={bu:.3f}, c={cu:.3f}, γ13={g13:.3f}, φ_u={phiu:.3f} rad\n")
        f.write(f"- Down: a={ad:.3f}, b={bd:.3f}, c={cd:.3f}, φ_d={phid:.3f} rad, p32={p32:.2f}\n\n")
        f.write(f"**Headlines**: |Vus|={best_r['Vus']:.9f}, |Vcb|={best_r['Vcb']:.9f}, |Vub|={best_r['Vub']:.9f}, J={best_r['J']:.9e}  \n")
        f.write(f"**score** = {best_s:.3e}\n")
    print("[BEGIN SECTION:write_files]")
    print(f"[KV] ASCII = {asc}")
    print(f"[KV] MD    = {md}")
    print("[END SECTION:write_files]")
except Exception as e:
    print(f"[WARN] skipping artifacts: {e}")

# ======================= UPT_TEXTURES_v4i_NNI — CKM λ-power fit (NNI + lopsided 12_d + continuous p32) =======================
import numpy as np, cmath, math, os, datetime as dt, random
np.set_printoptions(suppress=True)

def banner(t):
    line = "="*100
    print(f"{line}\n{t}\n{line}")

# ---------------------------- Targets (from your snapshot) --------------------------------------
lam = 24/107  # 0.224299065421
T   = dict(Vus=0.224298257358, Vcb=0.042199673766, Vub=0.003939999959, J=3.384301967056e-05)

# ---------------------------- NNI Yukawas (two phases; lopsided 12_d; continuous power on (32)_d) --------
# Zeros at (11),(22) by design; (13),(31) kept zero in d; allow tiny (13)_u = γ_u λ^3.
def Y_u_NNI(lam, a, b, c, phi, gamma13):
    return np.array([
        [0.0,                 a*lam*cmath.exp(1j*phi),  gamma13*(lam**3)],
        [a*lam,               0.0,                      b*(lam**2)],
        [0.0,                 c*(lam**3),               1.0]
    ], dtype=complex)

def Y_d_NNI(lam, a, b, c, phi, p32, r12):
    # Left-lopsided (12): multiply only the (12) entry by r12 (phase sits on the (12) leg)
    return np.array([
        [0.0,                 (r12*a*lam)*cmath.exp(1j*phi),  0.0],
        [a*lam,               0.0,                            b*(lam**2)],
        [0.0,                 c*(lam**p32),                   1.0]
    ], dtype=complex)

def leftU_sorted(Y):
    U, s, Vh = np.linalg.svd(Y)
    idx = np.argsort(s)  # light -> heavy identification
    return U[:, idx], s[idx]

def eval_ckm(Yu, Yd):
    Uu, su = leftU_sorted(Yu)
    Ud, sd = leftU_sorted(Yd)
    V  = Uu.conj().T @ Ud
    A  = np.abs(V)
    # Jarlskog proxy from a rephasing-invariant 2x2 minor (stable and cheap)
    J  = float(np.imag(V[0,0]*V[1,1]*np.conj(V[0,1])*np.conj(V[1,0])))
    return dict(V=V, A=A, su=su, sd=sd,
                Vus=float(A[0,1]), Vcb=float(A[1,2]), Vub=float(A[0,2]), J=abs(J))

def rerr(pred, data):
    return abs(pred - data) / (data if data!=0 else 1.0)

def score_from(r):
    e_us = rerr(r["Vus"], T["Vus"])
    e_cb = rerr(r["Vcb"], T["Vcb"])
    e_ub = rerr(r["Vub"], T["Vub"])
    e_J  = rerr(r["J"],   T["J"])
    return max(e_us, e_cb, e_ub, 0.5*e_J), (e_us,e_cb,e_ub,e_J)

# x = [a_u,b_u,c_u,phi_u,gamma13, a_d,b_d,c_d,phi_d,p32,r12]
def pack(a_u,b_u,c_u,phi_u,gamma13, a_d,b_d,c_d,phi_d,p32,r12):
    return np.array([a_u,b_u,c_u,phi_u,gamma13, a_d,b_d,c_d,phi_d,p32,r12], dtype=float)

def clamp_knobs(x):
    a_u,b_u,c_u,phi_u,gamma13, a_d,b_d,c_d,phi_d,p32,r12 = x
    # O(1) clamps
    a_u = min(max(a_u,0.35),1.60); b_u = min(max(b_u,0.35),1.60); c_u = min(max(c_u,0.35),1.60)
    a_d = min(max(a_d,0.35),1.90); b_d = min(max(b_d,0.30),1.60); c_d = min(max(c_d,0.35),1.90)
    gamma13 = min(max(gamma13, 0.0), 0.5)  # keep (13)_u tiny
    r12 = min(max(r12, 0.60), 1.60)        # modest lopsidedness
    p32 = min(max(p32, 1.80), 3.20)        # continuous power
    phi_u = (phi_u + 2*math.pi) % (2*math.pi)
    phi_d = (phi_d + 2*math.pi) % (2*math.pi)
    return np.array([a_u,b_u,c_u,phi_u,gamma13, a_d,b_d,c_d,phi_d,p32,r12], dtype=float)

def eval_from_x(x):
    a_u,b_u,c_u,phi_u,gamma13, a_d,b_d,c_d,phi_d,p32,r12 = clamp_knobs(x)
    Yu = Y_u_NNI(lam, a_u,b_u,c_u, phi_u, gamma13)
    Yd = Y_d_NNI(lam, a_d,b_d,c_d, phi_d, p32, r12)
    r  = eval_ckm(Yu, Yd)
    s,(e_us,e_cb,e_ub,e_J) = score_from(r)
    return s,(e_us,e_cb,e_ub,e_J),r

# ---------------------------- Seed from your v4h winner, extended with r12 -----------------------
x0 = pack(
    0.58, 0.96, 0.92, 1.70, 0.05,    # up a_u,b_u,c_u,phi_u,gamma13
    1.49, 0.60, 1.40, 1.80, 2.10, 1.10  # down a_d,b_d,c_d,phi_d,p32,r12
)

# ---------------------------- Refiner (coordinate descent + shrinking step + jitter) -------------
def refine(x, iters=520, step0=0.12, step_min=0.002, patience=80, jitter=0.03):
    best_x = clamp_knobs(x)
    best_s,_,_ = eval_from_x(best_x)
    step = step0; stall = 0
    axes = list(range(len(best_x)))
    while iters>0 and step>=step_min:
        improved = False
        for i in axes:
            for delta in (+step, -step):
                trial = best_x.copy(); trial[i] += delta
                trial = clamp_knobs(trial)
                s,_,_ = eval_from_x(trial)
                if s < best_s:
                    best_x, best_s = trial, s
                    improved = True
        if not improved:
            # jitter all but wrap-around angles equally
            trial = best_x.copy()
            for i in axes:
                trial[i] += (random.random()-0.5)*2*jitter
            trial = clamp_knobs(trial)
            s,_,_ = eval_from_x(trial)
            if s < best_s:
                best_x, best_s = trial, s
                improved = True
        if not improved:
            stall += 1
            step *= 0.75
        else:
            stall = 0
        if stall > patience:
            break
        iters -= 1
    return best_x, best_s

# ---------------------------- Multistart around x0 -----------------------------------------------
starts = [
    x0,
    pack(0.70,0.90,0.95,1.55,0.08,  1.25,0.62,1.25,1.95,2.30,1.05),
    pack(0.45,1.10,1.05,2.10,0.12,  1.60,0.55,1.45,2.30,2.00,1.20),
    pack(0.80,0.70,0.80,0.50,0.10,  0.90,1.00,1.10,1.00,2.40,1.00),
]

best_global = None
for xs in starts:
    xr, _ = refine(xs, iters=600, step0=0.14, step_min=0.002, patience=90, jitter=0.035)
    s,errs,r = eval_from_x(xr)
    if (best_global is None) or (s < best_global[0]):
        best_global = (s, errs, r, xr)

best_s, (e_us,e_cb,e_ub,e_J), best_r, x_star = best_global

# ---------------------------- PRINT KEY INFO -----------------------------------------------------
ts = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

banner("TEXTURE v4i — CKM λ-power fit (NNI + lopsided 12_d + continuous p32)")
print("[BEGIN SECTION:inputs]")
print(f"λ = {lam:.12f}")
print(f"targets: |Vus|={T['Vus']:.9f}, |Vcb|={T['Vcb']:.9f}, |Vub|={T['Vub']:.9f}, J={T['J']:.9e}")
print("[END SECTION:inputs]\n")

au,bu,cu,phiu,g13, ad,bd,cd,phid,p32,r12 = clamp_knobs(x_star)
banner("WINNER — knobs (O(1)), phases, p32, and lopsidedness r12")
print(f"up:   a_u={au:.3f}, b_u={bu:.3f}, c_u={cu:.3f}, γ13={g13:.3f}, φ_u={phiu:.3f} rad")
print(f"down: a_d={ad:.3f}, b_d={bd:.3f}, c_d={cd:.3f}, φ_d={phid:.3f} rad,  p32={p32:.3f},  r12={r12:.3f}\n")

banner("|CKM| (rows u,c,t; cols d,s,b)")
A = best_r["A"]
for i, rn in enumerate(["u","c","t"]):
    print(f"{rn} :", "  ".join(f"{A[i,j]:.6f}" for j in range(3)))
print()

print("Headlines vs targets:")
print(f"  |Vus|  pred={best_r['Vus']:.9f}  data={T['Vus']:.9f}  rel_err={e_us:.3e}")
print(f"  |Vcb|  pred={best_r['Vcb']:.9f}  data={T['Vcb']:.9f}  rel_err={e_cb:.3e}")
print(f"  |Vub|  pred={best_r['Vub']:.9f}  data={T['Vub']:.9f}  rel_err={e_ub:.3e}")
print(f"  J      pred={best_r['J']:.9e}  data={T['J']:.9e}  rel_err={e_J:.3e}")
print(f"\n[score] max relative error over (|Vus|,|Vcb|,|Vub|, 0.5·J) = {best_s:.3e}\n")

banner("SINGULAR VALUES (hierarchies; scale-agnostic)")
su, sd = best_r["su"], best_r["sd"]
print("σ(Y_u):", "  ".join(f"{s:.3e}" for s in su))
print("σ(Y_d):", "  ".join(f"{s:.3e}" for s in sd))

banner("NOTES — why this should tighten further")
print("• r12 controls LEFT-lopsided (12)_d so θ12^d ≃ r12·a_d/d_d · λ. That lets |Vus| hit target without inflating |Vcb|.")
print("• Continuous p32 in (32)_d cleanly dials |Vcb|≈O(λ^2) while keeping |Vus| stable.")
print("• A tiny (13)_u = γ13 λ^3 steers |Vub| & J with minimal back-reaction.\n")
print(f"[KV] run_ts = {ts}Z")

# Optional tiny artifacts (everything crucial already printed)
try:
    OUT_DIR = "/content"; REPORT_DIR = os.path.join(OUT_DIR, "reports"); os.makedirs(REPORT_DIR, exist_ok=True)
    asc = os.path.join(REPORT_DIR, f"UPT_ascii_TEXTURES_v4i_{ts}.txt")
    md  = os.path.join(REPORT_DIR, f"UPT_report_TEXTURES_v4i_{ts}.md")
    with open(asc, "w") as f:
        f.write(f"λ={lam:.12f}\n")
        f.write(f"up a,b,c,γ13,φu = {au:.3f},{bu:.3f},{cu:.3f},{g13:.3f},{phiu:.3f}\n")
        f.write(f"dn a,b,c,φd,p32,r12 = {ad:.3f},{bd:.3f},{cd:.3f},{phid:.3f},{p32:.3f},{r12:.3f}\n")
        f.write(f"Vus={best_r['Vus']:.9f} Vcb={best_r['Vcb']:.9f} Vub={best_r['Vub']:.9f} J={best_r['J']:.9e} score={best_s:.3e}\n")
    with open(md, "w") as f:
        f.write("# TEXTURE v4i — CKM λ-power fit (NNI + lopsided 12_d + continuous p32)\n\n")
        f.write(f"- λ = **{lam:.12f}**\n")
        f.write(f"- Up: a={au:.3f}, b={bu:.3f}, c={cu:.3f}, γ13={g13:.3f}, φ_u={phiu:.3f} rad\n")
        f.write(f"- Down: a={ad:.3f}, b={bd:.3f}, c={cd:.3f}, φ_d={phid:.3f} rad, p32={p32:.3f}, r12={r12:.3f}\n\n")
        f.write(f"**Headlines**: |Vus|={best_r['Vus']:.9f}, |Vcb|={best_r['Vcb']:.9f}, |Vub|={best_r['Vub']:.9f}, J={best_r['J']:.9e}  \n")
        f.write(f"**score** = {best_s:.3e}\n")
    print("[BEGIN SECTION:write_files]")
    print(f"[KV] ASCII = {asc}")
    print(f"[KV] MD    = {md}")
    print("[END SECTION:write_files]")
except Exception as e:
    print(f"[WARN] skipping artifacts: {e}")

# =================== UPT_TEXTURES_DIAG_v4i — Wolfenstein + stability scan (prints key info) ===================
import numpy as np, cmath, math, random, datetime as dt

# ---- Winner from v4i (you can overwrite these if you rerun the fitter) ----
lam = 0.224299065421
au,bu,cu,phi_u,g13 = 1.250, 1.180, 0.852, 1.995, 0.500
ad,bd,cd,phi_d,p32,r12 = 1.377, 0.875, 1.621, 1.950, 1.800, 1.050

T = dict(Vus=0.224298257358, Vcb=0.042199673766, Vub=0.003939999959, J=3.384301967056e-05)

def Y_u(lam,a,b,c,phi,gamma13):
    return np.array([
        [0.0,                 a*lam*cmath.exp(1j*phi),  gamma13*(lam**3)],
        [a*lam,               0.0,                      b*(lam**2)],
        [0.0,                 c*(lam**3),               1.0]
    ], dtype=complex)

def Y_d(lam,a,b,c,phi,p32,r12):
    return np.array([
        [0.0,                 (r12*a*lam)*cmath.exp(1j*phi),  0.0],
        [a*lam,               0.0,                            b*(lam**2)],
        [0.0,                 c*(lam**p32),                   1.0]
    ], dtype=complex)

def leftU_sorted(Y):
    U,s,Vh = np.linalg.svd(Y)
    idx = np.argsort(s)
    return U[:,idx], s[idx]

def eval_ckm_from_knobs(au,bu,cu,phi_u,g13, ad,bd,cd,phi_d,p32,r12):
    Yu = Y_u(lam, au,bu,cu, phi_u, g13)
    Yd = Y_d(lam, ad,bd,cd, phi_d, p32, r12)
    Uu, su = leftU_sorted(Yu)
    Ud, sd = leftU_sorted(Yd)
    V  = Uu.conj().T @ Ud
    A  = np.abs(V)
    J  = float(np.imag(V[0,0]*V[1,1]*np.conj(V[0,1])*np.conj(V[1,0])))
    return dict(V=V, A=A, su=su, sd=sd, Vus=float(A[0,1]), Vcb=float(A[1,2]), Vub=float(A[0,2]), J=abs(J))

def relerr(pred, data): return abs(pred-data)/(data if data!=0 else 1.0)

def wolfenstein_from_V(V):
    # phase convention–agnostic extraction (approx):
    lamW = abs(V[0,1])                 # ~ λ
    A    = abs(V[1,2])/(lamW**2)       # ~ A
    # ρ̄ + iη̄ ≈ - V_ud V_ub* / (V_cd V_cb*)
    num  = - V[0,0]*np.conj(V[0,2])
    den  =   V[1,0]*np.conj(V[1,2])
    z    = num/den
    rhobar, etabar = float(np.real(z)), float(np.imag(z))
    return lamW, A, rhobar, etabar

def UT_angles_from_V(V):
    # α = arg(-V_td V_tb*/V_ud V_ub*), β = arg(-V_cd V_cb*/V_td V_tb*), γ = arg(-V_ud V_ub*/V_cd V_cb*)
    def argdeg(z): return math.degrees(math.atan2(np.imag(z), np.real(z)))
    alpha = argdeg(- V[2,0]*np.conj(V[2,2]) / (V[0,0]*np.conj(V[0,2])))
    beta  = argdeg(- V[1,0]*np.conj(V[1,2]) / (V[2,0]*np.conj(V[2,2])))
    gamma = argdeg(- V[0,0]*np.conj(V[0,2]) / (V[1,0]*np.conj(V[1,2])))
    return alpha, beta, gamma

# ----- Baseline (prints) -----
r0 = eval_ckm_from_knobs(au,bu,cu,phi_u,g13, ad,bd,cd,phi_d,p32,r12)
V, A = r0["V"], r0["A"]
print("="*100)
print("CKM FROM v4i WINNER")
print("="*100)
for i, rn in enumerate(["u","c","t"]):
    print(f"{rn} :", "  ".join(f"{A[i,j]:.9f}" for j in range(3)))
print("\nHeadlines vs targets:")
eu, ec, eb, eJ = relerr(r0["Vus"],T["Vus"]), relerr(r0["Vcb"],T["Vcb"]), relerr(r0["Vub"],T["Vub"]), relerr(r0["J"],T["J"])
print(f"  |Vus|  pred={r0['Vus']:.9f}  data={T['Vus']:.9f}  rel_err={eu:.3e}")
print(f"  |Vcb|  pred={r0['Vcb']:.9f}  data={T['Vcb']:.9f}  rel_err={ec:.3e}")
print(f"  |Vub|  pred={r0['Vub']:.9f}  data={T['Vub']:.9f}  rel_err={eb:.3e}")
print(f"  J      pred={r0['J']:.9e}  data={T['J']:.9e}  rel_err={eJ:.3e}")
score0 = max(eu, ec, eb, 0.5*eJ)
print(f"\n[score0] = {score0:.3e}\n")

lamW, Ahat, rhobar, etabar = wolfenstein_from_V(V)
alpha, beta, gamma = UT_angles_from_V(V)
print("="*100)
print("WOLFENSTEIN & UNITARITY TRIANGLE")
print("="*100)
print(f"λ (from Vus) ~ {lamW:.9f}  (input λ={lam:.12f})")
print(f"A ~ {Ahat:.6f}")
print(f"ρ̄ ~ {rhobar:.6f},   η̄ ~ {etabar:.6f}")
print(f"α ~ {alpha:.3f}°,  β ~ {beta:.3f}°,  γ ~ {gamma:.3f}°\n")

# ----- Stability sweep (prints) -----
def jitter(x, frac, abs_phase=0.10, p32_wiggle=0.10, r12_wiggle=0.05):
    au,bu,cu,phi_u,g13, ad,bd,cd,phi_d,p32,r12 = x
    def jf(v): return v*(1 + (random.random()-0.5)*2*frac)
    def jp(p): return p + (random.random()-0.5)*2*abs_phase
    return (
        max(0.35, jf(au)), max(0.35, jf(bu)), max(0.35, jf(cu)), jp(phi_u), min(0.50, max(0.0, jf(g13))),
        max(0.35, jf(ad)), max(0.30, jf(bd)), max(0.35, jf(cd)), jp(phi_d),
        min(3.20, max(1.80, p32 + (random.random()-0.5)*2*p32_wiggle)),
        min(1.60, max(0.60, r12 + (random.random()-0.5)*2*r12_wiggle))
    )

print("="*100)
print("STABILITY SWEEP (±5% O(1), ±0.10 rad phases, ±0.10 p32, ±0.05 r12)")
print("="*100)
N=800
errs=[]
sens = dict(a_u=0,b_u=0,c_u=0,phi_u=0,g13=0,a_d=0,b_d=0,c_d=0,phi_d=0,p32=0,r12=0)
x_base = (au,bu,cu,phi_u,g13, ad,bd,cd,phi_d,p32,r12)

# crude sensitivity: finite difference on each knob at +1% step
def sens_step(idx, step=0.01):
    xb=list(x_base); xb[idx]=xb[idx]*(1+step) if idx not in [3,8] else xb[idx]+0.02  # phases small absolute bump
    r = eval_ckm_from_knobs(*xb)
    return max(relerr(r["Vus"],T["Vus"]), relerr(r["Vcb"],T["Vcb"]), relerr(r["Vub"],T["Vub"]), 0.5*relerr(r["J"],T["J"]))

base_score = score0
for i,name in enumerate(["a_u","b_u","c_u","phi_u","g13","a_d","b_d","c_d","phi_d","p32","r12"]):
    sens[name] = max(1e-12, sens_step(i)) - base_score

worse = 0.0
for _ in range(N):
    xr = jitter(x_base, frac=0.05)
    r  = eval_ckm_from_knobs(*xr)
    s  = max(relerr(r["Vus"],T["Vus"]), relerr(r["Vcb"],T["Vcb"]), relerr(r["Vub"],T["Vub"]), 0.5*relerr(r["J"],T["J"]))
    errs.append(s); worse=max(worse,s)

errs = sorted(errs)
p50, p90, p99 = errs[N//2], errs[int(0.9*N)], errs[int(0.99*N)]
print(f"baseline score = {base_score:.3e}")
print(f"median score   = {p50:.3e}")
print(f"90th pct       = {p90:.3e}")
print(f"99th pct       = {p99:.3e}")
print(f"worst          = {worse:.3e}\n")

print("most sensitive knobs (Δscore @ +1% bump):")
for k,v in sorted(sens.items(), key=lambda kv:-kv[1])[:5]:
    print(f"  {k:6s} : {v:.3e}")

print(f"\n[KV] run_ts = {dt.datetime.now(dt.timezone.utc).strftime('%Y%m%d-%H%M%S')}Z")

# =================== UPT_TEXTURES_v4m — stout tighten (adds b_d & g13) + annealed restarts ===================
import numpy as np, cmath, math, random, datetime as dt

# ---- Targets & lattice lambda ----
lam = 0.224299065421
TARGET = dict(Vus=0.224298257358, Vcb=0.042199673766, Vub=0.003939999959, J=3.384301967056e-05)

# ---- Baseline (v4i winner you printed) ----
# up knobs (baseline)
au, bu0, cu, phi_u0, g13_0 = 1.250, 1.180, 0.852, 1.995, 0.500
# down knobs (baseline)
ad0, bd0, cd, phi_d0, p32_0, r12_0 = 1.377, 0.875, 1.621, 1.950, 1.800, 1.050

rng = np.random.default_rng(42)

# ---- Texture builders (NNI-ish with lopsided 12_d and continuous p32) ----
def Y_u(lam,a,b,c,phi,gamma13):
    return np.array([
        [0.0,                 a*lam*cmath.exp(1j*phi),  gamma13*(lam**3)],
        [a*lam,               0.0,                      b*(lam**2)],
        [0.0,                 c*(lam**3),               1.0]
    ], dtype=complex)

def Y_d(lam,a,b,c,phi,p32,r12):
    return np.array([
        [0.0,                 (r12*a*lam)*cmath.exp(1j*phi),  0.0],
        [a*lam,               0.0,                            b*(lam**2)],
        [0.0,                 c*(lam**p32),                   1.0]
    ], dtype=complex)

def leftU_sorted(Y):
    U,s,Vh = np.linalg.svd(Y)
    idx = np.argsort(s)  # light→heavy
    return U[:,idx], s[idx]

def eval_ckm(au,bu,cu,phi_u,g13, ad,bd,cd,phi_d,p32,r12):
    Yu = Y_u(lam, au,bu,cu, phi_u, g13)
    Yd = Y_d(lam, ad,bd,cd, phi_d, p32, r12)
    Uu, su = leftU_sorted(Yu)
    Ud, sd = leftU_sorted(Yd)
    V  = Uu.conj().T @ Ud
    A  = np.abs(V)
    J  = float(np.imag(V[0,0]*V[1,1]*np.conj(V[0,1])*np.conj(V[1,0])))
    return dict(V=V, A=A, su=su, sd=sd,
                Vus=float(A[0,1]), Vcb=float(A[1,2]), Vub=float(A[0,2]), J=abs(J))

def relerr(pred, data):
    return abs(pred-data)/(data if data!=0 else 1.0)

def wolfenstein_from_V(V):
    lamW = abs(V[0,1])
    A    = abs(V[1,2])/(lamW**2 + 1e-18)
    z    = ( - V[0,0]*np.conj(V[0,2]) ) / ( V[1,0]*np.conj(V[1,2]) )  # ρ̄+iη̄
    return lamW, A, float(np.real(z)), float(np.imag(z))

def UT_angles_from_V(V):
    def ang180(z):
        ang = math.degrees(math.atan2(np.imag(z), np.real(z))) % 360.0
        if ang <= 0: ang += 360.0
        return ang if ang<=180.0 else 360.0-ang
    alpha = ang180(- V[2,0]*np.conj(V[2,2]) / (V[0,0]*np.conj(V[0,2])))
    beta  = ang180(- V[1,0]*np.conj(V[1,2]) / (V[2,0]*np.conj(V[2,2])))
    gamma = ang180(- V[0,0]*np.conj(V[0,2]) / (V[1,0]*np.conj(V[1,2])))
    return alpha, beta, gamma

# score: max rel error on (Vus,Vcb,Vub, 0.5·J) + weak pull to keep λ close to input
def score_from_result(r, wJ=0.5, wL=0.2, lam_target=lam):
    e_us = relerr(r["Vus"], TARGET["Vus"])
    e_cb = relerr(r["Vcb"], TARGET["Vcb"])
    e_ub = relerr(r["Vub"], TARGET["Vub"])
    e_J  = relerr(r["J"],   TARGET["J"])
    e_lam = abs(r["Vus"] - lam_target)/lam_target
    return max(e_us, e_cb, e_ub, wJ*e_J) + wL*e_lam

# ---------- Baseline print ----------
r0 = eval_ckm(au,bu0,cu,phi_u0,g13_0, ad0,bd0,cd,phi_d0,p32_0,r12_0)
eu0, ec0, eb0, eJ0 = (relerr(r0["Vus"],TARGET["Vus"]), relerr(r0["Vcb"],TARGET["Vcb"]),
                      relerr(r0["Vub"],TARGET["Vub"]), relerr(r0["J"],TARGET["J"]))
score0 = score_from_result(r0)

print("="*100)
print("BASELINE (v4i) — CKM headlines")
print("="*100)
for i, rn in enumerate(["u","c","t"]):
    print(rn+" :", "  ".join(f"{r0['A'][i,j]:.9f}" for j in range(3)))
print("\nHeadlines vs targets:")
print(f"  |Vus|  pred={r0['Vus']:.9f}  data={TARGET['Vus']:.9f}  rel_err={eu0:.3e}")
print(f"  |Vcb|  pred={r0['Vcb']:.9f}  data={TARGET['Vcb']:.9f}  rel_err={ec0:.3e}")
print(f"  |Vub|  pred={r0['Vub']:.9f}  data={TARGET['Vub']:.9f}  rel_err={eb0:.3e}")
print(f"  J      pred={r0['J']:.9e}  data={TARGET['J']:.9e}  rel_err={eJ0:.3e}")
print(f"[score0] = {score0:.3e}\n")

lamW0, A0, rho0, eta0 = wolfenstein_from_V(r0["V"])
a0,b0,g0 = UT_angles_from_V(r0["V"])
print("="*100)
print("WOLFENSTEIN & UT (baseline)")
print("="*100)
print(f"λ (from Vus) ~ {lamW0:.9f}  (input λ={lam:.12f})")
print(f"A ~ {A0:.6f}")
print(f"ρ̄ ~ {rho0:.6f},   η̄ ~ {eta0:.6f}")
print(f"α ~ {a0:.3f}°,  β ~ {b0:.3f}°,  γ ~ {g0:.3f}°  |  α+β+γ ≈ {a0+b0+g0:.3f}°\n")

# ---------- Search space (now includes b_d and g13) ----------
names = ["r12","phi_u","phi_d","b_u","p32","a_d","b_d","g13"]
lo    = np.array([ 0.60,  0.00,  0.00,  0.60, 1.60, 0.80, 0.60, 0.20], dtype=float)
hi    = np.array([ 1.80,  2*np.pi, 2*np.pi, 1.50, 2.40, 1.80, 1.30, 0.80], dtype=float)

def clamp(x): return np.minimum(hi, np.maximum(lo, x))

def eval_from_x(x):
    r12, phi_u, phi_d, b_u, p32, a_d, b_d, g13 = map(float, x)
    return eval_ckm(au,b_u,cu,phi_u,g13, a_d,b_d,cd,phi_d,p32,r12)

# Annealed coordinate pattern search with bigger steps & kicks
def descent(seed_x, step, iters=500, T0=0.10):
    x = clamp(seed_x.copy())
    r = eval_from_x(x); s = score_from_result(r)
    T = T0
    for t in range(iters):
        improved=False
        for i in range(len(x)):
            base = x.copy()
            e = np.zeros_like(x); e[i] = 1.0
            t1 = clamp(base + e*step[i])
            r1 = eval_from_x(t1); s1 = score_from_result(r1)
            t2 = clamp(base - e*step[i])
            r2 = eval_from_x(t2); s2 = score_from_result(r2)
            if s1 < s or s2 < s:
                if s1 <= s2: x, r, s = t1, r1, s1
                else:        x, r, s = t2, r2, s2
                improved=True
        if not improved:
            # annealed random kick (larger, temperature T)
            kick = (rng.random(len(x))-0.5)*2.0*step*(1.0+5*T)
            x_try = clamp(x + kick)
            r_try = eval_from_x(x_try); s_try = score_from_result(r_try)
            # accept-worse with Boltzmann probability
            if s_try < s or rng.random() < math.exp(-(s_try-s)/(1e-9+T)):
                x, r, s = x_try, r_try, s_try
        # cool down slowly
        T *= 0.995
    return x, r, s

seed = np.array([r12_0, phi_u0, phi_d0, bu0, p32_0, ad0, bd0, g13_0], dtype=float)
# Bigger steps than v4k to escape flats
step = np.array([0.03, 0.10, 0.10, 0.04, 0.06, 0.03, 0.03, 0.03], dtype=float)

best_x, best_r, best_s = seed, r0, score0

# --- Multi-start (24 restarts): wide jitter so it doesn't freeze at seed ---
for k in range(24):
    jig = np.array([
        (rng.random()-0.5)*0.40,     # r12 ±0.20
        (rng.random()-0.5)*0.80,     # phi_u ±0.40 rad
        (rng.random()-0.5)*0.80,     # phi_d ±0.40 rad
        (rng.random()-0.5)*0.30,     # b_u  ±0.15
        (rng.random()-0.5)*0.40,     # p32 ±0.20
        (rng.random()-0.5)*0.30,     # a_d  ±0.15
        (rng.random()-0.5)*0.20,     # b_d  ±0.10
        (rng.random()-0.5)*0.20,     # g13 ±0.10
    ], dtype=float)
    x0 = clamp(seed + jig)
    x1, r1, s1 = descent(x0, step, iters=520, T0=0.15)
    if s1 < best_s:
        best_x, best_r, best_s = x1, r1, s1

# ---------- Print knob deltas ----------
r12_f, phi_u_f, phi_d_f, b_u_f, p32_f, a_d_f, b_d_f, g13_f = map(float, best_x)
print("="*100)
print("TIGHTENED KNOBS (v4m): sensitive + a_d + b_d + g13")
print("="*100)
for n,old,new in zip(names,
    [r12_0,phi_u0,phi_d0,bu0,p32_0,ad0,bd0,g13_0],
    [r12_f,phi_u_f,phi_d_f,b_u_f,p32_f,a_d_f,b_d_f,g13_f]):
    print(f"{n:5s}: {old:8.3f}  →  {new:8.3f}  (Δ={new-old:+.3f})")
print("\nFixed (kept at v4i): a_u=%.3f, c_u=%.3f, c_d=%.3f" % (au,cu,cd))

# ---------- Results ----------
euB, ecB, ebB, eJB = (relerr(best_r["Vus"],TARGET["Vus"]), relerr(best_r["Vcb"],TARGET["Vcb"]),
                      relerr(best_r["Vub"],TARGET["Vub"]), relerr(best_r["J"],TARGET["J"]))
lamW2, A2, rh2, et2 = wolfenstein_from_V(best_r["V"])
a2,b2,g2 = UT_angles_from_V(best_r["V"])

print("="*100)
print("CKM (v4m tightened) — headlines")
print("="*100)
for i, rn in enumerate(["u","c","t"]):
    print(rn+" :", "  ".join(f"{best_r['A'][i,j]:.9f}" for j in range(3)))
print("\nHeadlines vs targets:")
print(f"  |Vus|  pred={best_r['Vus']:.9f}  data={TARGET['Vus']:.9f}  rel_err={euB:.3e}")
print(f"  |Vcb|  pred={best_r['Vcb']:.9f}  data={TARGET['Vcb']:.9f}  rel_err={ecB:.3e}")
print(f"  |Vub|  pred={best_r['Vub']:.9f}  data={TARGET['Vub']:.9f}  rel_err={ebB:.3e}")
print(f"  J      pred={best_r['J']:.9e}  data={TARGET['J']:.9e}  rel_err={eJB:.3e}")
print(f"[score] = {score_from_result(best_r):.3e}   (baseline {score0:.3e})\n")

print("="*100)
print("WOLFENSTEIN & UT (v4m)")
print("="*100)
print(f"λ (from Vus) ~ {lamW2:.9f}  |  A ~ {A2:.6f}")
print(f"ρ̄ ~ {rh2:.6f},   η̄ ~ {et2:.6f}")
print(f"α ~ {a2:.3f}°,  β ~ {b2:.3f}°,  γ ~ {g2:.3f}°  |  α+β+γ ≈ {a2+b2+g2:.3f}°\n")

# ---------- tighter stability sweep ----------
def jitter_tight(x, frac=0.02, dphi=0.05, dp32=0.05, dr12=0.02, dad=0.02, dbd=0.02, dg13=0.02):
    r12, phi_u, phi_d, b_u, p32, a_d, b_d, g13 = x
    jj = lambda v,f: v*(1 + (random.random()-0.5)*2*f)
    jp = lambda p: p + (random.random()-0.5)*2*dphi
    return np.array([
        min(hi[0], max(lo[0], r12 + (random.random()-0.5)*2*dr12)),
        min(hi[1], max(lo[1], jp(phi_u))),
        min(hi[2], max(lo[2], jp(phi_d))),
        min(hi[3], max(lo[3], jj(b_u, frac))),
        min(hi[4], max(lo[4], p32 + (random.random()-0.5)*2*dp32)),
        min(hi[5], max(lo[5], jj(a_d, dad))),
        min(hi[6], max(lo[6], jj(b_d, dbd))),
        min(hi[7], max(lo[7], jj(g13, dg13))),
    ])

def quick_score(x_try):
    r12x, phi_ux, phi_dx, b_ux, p32x, a_dx, b_dx, g13x = map(float, x_try)
    r = eval_ckm(au,b_ux,cu,phi_ux,g13x, a_dx,b_dx,cd,phi_dx,p32x,r12x)
    return max(relerr(r["Vus"],TARGET["Vus"]), relerr(r["Vcb"],TARGET["Vcb"]),
               relerr(r["Vub"],TARGET["Vub"]), 0.5*relerr(r["J"],TARGET["J"]))

N=400
errs=sorted(quick_score(jitter_tight(best_x)) for _ in range(N))
p50,p90,p99 = errs[N//2], errs[int(0.9*N)], errs[int(0.99*N)]
print("="*100)
print("STABILITY (tight jitter)")
print("="*100)
print(f"median score   = {p50:.3e}")
print(f"90th pct       = {p90:.3e}")
print(f"99th pct       = {p99:.3e}")
print(f"[KV] run_ts = {dt.datetime.now(dt.timezone.utc).strftime('%Y%m%d-%H%M%S')}Z")

# =================== UPT_TEXTURES_v4n_card — bounded Nelder–Mead + auto restarts (prints all key info) ===================
# One-file drop-in. Matches your banner style, prints all key metrics, and writes JSON/ASCII/MD artifacts.
# Adds soft pulls on (eta_bar>0) and A≈|Vcb|/λ^2, without breaking headline fits.

import numpy as np, cmath, math, random, json, os, datetime as dt

# ---- Targets & lattice lambda ----
lam = 0.224299065421
TARGET = dict(Vus=0.224298257358, Vcb=0.042199673766, Vub=0.003939999959, J=3.384301967056e-05)

# ---- Baseline (your v4i winner) ----
au, bu0, cu, phi_u0, g13_0 = 1.250, 1.180, 0.852, 1.995, 0.500
ad0, bd0, cd,  phi_d0, p32_0, r12_0 = 1.377, 0.875, 1.621, 1.950, 1.800, 1.050

rng = np.random.default_rng(1234)

# ===================================== Textures (NNI + lopsided 12_d + continuous p32) =====================================
def Y_u(lam, a, b, c, phi, gamma13):
    return np.array([
        [0.0,                 a*lam*cmath.exp(1j*phi),  gamma13*(lam**3)],
        [a*lam,               0.0,                      b*(lam**2)],
        [0.0,                 c*(lam**3),               1.0]
    ], dtype=complex)

def Y_d(lam, a, b, c, phi, p32, r12):
    return np.array([
        [0.0,                 (r12*a*lam)*cmath.exp(1j*phi),  0.0],
        [a*lam,               0.0,                            b*(lam**2)],
        [0.0,                 c*(lam**p32),                   1.0]
    ], dtype=complex)

def leftU_sorted(Y):
    U, s, Vh = np.linalg.svd(Y)
    idx = np.argsort(s)  # light→heavy
    return U[:, idx], s[idx]

def eval_ckm(au, bu, cu, phi_u, g13, ad, bd, cd, phi_d, p32, r12):
    Yu = Y_u(lam, au, bu, cu, phi_u, g13)
    Yd = Y_d(lam, ad, bd, cd, phi_d, p32, r12)
    Uu, su = leftU_sorted(Yu)
    Ud, sd = leftU_sorted(Yd)
    V  = Uu.conj().T @ Ud
    A  = np.abs(V)
    # Standard rephasing-invariant J (row/col cycled choices equivalent up to sign)
    J  = float(np.imag(V[0,0]*V[1,1]*np.conj(V[0,1])*np.conj(V[1,0])))
    return dict(V=V, A=A, su=su, sd=sd,
                Vus=float(A[0,1]), Vcb=float(A[1,2]), Vub=float(A[0,2]), J=abs(J))

def relerr(pred, data):
    return abs(pred-data)/(data if data!=0 else 1.0)

def wolfenstein_from_V(V):
    lamW = abs(V[0,1])
    A    = abs(V[1,2])/(lamW**2 + 1e-18)
    z    = (- V[0,0]*np.conj(V[0,2])) / (V[1,0]*np.conj(V[1,2]))  # ρ̄+iη̄ (rephasing-invariant)
    return lamW, A, float(np.real(z)), float(np.imag(z))

def UT_angles_from_V(V):
    def ang180(z):
        ang = math.degrees(math.atan2(float(np.imag(z)), float(np.real(z))))
        ang = (ang + 360.0) % 360.0
        return ang if ang <= 180.0 else 360.0 - ang
    alpha = ang180(- V[2,0]*np.conj(V[2,2]) / (V[0,0]*np.conj(V[0,2])))
    beta  = ang180(- V[1,0]*np.conj(V[1,2]) / (V[2,0]*np.conj(V[2,2])))
    gamma = ang180(- V[0,0]*np.conj(V[0,2]) / (V[1,0]*np.conj(V[1,2])))
    return alpha, beta, gamma

def A_from_targets(T=TARGET, lam_in=lam):
    return T["Vcb"]/(lam_in**2)

def softplus(x):  # smooth one-sided penalty
    return math.log1p(math.exp(x))

# score: max rel error on (Vus,Vcb,Vub, 0.5·J) + gentle pulls (λ→input, η̄>0, A→target)
def score_from_result(r, wJ=0.5, wL=0.2, lam_target=lam, w_eta=0.05, w_A=0.05):
    e_us = relerr(r["Vus"], TARGET["Vus"])
    e_cb = relerr(r["Vcb"], TARGET["Vcb"])
    e_ub = relerr(r["Vub"], TARGET["Vub"])
    e_J  = relerr(r["J"],   TARGET["J"])
    e_lam = abs(r["Vus"] - lam_target)/lam_target
    lamW, Ahat, rho_bar, eta_bar = wolfenstein_from_V(r["V"])
    pen_eta = softplus(-10.0*eta_bar)/10.0
    A_star = A_from_targets()
    pen_A = abs(Ahat - A_star)/max(1e-12, A_star)
    base = max(e_us, e_cb, e_ub, wJ*e_J) + wL*e_lam
    return float(base + w_eta*pen_eta + w_A*pen_A)

# ============================================ Baseline print ============================================
r0 = eval_ckm(au, bu0, cu, phi_u0, g13_0, ad0, bd0, cd, phi_d0, p32_0, r12_0)
eu0, ec0, eb0, eJ0 = (relerr(r0["Vus"],TARGET["Vus"]), relerr(r0["Vcb"],TARGET["Vcb"]),
                      relerr(r0["Vub"],TARGET["Vub"]), relerr(r0["J"],TARGET["J"]))
score0 = score_from_result(r0)
lamW0, A0, rho0, eta0 = wolfenstein_from_V(r0["V"])
a0, b0, g0 = UT_angles_from_V(r0["V"])

print("="*100)
print("BASELINE (v4i) — CKM headlines")
print("="*100)
for i, rn in enumerate(["u","c","t"]):
    print(rn+" :", "  ".join(f"{r0['A'][i,j]:.9f}" for j in range(3)))
print("\nHeadlines vs targets:")
print(f"  |Vus|  pred={r0['Vus']:.9f}  data={TARGET['Vus']:.9f}  rel_err={eu0:.3e}")
print(f"  |Vcb|  pred={r0['Vcb']:.9f}  data={TARGET['Vcb']:.9f}  rel_err={ec0:.3e}")
print(f"  |Vub|  pred={r0['Vub']:.9f}  data={TARGET['Vub']:.9f}  rel_err={eb0:.3e}")
print(f"  J      pred={r0['J']:.9e}  data={TARGET['J']:.9e}  rel_err={eJ0:.3e}")
print(f"[score0] = {score0:.3e}\n")

print("="*100)
print("WOLFENSTEIN & UT (baseline)")
print("="*100)
print(f"λ (from Vus) ~ {lamW0:.9f}  (input λ={lam:.12f})")
print(f"A ~ {A0:.6f}")
print(f"ρ̄ ~ {rho0:.6f},   η̄ ~ {eta0:.6f}")
print(f"α ~ {a0:.3f}°,  β ~ {b0:.3f}°,  γ ~ {g0:.3f}°  |  α+β+γ ≈ {a0+b0+g0:.3f}°\n")

# ============================================ Search space ============================================
names = ["r12","phi_u","phi_d","b_u","p32","a_d","b_d","g13"]
lo    = np.array([ 0.60,  0.00,  0.00,  0.60, 1.50, 0.80, 0.60, 0.20], dtype=float)
hi    = np.array([ 1.80,  2*np.pi, 2*np.pi, 1.60, 2.60, 1.90, 1.20, 0.90], dtype=float)

def clamp(x): return np.minimum(hi, np.maximum(lo, x))

def eval_from_x(x):
    r12, phi_u, phi_d, b_u, p32, a_d, b_d, g13 = map(float, x)
    return eval_ckm(au, b_u, cu, phi_u, g13, a_d, b_d, cd, phi_d, p32, r12)

def score_from_x(x):
    return score_from_result(eval_from_x(x))

# ========================================= Bounded Nelder–Mead =========================================
def nelder_mead(x0, step, max_eval=2200, alpha=1.0, gamma=2.0, rho=0.5, sigma=0.5):
    N = len(x0)
    simplex = [clamp(x0.copy())]
    for i in range(N):
        xi = x0.copy()
        xi[i] = xi[i] + step[i]
        simplex.append(clamp(xi))
    fvals = [score_from_x(x) for x in simplex]
    evals = len(fvals)

    def sort_simplex():
        nonlocal simplex, fvals
        idx = np.argsort(fvals)
        simplex = [simplex[i] for i in idx]
        fvals   = [fvals[i]   for i in idx]

    sort_simplex()

    while evals < max_eval:
        sort_simplex()
        best, worst, second = simplex[0], simplex[-1], simplex[-2]
        fbest, fworst, fsecond = fvals[0], fvals[-1], fvals[-2]
        xbar = np.mean(simplex[:-1], axis=0)
        # reflect
        xr = clamp(xbar + alpha*(xbar - worst))
        fr = score_from_x(xr); evals += 1
        if fr < fsecond and fr >= fbest:
            simplex[-1], fvals[-1] = xr, fr
            continue
        if fr < fbest:
            # expand
            xe = clamp(xbar + gamma*(xr - xbar))
            fe = score_from_x(xe); evals += 1
            if fe < fr:
                simplex[-1], fvals[-1] = xe, fe
            else:
                simplex[-1], fvals[-1] = xr, fr
            continue
        # contract
        xc = clamp(xbar + rho*(worst - xbar))
        fc = score_from_x(xc); evals += 1
        if fc < fworst:
            simplex[-1], fvals[-1] = xc, fc
            continue
        # shrink
        for i in range(1, len(simplex)):
            simplex[i] = clamp(best + sigma*(simplex[i] - best))
            fvals[i] = score_from_x(simplex[i])
            evals += 1
            if evals >= max_eval:
                break

    sort_simplex()
    return simplex[0], fvals[0]

# ======================================= Auto restarts & search =======================================
seed = np.array([r12_0, phi_u0, phi_d0, bu0, p32_0, ad0, bd0, g13_0], dtype=float)
base_step = np.array([0.08, 0.25, 0.25, 0.08, 0.10, 0.06, 0.05, 0.06], dtype=float)

best_x, best_s = seed.copy(), score0
tries = []
for restart in range(12):
    # jitter seed; inflate steps on later restarts
    jitter = np.array([
        (rng.random()-0.5)*0.60,   # r12 ±0.30
        (rng.random()-0.5)*1.50,   # phi_u ±0.75
        (rng.random()-0.5)*1.50,   # phi_d ±0.75
        (rng.random()-0.5)*0.40,   # b_u  ±0.20
        (rng.random()-0.5)*0.60,   # p32 ±0.30
        (rng.random()-0.5)*0.40,   # a_d  ±0.20
        (rng.random()-0.5)*0.30,   # b_d  ±0.15
        (rng.random()-0.5)*0.30,   # g13 ±0.15
    ])
    x0 = clamp(seed + jitter)
    step = base_step * (1.0 + 0.35*restart)  # inflate each restart

    x_star, s_star = nelder_mead(x0, step, max_eval=2200)
    tries.append((s_star, x_star))
    if s_star < best_s:
        best_x, best_s = x_star, s_star

best_s, best_x = min(tries, key=lambda t: t[0]) if tries else (score0, seed)

# ============================================ Print knob deltas ============================================
r12_f, phi_u_f, phi_d_f, b_u_f, p32_f, a_d_f, b_d_f, g13_f = map(float, best_x)

print("="*100)
print("TIGHTENED KNOBS (v4n): sensitive + a_d + b_d + g13")
print("="*100)
for n, old, new in zip(
    ["r12","phi_u","phi_d","b_u","p32","a_d","b_d","g13"],
    [r12_0,phi_u0,phi_d0,bu0,p32_0,ad0,bd0,g13_0],
    [r12_f,phi_u_f,phi_d_f,b_u_f,p32_f,a_d_f,b_d_f,g13_f]
):
    print(f"{n:5s}: {old:8.3f}  →  {new:8.3f}  (Δ={new-old:+.3f})")
print("\nFixed (kept at v4i): a_u=%.3f, c_u=%.3f, c_d=%.3f" % (au, cu, cd))

# =============================================== Results ===============================================
best_r = eval_from_x(best_x)
euB, ecB, ebB, eJB = (relerr(best_r["Vus"],TARGET["Vus"]), relerr(best_r["Vcb"],TARGET["Vcb"]),
                      relerr(best_r["Vub"],TARGET["Vub"]), relerr(best_r["J"],TARGET["J"]))
lamW2, A2, rh2, et2 = wolfenstein_from_V(best_r["V"])
a2, b2, g2 = UT_angles_from_V(best_r["V"])

print("="*100)
print("CKM (v4n tightened) — headlines")
print("="*100)
for i, rn in enumerate(["u","c","t"]):
    print(rn+" :", "  ".join(f"{best_r['A'][i,j]:.9f}" for j in range(3)))
print("\nHeadlines vs targets:")
print(f"  |Vus|  pred={best_r['Vus']:.9f}  data={TARGET['Vus']:.9f}  rel_err={euB:.3e}")
print(f"  |Vcb|  pred={best_r['Vcb']:.9f}  data={TARGET['Vcb']:.9f}  rel_err={ecB:.3e}")
print(f"  |Vub|  pred={best_r['Vub']:.9f}  data={TARGET['Vub']:.9f}  rel_err={ebB:.3e}")
print(f"  J      pred={best_r['J']:.9e}  data={TARGET['J']:.9e}  rel_err={eJB:.3e}")
print(f"[score] = {score_from_result(best_r):.3e}   (baseline {score0:.3e})\n")

print("="*100)
print("WOLFENSTEIN & UT (v4n)")
print("="*100)
print(f"λ (from Vus) ~ {lamW2:.9f}  |  A ~ {A2:.6f}")
print(f"ρ̄ ~ {rh2:.6f},   η̄ ~ {et2:.6f}")
print(f"α ~ {a2:.3f}°,  β ~ {b2:.3f}°,  γ ~ {g2:.3f}°  |  α+β+γ ≈ {a2+b2+g2:.3f}°\n")

# ========================================= tight stability sweep =========================================
def jitter_tight(x, frac=0.02, dphi=0.05, dp32=0.05, dr12=0.02, dad=0.02, dbd=0.02, dg13=0.02):
    r12, phi_u, phi_d, b_u, p32, a_d, b_d, g13 = x
    def jj(v,f): return v*(1 + (random.random()-0.5)*2*f)
    def jp(p):  return p + (random.random()-0.5)*2*dphi
    return np.array([
        min(hi[0], max(lo[0], r12 + (random.random()-0.5)*2*dr12)),
        min(hi[1], max(lo[1], jp(phi_u))),
        min(hi[2], max(lo[2], jp(phi_d))),
        min(hi[3], max(lo[3], jj(b_u, frac))),
        min(hi[4], max(lo[4], p32 + (random.random()-0.5)*2*dp32)),
        min(hi[5], max(lo[5], jj(a_d, dad))),
        min(hi[6], max(lo[6], jj(b_d, dbd))),
        min(hi[7], max(lo[7], jj(g13, dg13))),
    ])

def quick_score(x_try):
    r12x, phi_ux, phi_dx, b_ux, p32x, a_dx, b_dx, g13x = map(float, x_try)
    r = eval_ckm(au, b_ux, cu, phi_ux, g13x, a_dx, b_dx, cd, phi_dx, p32x, r12x)
    return max(relerr(r["Vus"],TARGET["Vus"]), relerr(r["Vcb"],TARGET["Vcb"]),
               relerr(r["Vub"],TARGET["Vub"]), 0.5*relerr(r["J"],TARGET["J"]))

N=400
errs = sorted(quick_score(jitter_tight(best_x)) for _ in range(N))
p50, p90, p99 = errs[N//2], errs[int(0.9*N)], errs[int(0.99*N)]
print("="*100)
print("STABILITY (tight jitter)")
print("="*100)
print(f"median score   = {p50:.3e}")
print(f"90th pct       = {p90:.3e}")
print(f"99th pct       = {p99:.3e}")

# ============================================== Artifacts ==============================================
OUT_DIR = "/content"
REPORT_DIR = os.path.join(OUT_DIR, "reports")
os.makedirs(REPORT_DIR, exist_ok=True)
RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

card = {
  "module": "UPT_TEXTURES_v4n_card",
  "run_ts": RUN_TS + "Z",
  "inputs": {
    "lambda_in": lam,
    "targets": TARGET
  },
  "knobs": {
    "a_u": au, "b_u": float(b_u_f), "c_u": cu, "phi_u": float(phi_u_f),
    "a_d": float(a_d_f), "b_d": float(b_d_f), "c_d": cd, "phi_d": float(phi_d_f),
    "p32": float(p32_f), "r12": float(r12_f), "g13": float(g13_f)
  },
  "ckm": {
    "Vus": float(best_r["Vus"]), "Vcb": float(best_r["Vcb"]),
    "Vub": float(best_r["Vub"]), "J": float(best_r["J"])
  },
  "wolfenstein": {
    "lambda_from_Vus": float(lamW2), "A": float(A2),
    "rho_bar": float(rh2), "eta_bar": float(et2)
  },
  "UT_deg": {"alpha": float(a2), "beta": float(b2), "gamma": float(g2)},
  "score": float(score_from_result(best_r)),
  "stability": {"p50": float(p50), "p90": float(p90), "p99": float(p99)}
}

CARD_JSON = os.path.join(OUT_DIR, f"UPT_texture_card_v4n_{RUN_TS}.json")
with open(CARD_JSON, "w") as f:
    f.write(json.dumps(card, ensure_ascii=False, indent=2))

ASC_OUT = os.path.join(REPORT_DIR, f"UPT_ascii_TEXTURES_v4n_{RUN_TS}.txt")
MD_OUT  = os.path.join(REPORT_DIR, f"UPT_report_TEXTURES_v4n_{RUN_TS}.md")

with open(ASC_OUT, "w") as f:
    f.write("="*100 + "\nTEXTURE v4n — CARD\n" + "="*100 + "\n\n")
    f.write(json.dumps(card, ensure_ascii=False, indent=2))

with open(MD_OUT, "w") as f:
    f.write("# TEXTURE v4n — card\n\n")
    f.write(f"- Run: **{RUN_TS}Z**\n\n")
    f.write("```json\n" + json.dumps(card, ensure_ascii=False, indent=2) + "\n```\n")

print("="*100)
print("EMIT ARTIFACTS")
print("="*100)
print("[BEGIN SECTION:write_files]")
print(f"[KV] CARD  = {CARD_JSON}")
print(f"[KV] ASCII = {ASC_OUT}")
print(f"[KV] MD    = {MD_OUT}")
print("[END SECTION:write_files]")
print(f"[KV] run_ts = {RUN_TS}Z")

# =================== UPT_TEXTURES_v4n_plus2 — NM + multistart, stability, sensitivities, artifacts ===================
# Jupyter/Colab-safe: uses argparse.parse_known_args() to ignore unknown flags like "-f <kernel.json>".

import numpy as np, cmath, math, random, json, os, datetime as dt, argparse, sys, csv

# ============================ USER-FACING CONSTANTS (safe defaults; overridable via CLI) ============================
lam_default = 0.224299065421
TARGET_default = dict(
    Vus=0.224298257358,
    Vcb=0.042199673766,
    Vub=0.003939999959,
    J=3.384301967056e-05
)
# v4i baseline (fixed “O(1)” entries kept unless searched)
au_def, bu0_def, cu_def, phi_u0_def, g13_0_def = 1.250, 1.180, 0.852, 1.995, 0.500
ad0_def, bd0_def, cd_def, phi_d0_def, p32_0_def, r12_0_def = 1.377, 0.875, 1.621, 1.950, 1.800, 1.050

# ============================================== Utilities ==============================================
def safe_deg(z):
    return math.degrees(math.atan2(float(np.imag(z)), float(np.real(z))))

def format_row(vals, prec=9):
    return "  ".join(f"{float(v):.{prec}f}" for v in vals)

def softplus(x):  # smooth one-sided penalty
    return math.log1p(math.exp(x))

# ============================================== Textures ===============================================
def Y_u(lam, a, b, c, phi, gamma13):
    return np.array([
        [0.0,                 a*lam*cmath.exp(1j*phi),  gamma13*(lam**3)],
        [a*lam,               0.0,                      b*(lam**2)],
        [0.0,                 c*(lam**3),               1.0]
    ], dtype=complex)

def Y_d(lam, a, b, c, phi, p32, r12):
    return np.array([
        [0.0,                 (r12*a*lam)*cmath.exp(1j*phi),  0.0],
        [a*lam,               0.0,                            b*(lam**2)],
        [0.0,                 c*(lam**p32),                   1.0]
    ], dtype=complex)

def leftU_sorted(Y):
    U, s, Vh = np.linalg.svd(Y)
    idx = np.argsort(s)  # light→heavy
    return U[:, idx], s[idx]

def eval_ckm(lam, au, bu, cu, phi_u, g13, ad, bd, cd, phi_d, p32, r12):
    Yu = Y_u(lam, au, bu, cu, phi_u, g13)
    Yd = Y_d(lam, ad, bd, cd, phi_d, p32, r12)
    Uu, su = leftU_sorted(Yu)
    Ud, sd = leftU_sorted(Yd)
    V  = Uu.conj().T @ Ud
    A  = np.abs(V)
    J  = float(np.imag(V[0,0]*V[1,1]*np.conj(V[0,1])*np.conj(V[1,0])))
    return dict(V=V, A=A, su=su, sd=sd,
                Vus=float(A[0,1]), Vcb=float(A[1,2]), Vub=float(A[0,2]), J=abs(J))

def relerr(pred, data):
    return abs(pred-data)/(data if data!=0 else 1.0)

def wolfenstein_from_V(V):
    lamW = abs(V[0,1])
    A    = abs(V[1,2])/(lamW**2 + 1e-18)
    z    = (- V[0,0]*np.conj(V[0,2])) / (V[1,0]*np.conj(V[1,2]))  # ρ̄+iη̄ (rephasing-invariant)
    return lamW, A, float(np.real(z)), float(np.imag(z))

def UT_angles_from_V(V):
    def to_0_180(deg):
        x = (deg + 360.0) % 360.0
        return x if x <= 180.0 else 360.0 - x
    alpha = to_0_180(safe_deg(- V[2,0]*np.conj(V[2,2]) / (V[0,0]*np.conj(V[0,2]))))
    beta  = to_0_180(safe_deg(- V[1,0]*np.conj(V[1,2]) / (V[2,0]*np.conj(V[2,2]))))
    gamma = to_0_180(safe_deg(- V[0,0]*np.conj(V[0,2]) / (V[1,0]*np.conj(V[1,2]))))
    return alpha, beta, gamma

def unitarity_error(V):
    I = np.eye(3, dtype=complex)
    delta = V.conj().T @ V - I
    return float(np.max(np.abs(delta)))

def A_from_targets(T, lam_in):
    return T["Vcb"]/(lam_in**2)

# ============================================== Score ===============================================
def score_from_result(r, T, lam_target, wJ=0.5, wL=0.2, w_eta=0.05, w_A=0.05):
    e_us = relerr(r["Vus"], T["Vus"])
    e_cb = relerr(r["Vcb"], T["Vcb"])
    e_ub = relerr(r["Vub"], T["Vub"])
    e_J  = relerr(r["J"],   T["J"])
    e_lam = abs(r["Vus"] - lam_target)/lam_target
    lamW, Ahat, rho_bar, eta_bar = wolfenstein_from_V(r["V"])
    pen_eta = softplus(-10.0*eta_bar)/10.0
    A_star = A_from_targets(T, lam_target)
    pen_A = abs(Ahat - A_star)/max(1e-12, A_star)
    base = max(e_us, e_cb, e_ub, wJ*e_J) + wL*e_lam
    return float(base + w_eta*pen_eta + w_A*pen_A)

# ============================================ Bounded NM ============================================
def clamp(x, lo, hi):
    return np.minimum(hi, np.maximum(lo, x))

def nelder_mead(x0, step, lo, hi, eval_fn, max_eval=2200, alpha=1.0, gamma=2.0, rho=0.5, sigma=0.5):
    N = len(x0)
    simplex = [clamp(x0.copy(), lo, hi)]
    for i in range(N):
        xi = x0.copy()
        xi[i] = xi[i] + step[i]
        simplex.append(clamp(xi, lo, hi))
    fvals = [eval_fn(x) for x in simplex]
    evals = len(fvals)
    def sort_simplex():
        nonlocal simplex, fvals
        idx = np.argsort(fvals)
        simplex = [simplex[i] for i in idx]
        fvals   = [fvals[i]   for i in idx]
    sort_simplex()
    while evals < max_eval:
        sort_simplex()
        best, worst, second = simplex[0], simplex[-1], simplex[-2]
        fbest, fworst, fsecond = fvals[0], fvals[-1], fvals[-2]
        xbar = np.mean(simplex[:-1], axis=0)
        xr = clamp(xbar + alpha*(xbar - worst), lo, hi)
        fr = eval_fn(xr); evals += 1
        if fr < fsecond and fr >= fbest:
            simplex[-1], fvals[-1] = xr, fr
            continue
        if fr < fbest:
            xe = clamp(xbar + gamma*(xr - xbar), lo, hi)
            fe = eval_fn(xe); evals += 1
            simplex[-1], fvals[-1] = (xe, fe) if fe < fr else (xr, fr)
            continue
        xc = clamp(xbar + rho*(worst - xbar), lo, hi)
        fc = eval_fn(xc); evals += 1
        if fc < fworst:
            simplex[-1], fvals[-1] = xc, fc
            continue
        for i in range(1, len(simplex)):
            simplex[i] = clamp(best + sigma*(simplex[i] - best), lo, hi)
            fvals[i] = eval_fn(simplex[i])
            evals += 1
            if evals >= max_eval: break
    sort_simplex()
    return simplex[0], fvals[0]

# ============================================== Driver ===============================================
def run_module(seed_rng=1234, lam=lam_default, TARGET=TARGET_default,
               au=au_def, bu0=bu0_def, cu=cu_def, phi_u0=phi_u0_def, g13_0=g13_0_def,
               ad0=ad0_def, bd0=bd0_def, cd=cd_def, phi_d0=phi_d0_def, p32_0=p32_0_def, r12_0=r12_0_def,
               restarts=12, max_eval=2200, jitter_scale=1.0, out_dir="/content"):
    rng = np.random.default_rng(seed_rng)

    # Search box & names
    names = ["r12","phi_u","phi_d","b_u","p32","a_d","b_d","g13"]
    lo = np.array([ 0.60,  0.00,  0.00,  0.60, 1.50, 0.80, 0.60, 0.20], dtype=float)
    hi = np.array([ 1.80,  2*np.pi, 2*np.pi, 1.60, 2.60, 1.90, 1.20, 0.90], dtype=float)

    def eval_from_x(x):
        r12, phi_u, phi_d, b_u, p32, a_d, b_d, g13 = map(float, x)
        return eval_ckm(lam, au, b_u, cu, phi_u, g13, a_d, b_d, cd, phi_d, p32, r12)

    def score_from_x(x):
        return score_from_result(eval_from_x(x), TARGET, lam)

    # Baseline print
    r0 = eval_ckm(lam, au, bu0, cu, phi_u0, g13_0, ad0, bd0, cd, phi_d0, p32_0, r12_0)
    eu0, ec0, eb0, eJ0 = (relerr(r0["Vus"],TARGET["Vus"]), relerr(r0["Vcb"],TARGET["Vcb"]),
                          relerr(r0["Vub"],TARGET["Vub"]), relerr(r0["J"],TARGET["J"]))
    score0 = score_from_result(r0, TARGET, lam)
    lamW0, A0, rho0, eta0 = wolfenstein_from_V(r0["V"])
    a0, b0, g0 = UT_angles_from_V(r0["V"])
    uni0 = unitarity_error(r0["V"])

    print("="*100)
    print("BASELINE (v4i) — CKM headlines")
    print("="*100)
    for i, rn in enumerate(["u","c","t"]):
        print(rn+" :", format_row([r0["A"][i,0], r0["A"][i,1], r0["A"][i,2]]))
    print("\nHeadlines vs targets:")
    print(f"  |Vus|  pred={r0['Vus']:.9f}  data={TARGET['Vus']:.9f}  rel_err={eu0:.3e}")
    print(f"  |Vcb|  pred={r0['Vcb']:.9f}  data={TARGET['Vcb']:.9f}  rel_err={ec0:.3e}")
    print(f"  |Vub|  pred={r0['Vub']:.9f}  data={TARGET['Vub']:.9f}  rel_err={eb0:.3e}")
    print(f"  J      pred={r0['J']:.9e}  data={TARGET['J']:.9e}  rel_err={eJ0:.3e}")
    print(f"[unitarity err] = {uni0:.3e}")
    print(f"[score0] = {score0:.3e}\n")

    print("="*100)
    print("WOLFENSTEIN & UT (baseline)")
    print("="*100)
    print(f"λ (from Vus) ~ {lamW0:.9f}  (input λ={lam:.12f})")
    print(f"A ~ {A0:.6f}")
    print(f"ρ̄ ~ {rho0:.6f},   η̄ ~ {eta0:.6f}")
    print(f"α ~ {a0:.3f}°,  β ~ {b0:.3f}°,  γ ~ {g0:.3f}°  |  α+β+γ ≈ {a0+b0+g0:.3f}°\n")

    # Simplex search configuration
    seed_x = np.array([r12_0, phi_u0, phi_d0, bu0, p32_0, ad0, bd0, g13_0], dtype=float)
    base_step = np.array([0.08, 0.25, 0.25, 0.08, 0.10, 0.06, 0.05, 0.06], dtype=float)

    best_x, best_s = seed_x.copy(), score0
    tries = []
    for restart in range(restarts):
        # jitter seed; inflate steps on later restarts
        jitter = np.array([
            (rng.random()-0.5)*0.60*jitter_scale,   # r12 ±0.30
            (rng.random()-0.5)*1.50*jitter_scale,   # phi_u ±0.75
            (rng.random()-0.5)*1.50*jitter_scale,   # phi_d ±0.75
            (rng.random()-0.5)*0.40*jitter_scale,   # b_u  ±0.20
            (rng.random()-0.5)*0.60*jitter_scale,   # p32 ±0.30
            (rng.random()-0.5)*0.40*jitter_scale,   # a_d  ±0.20
            (rng.random()-0.5)*0.30*jitter_scale,   # b_d  ±0.15
            (rng.random()-0.5)*0.30*jitter_scale,   # g13 ±0.15
        ])
        x0 = clamp(seed_x + jitter, lo, hi)
        step = base_step * (1.0 + 0.35*restart)

        x_star, s_star = nelder_mead(x0, step, lo, hi, score_from_x, max_eval=max_eval)
        tries.append((s_star, x_star))
        if s_star < best_s:
            best_x, best_s = x_star, s_star

    best_s, best_x = min(tries, key=lambda t: t[0]) if tries else (score0, seed_x)
    r12_f, phi_u_f, phi_d_f, b_u_f, p32_f, a_d_f, b_d_f, g13_f = map(float, best_x)

    # Print knob deltas
    print("="*100)
    print("TIGHTENED KNOBS (v4n_plus2)")
    print("="*100)
    for n, old, new in zip(
        ["r12","phi_u","phi_d","b_u","p32","a_d","b_d","g13"],
        [r12_0,phi_u0,phi_d0,bu0,p32_0,ad0,bd0,g13_0],
        [r12_f,phi_u_f,phi_d_f,b_u_f,p32_f,a_d_f,b_d_f,g13_f]
    ):
        print(f"{n:5s}: {old:8.3f}  →  {new:8.3f}  (Δ={new-old:+.3f})")
    print("\nFixed (kept at v4i): a_u=%.3f, c_u=%.3f, c_d=%.3f" % (au, cu, cd))

    # Results & prints
    best_r = eval_from_x(best_x)
    euB, ecB, ebB, eJB = (relerr(best_r["Vus"],TARGET["Vus"]), relerr(best_r["Vcb"],TARGET["Vcb"]),
                          relerr(best_r["Vub"],TARGET["Vub"]), relerr(best_r["J"],TARGET["J"]))
    lamW2, A2, rh2, et2 = wolfenstein_from_V(best_r["V"])
    a2, b2, g2 = UT_angles_from_V(best_r["V"])
    uni = unitarity_error(best_r["V"])

    print("="*100)
    print("CKM (v4n_plus2 tightened) — headlines")
    print("="*100)
    for i, rn in enumerate(["u","c","t"]):
        print(rn+" :", format_row([best_r["A"][i,0], best_r["A"][i,1], best_r["A"][i,2]]))
    print("\nHeadlines vs targets:")
    print(f"  |Vus|  pred={best_r['Vus']:.9f}  data={TARGET['Vus']:.9f}  rel_err={euB:.3e}")
    print(f"  |Vcb|  pred={best_r['Vcb']:.9f}  data={TARGET['Vcb']:.9f}  rel_err={ecB:.3e}")
    print(f"  |Vub|  pred={best_r['Vub']:.9f}  data={TARGET['Vub']:.9f}  rel_err={ebB:.3e}")
    print(f"  J      pred={best_r['J']:.9e}  data={TARGET['J']:.9e}  rel_err={eJB:.3e}")
    print(f"[unitarity err] = {uni:.3e}")
    print(f"[score] = {score_from_result(best_r, TARGET, lam):.3e}   (baseline {score0:.3e})\n")

    print("="*100)
    print("WOLFENSTEIN & UT (v4n_plus2)")
    print("="*100)
    print(f"λ (from Vus) ~ {lamW2:.9f}  |  A ~ {A2:.6f}")
    print(f"ρ̄ ~ {rh2:.6f},   η̄ ~ {et2:.6f}")
    print(f"α ~ {a2:.3f}°,  β ~ {b2:.3f}°,  γ ~ {g2:.3f}°  |  α+β+γ ≈ {a2+b2+g2:.3f}°\n")

    # Stability jitter
    def jitter_tight(x, frac=0.02, dphi=0.05, dp32=0.05, dr12=0.02, dad=0.02, dbd=0.02, dg13=0.02):
        r12, phi_u, phi_d, b_u, p32, a_d, b_d, g13 = x
        def jj(v,f): return v*(1 + (random.random()-0.5)*2*f)
        def jp(p):  return p + (random.random()-0.5)*2*dphi
        return np.array([
            min(hi[0], max(lo[0], r12 + (random.random()-0.5)*2*dr12)),
            min(hi[1], max(lo[1], jp(phi_u))),
            min(hi[2], max(lo[2], jp(phi_d))),
            min(hi[3], max(lo[3], jj(b_u, frac))),
            min(hi[4], max(lo[4], p32 + (random.random()-0.5)*2*dp32)),
            min(hi[5], max(lo[5], jj(a_d, dad))),
            min(hi[6], max(lo[6], jj(b_d, dbd))),
            min(hi[7], max(lo[7], jj(g13, dg13))),
        ])

    def quick_score(x_try):
        r12x, phi_ux, phi_dx, b_ux, p32x, a_dx, b_dx, g13x = map(float, x_try)
        r = eval_ckm(lam, au, b_ux, cu, phi_ux, g13x, a_dx, b_dx, cd, phi_dx, p32x, r12x)
        return max(relerr(r["Vus"],TARGET["Vus"]), relerr(r["Vcb"],TARGET["Vcb"]),
                   relerr(r["Vub"],TARGET["Vub"]), 0.5*relerr(r["J"],TARGET["J"]))

    N=400
    errs = sorted(quick_score(jitter_tight(best_x)) for _ in range(N))
    p50, p90, p99 = errs[N//2], errs[int(0.9*N)], errs[int(0.99*N)]
    print("="*100)
    print("STABILITY (tight jitter)")
    print("="*100)
    print(f"median score   = {p50:.3e}")
    print(f"90th pct       = {p90:.3e}")
    print(f"99th pct       = {p99:.3e}")

    # Simple finite-difference sensitivities (+1% bumps, +0.05 for phases, +0.05 for p32/r12)
    print("\n" + "="*100)
    print("SENSITIVITIES — Δscore for small bumps")
    print("="*100)
    base_score = score_from_result(best_r, TARGET, lam)
    bumps = {
        "r12": 0.01, "phi_u": 0.05, "phi_d": 0.05, "b_u": 0.01, "p32": 0.05,
        "a_d": 0.01, "b_d": 0.01, "g13": 0.01
    }
    sens = {}
    for i, n in enumerate(["r12","phi_u","phi_d","b_u","p32","a_d","b_d","g13"]):
        xb = best_x.copy()
        if n in ["phi_u","phi_d","p32","r12"]:
            inc = bumps[n]
            xb[i] = min(hi[i], max(lo[i], xb[i] + inc))
        else:
            inc = bumps[n]*xb[i]
            xb[i] = min(hi[i], max(lo[i], xb[i] + inc))
        s = score_from_x(xb)
        sens[n] = float(s - best_s)
        print(f"{n:5s} : Δscore≈ {s - best_s:+.3e}")

    # =============================== Artifacts ===============================
    OUT_DIR = out_dir
    REPORT_DIR = os.path.join(OUT_DIR, "reports")
    os.makedirs(REPORT_DIR, exist_ok=True)
    RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

    card = {
      "module": "UPT_TEXTURES_v4n_plus2",
      "run_ts": RUN_TS + "Z",
      "rng_seed": int(seed_rng),
      "inputs": {
        "lambda_in": float(lam),
        "targets": {k: float(v) for k,v in TARGET.items()},
        "fixed": {"a_u": float(au), "c_u": float(cu), "c_d": float(cd)}
      },
      "knobs": {
        "r12": float(r12_f), "phi_u": float(phi_u_f), "phi_d": float(phi_d_f),
        "b_u": float(b_u_f), "p32": float(p32_f), "a_d": float(a_d_f),
        "b_d": float(b_d_f), "g13": float(g13_f)
      },
      "ckm": {
        "Vus": float(best_r["Vus"]), "Vcb": float(best_r["Vcb"]),
        "Vub": float(best_r["Vub"]), "J": float(best_r["J"]),
        "unitarity_err": float(unitarity_error(best_r["V"]))
      },
      "wolfenstein": {
        "lambda_from_Vus": float(lamW2), "A": float(A2),
        "rho_bar": float(rh2), "eta_bar": float(et2)
      },
      "UT_deg": {"alpha": float(a2), "beta": float(b2), "gamma": float(g2)},
      "score": float(base_score),
      "stability": {"p50": float(p50), "p90": float(p90), "p99": float(p99)},
      "sensitivities": sens
    }

    CARD_JSON = os.path.join(OUT_DIR, f"UPT_texture_card_v4n_plus2_{RUN_TS}.json")
    with open(CARD_JSON, "w") as f:
        f.write(json.dumps(card, ensure_ascii=False, indent=2))

    ASC_OUT = os.path.join(REPORT_DIR, f"UPT_ascii_TEXTURES_v4n_plus2_{RUN_TS}.txt")
    MD_OUT  = os.path.join(REPORT_DIR, f"UPT_report_TEXTURES_v4n_plus2_{RUN_TS}.md")
    CKM_CSV = os.path.join(REPORT_DIR, f"UPT_texture_ckm_v4n_plus2_{RUN_TS}.csv")

    with open(ASC_OUT, "w") as f:
        f.write("="*100 + "\nTEXTURE v4n_plus2 — CARD\n" + "="*100 + "\n\n")
        f.write(json.dumps(card, ensure_ascii=False, indent=2))

    with open(MD_OUT, "w") as f:
        f.write("# TEXTURE v4n_plus2 — card\n\n")
        f.write(f"- Run: **{RUN_TS}Z**  \n- Seed: **{seed_rng}**\n\n")
        f.write("```json\n" + json.dumps(card, ensure_ascii=False, indent=2) + "\n```\n")

    # CKM CSV (magnitudes and phases)
    A = best_r["A"]; V = best_r["V"]
    with open(CKM_CSV, "w", newline="") as f:
        w = csv.writer(f)
        w.writerow(["slot","abs","real","imag","phase_deg"])
        for i in range(3):
            for j in range(3):
                vij = V[i,j]
                w.writerow([f"V{i+1}{j+1}", float(A[i,j]), float(np.real(vij)), float(np.imag(vij)),
                            float(safe_deg(vij))])

    print("="*100)
    print("EMIT ARTIFACTS")
    print("="*100)
    print("[BEGIN SECTION:write_files]")
    print(f"[KV] CARD  = {CARD_JSON}")
    print(f"[KV] ASCII = {ASC_OUT}")
    print(f"[KV] MD    = {MD_OUT}")
    print(f"[KV] CSV   = {CKM_CSV}")
    print("[END SECTION:write_files]")
    print(f"[KV] run_ts = {RUN_TS}Z")

    return card

# ============================================== CLI (Jupyter-safe) ===============================================
if __name__ == "__main__":
    p = argparse.ArgumentParser(description="UPT_TEXTURES_v4n_plus2 (Jupyter-safe)")
    p.add_argument("--seed", type=int, default=1234)
    p.add_argument("--restarts", type=int, default=12)
    p.add_argument("--max_eval", type=int, default=2200)
    p.add_argument("--jitter_scale", type=float, default=1.0)
    p.add_argument("--out_dir", type=str, default="/content")
    # optional: override λ and targets
    p.add_argument("--lam", type=float, default=lam_default)
    p.add_argument("--Vus", type=float, default=TARGET_default["Vus"])
    p.add_argument("--Vcb", type=float, default=TARGET_default["Vcb"])
    p.add_argument("--Vub", type=float, default=TARGET_default["Vub"])
    p.add_argument("--J",   type=float, default=TARGET_default["J"])

    # --- Jupyter/Colab fix: ignore unknown args like "-f <kernel.json>"
    args, unknown = p.parse_known_args()
    if unknown:
        # Keep it quiet but you can uncomment for debugging:
        # print(f"[info] Ignoring unknown CLI args: {unknown}")
        pass

    TARGET = dict(Vus=args.Vus, Vcb=args.Vcb, Vub=args.Vub, J=args.J)
    run_module(seed_rng=args.seed, lam=args.lam, TARGET=TARGET,
               restarts=args.restarts, max_eval=args.max_eval,
               jitter_scale=args.jitter_scale, out_dir=args.out_dir)

# =================== UPT_TEXTURES_addons_v1 — lock Vus, micro-tune, batch stability, card I/O, compare ===================
# Jupyter/Colab-safe: argparse.parse_known_args()

import numpy as np, cmath, math, random, json, os, argparse, sys, csv, datetime as dt

# ======== Shared defaults (copy from main module so this file is standalone if run alone) ========
lam_default = 0.224299065421
TARGET_default = dict(Vus=0.224298257358, Vcb=0.042199673766, Vub=0.003939999959, J=3.384301967056e-05)
# Baseline fixed O(1) (same as v4i/v4n_plus2)
au_def, cu_def, cd_def = 1.250, 0.852, 1.621

# ---- helper math/format
def softplus(x): return math.log1p(math.exp(x))
def safe_deg(z): return math.degrees(math.atan2(float(np.imag(z)), float(np.real(z))))
def format_row(vals, prec=9): return "  ".join(f"{float(v):.{prec}f}" for v in vals)

# ---- textures (NNI + lopsided 12_d + continuous p32)
def Y_u(lam, a, b, c, phi, gamma13):
    return np.array([
        [0.0,                 a*lam*cmath.exp(1j*phi),  gamma13*(lam**3)],
        [a*lam,               0.0,                      b*(lam**2)],
        [0.0,                 c*(lam**3),               1.0]
    ], dtype=complex)

def Y_d(lam, a, b, c, phi, p32, r12):
    return np.array([
        [0.0,                 (r12*a*lam)*cmath.exp(1j*phi),  0.0],
        [a*lam,               0.0,                            b*(lam**2)],
        [0.0,                 c*(lam**p32),                   1.0]
    ], dtype=complex)

def leftU_sorted(Y):
    U, s, Vh = np.linalg.svd(Y)
    idx = np.argsort(s)  # light→heavy
    return U[:, idx], s[idx]

def eval_ckm(lam, au, bu, cu, phi_u, g13, ad, bd, cd, phi_d, p32, r12):
    Yu = Y_u(lam, au, bu, cu, phi_u, g13)
    Yd = Y_d(lam, ad, bd, cd, phi_d, p32, r12)
    Uu, su = leftU_sorted(Yu)
    Ud, sd = leftU_sorted(Yd)
    V  = Uu.conj().T @ Ud
    A  = np.abs(V)
    J  = float(np.imag(V[0,0]*V[1,1]*np.conj(V[0,1])*np.conj(V[1,0])))
    return dict(V=V, A=A, su=su, sd=sd,
                Vus=float(A[0,1]), Vcb=float(A[1,2]), Vub=float(A[0,2]), J=abs(J))

def relerr(pred, data): return abs(pred-data)/(data if data!=0 else 1.0)

def wolfenstein_from_V(V):
    lamW = abs(V[0,1])
    A    = abs(V[1,2])/(lamW**2 + 1e-18)
    z    = (- V[0,0]*np.conj(V[0,2])) / (V[1,0]*np.conj(V[1,2]))  # ρ̄+iη̄
    return lamW, A, float(np.real(z)), float(np.imag(z))

def UT_angles_from_V(V):
    def to_0_180(deg):
        x = (deg + 360.0) % 360.0
        return x if x <= 180.0 else 360.0 - x
    alpha = to_0_180(safe_deg(- V[2,0]*np.conj(V[2,2]) / (V[0,0]*np.conj(V[0,2]))))
    beta  = to_0_180(safe_deg(- V[1,0]*np.conj(V[1,2]) / (V[2,0]*np.conj(V[2,2]))))
    gamma = to_0_180(safe_deg(- V[0,0]*np.conj(V[0,2]) / (V[1,0]*np.conj(V[1,2]))))
    return alpha, beta, gamma

def unitarity_error(V):
    I = np.eye(3, dtype=complex)
    delta = V.conj().T @ V - I
    return float(np.max(np.abs(delta)))

def A_from_targets(T, lam_in): return T["Vcb"]/(lam_in**2)

def score_from_result(r, T, lam_target, wJ=0.5, wL=0.2, w_eta=0.05, w_A=0.05):
    e_us = relerr(r["Vus"], T["Vus"])
    e_cb = relerr(r["Vcb"], T["Vcb"])
    e_ub = relerr(r["Vub"], T["Vub"])
    e_J  = relerr(r["J"],   T["J"])
    e_lam = abs(r["Vus"] - lam_target)/lam_target
    lamW, Ahat, rho_bar, eta_bar = wolfenstein_from_V(r["V"])
    pen_eta = softplus(-10.0*eta_bar)/10.0  # soft penalty if η̄ tries to go negative
    A_star = A_from_targets(T, lam_target)
    pen_A = abs(Ahat - A_star)/max(1e-12, A_star)
    base = max(e_us, e_cb, e_ub, wJ*e_J) + wL*e_lam
    return float(base + w_eta*pen_eta + w_A*pen_A)

# ============== 1) Vus-lock: solve r12 (and optional a_d) to hit |Vus| =================
def lock_Vus(lam, TARGET, knobs, tol=2e-6, itmax=40, adjust_ad=False):
    """
    knobs dict needs: a_u,b_u,c_u,phi_u,g13, a_d,b_d,c_d,phi_d,p32,r12
    Returns (knobs_locked, result)
    """
    k = knobs.copy()
    def vus_of(r12, a_d=None):
        ad = k["a_d"] if a_d is None else a_d
        r = eval_ckm(lam, k["a_u"], k["b_u"], k["c_u"], k["phi_u"], k["g13"],
                          ad, k["b_d"], k["c_d"], k["phi_d"], k["p32"], r12)
        return r["Vus"], r

    # bracket r12 in [0.6, 1.8]
    lo, hi = 0.60, 1.80
    v_lo, _ = vus_of(lo)
    v_hi, _ = vus_of(hi)
    tgt = TARGET["Vus"]

    # If monotonicity is weird, softly adjust a_d (optional)
    if not (min(v_lo, v_hi) <= tgt <= max(v_lo, v_hi)) and adjust_ad:
        # small scaling of a_d to push range to include target
        scale = tgt/max(1e-12, (v_lo+v_hi)/2)
        k["a_d"] = float(np.clip(k["a_d"]*scale, 0.80, 1.90))
        v_lo, _ = vus_of(lo)
        v_hi, _ = vus_of(hi)

    # bisection
    a,b = lo, hi
    fa,_ = vus_of(a); fb,_ = vus_of(b)
    for _ in range(itmax):
        m = 0.5*(a+b)
        fm, rmid = vus_of(m)
        if abs(fm - tgt) < tol:
            k["r12"] = m
            return k, rmid
        # decide side (assume local monotonic)
        if (fa - tgt)*(fm - tgt) <= 0:
            b, fb = m, fm
        else:
            a, fa = m, fm
    # fallback
    k["r12"] = 0.5*(a+b)
    return k, eval_ckm(lam, k["a_u"], k["b_u"], k["c_u"], k["phi_u"], k["g13"],
                            k["a_d"], k["b_d"], k["c_d"], k["phi_d"], k["p32"], k["r12"])

# ============== 2) Micro-tuner: coordinate greedy descent around a point ==============
def micro_tune(lam, TARGET, knobs, steps=None, iters=80):
    """
    Greedy coordinate search on (r12, phi_u, phi_d, b_u, p32, a_d, b_d, g13).
    """
    if steps is None:
        steps = dict(r12=0.01, phi_u=0.05, phi_d=0.05, b_u=0.01, p32=0.02, a_d=0.01, b_d=0.01, g13=0.01)
    box = dict(r12=(0.60,1.80), phi_u=(0.0,2*np.pi), phi_d=(0.0,2*np.pi),
               b_u=(0.60,1.60), p32=(1.50,2.60), a_d=(0.80,1.90), b_d=(0.60,1.20), g13=(0.20,0.90))

    def clamp(n, v):
        lo,hi = box[n]; return float(np.clip(v, lo, hi))

    def res(k):
        r = eval_ckm(lam, k["a_u"], k["b_u"], k["c_u"], k["phi_u"], k["g13"],
                          k["a_d"], k["b_d"], k["c_d"], k["phi_d"], k["p32"], k["r12"])
        return r, score_from_result(r, TARGET, lam)

    best_k = knobs.copy()
    best_r, best_s = res(best_k)
    names = ["r12","phi_u","phi_d","b_u","p32","a_d","b_d","g13"]

    for _ in range(iters):
        improved = False
        for n in names:
            st = steps[n]
            for sign in (+1,-1):
                trial = best_k.copy()
                inc = st if n in ["phi_u","phi_d","p32","r12"] else st*best_k[n]
                trial[n] = clamp(n, trial[n] + sign*inc)
                r_try, s_try = res(trial)
                if s_try < best_s:
                    best_k, best_r, best_s = trial, r_try, s_try
                    improved = True
        if not improved:
            break
    return best_k, best_r, best_s

# ============== 3) Batch stability sweep (CSV + summary) ============================
def batch_stability(lam, TARGET, knobs, N=400, out_dir="/content"):
    rng = np.random.default_rng(2025)
    lo = dict(r12=0.60, phi_u=0.0, phi_d=0.0, b_u=0.60, p32=1.50, a_d=0.80, b_d=0.60, g13=0.20)
    hi = dict(r12=1.80, phi_u=2*np.pi, phi_d=2*np.pi, b_u=1.60, p32=2.60, a_d=1.90, b_d=1.20, g13=0.90)

    def jitter(k):
        j = k.copy()
        # tight jitter ~2% amp on O(1), ~0.05 rad on phases, ~0.05 on p32/r12
        j["r12"] = float(np.clip(k["r12"] + (rng.random()-0.5)*0.10, lo["r12"], hi["r12"]))
        j["phi_u"] = float(np.clip(k["phi_u"] + (rng.random()-0.5)*0.10, lo["phi_u"], hi["phi_u"]))
        j["phi_d"] = float(np.clip(k["phi_d"] + (rng.random()-0.5)*0.10, lo["phi_d"], hi["phi_d"]))
        for n in ["b_u","a_d","b_d","g13"]:
            j[n] = float(np.clip(k[n]*(1 + (rng.random()-0.5)*0.04), lo[n], hi[n]))
        j["p32"] = float(np.clip(k["p32"] + (rng.random()-0.5)*0.10, lo["p32"], hi["p32"]))
        return j

    rows = []
    for _ in range(N):
        kk = jitter(knobs)
        r = eval_ckm(lam, kk["a_u"], kk["b_u"], kk["c_u"], kk["phi_u"], kk["g13"],
                         kk["a_d"], kk["b_d"], kk["c_d"], kk["phi_d"], kk["p32"], kk["r12"])
        s = score_from_result(r, TARGET, lam)
        rows.append(dict(score=s, Vus=r["Vus"], Vcb=r["Vcb"], Vub=r["Vub"], J=r["J"]))

    scores = sorted([row["score"] for row in rows])
    p50 = scores[N//2]; p90 = scores[int(0.9*N)]; p99 = scores[int(0.99*N)]

    RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")
    report_dir = os.path.join(out_dir, "reports"); os.makedirs(report_dir, exist_ok=True)
    csv_path = os.path.join(report_dir, f"UPT_texture_stability_{RUN_TS}.csv")
    with open(csv_path, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=["score","Vus","Vcb","Vub","J"])
        w.writeheader(); w.writerows(rows)

    print("="*100)
    print("STABILITY (batch) — summary")
    print("="*100)
    print(f"median score   = {p50:.3e}")
    print(f"90th pct       = {p90:.3e}")
    print(f"99th pct       = {p99:.3e}")
    print(f"[CSV] {csv_path}")
    return dict(p50=p50, p90=p90, p99=p99, csv=csv_path)

# ============== 4) Card I/O: load card, rebuild, re-score, re-emit ===================
def load_card(path):
    with open(path, "r") as f: return json.load(f)

def rebuild_from_card(card):
    lam = card["inputs"]["lambda_in"]
    T = card["inputs"]["targets"]
    k = dict(
        a_u=card["inputs"]["fixed"]["a_u"], b_u=card["knobs"]["b_u"], c_u=card["inputs"]["fixed"]["c_u"],
        phi_u=card["knobs"]["phi_u"], g13=card["knobs"]["g13"],
        a_d=card["knobs"]["a_d"], b_d=card["knobs"]["b_d"], c_d=card["inputs"]["fixed"]["c_d"],
        phi_d=card["knobs"]["phi_d"], p32=card["knobs"]["p32"], r12=card["knobs"]["r12"],
    )
    r = eval_ckm(lam, k["a_u"], k["b_u"], k["c_u"], k["phi_u"], k["g13"],
                      k["a_d"], k["b_d"], k["c_d"], k["phi_d"], k["p32"], k["r12"])
    s = score_from_result(r, T, lam)
    return lam, T, k, r, s

def emit_card(card, out_dir="/content", tag="reemit"):
    OUT_DIR = out_dir; REPORT_DIR = os.path.join(OUT_DIR, "reports"); os.makedirs(REPORT_DIR, exist_ok=True)
    RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")
    CARD_JSON = os.path.join(OUT_DIR, f"UPT_texture_card_{tag}_{RUN_TS}.json")
    with open(CARD_JSON, "w") as f: f.write(json.dumps(card, ensure_ascii=False, indent=2))
    print("[RE-EMIT]", CARD_JSON); return CARD_JSON

# ============== 5) Compare saved cards =============================================
def compare_cards(paths):
    rows = []
    for p in paths:
        c = load_card(p)
        rows.append((p, c["score"], c["ckm"]["Vus"], c["ckm"]["Vcb"], c["ckm"]["Vub"], c["ckm"]["J"]))
    print("="*100)
    print("COMPARE CARDS")
    print("="*100)
    print("path | score | Vus | Vcb | Vub | J")
    for p, s, vus, vcb, vub, J in rows:
        print(f"{os.path.basename(p)} | {s:.3e} | {vus:.9f} | {vcb:.9f} | {vub:.9f} | {J:.9e}")
    return rows

# ===================================== Demo/CLI =====================================
if __name__ == "__main__":
    ap = argparse.ArgumentParser(description="UPT_TEXTURES_addons_v1 (Jupyter-safe)")
    ap.add_argument("--mode", type=str, default="lock", choices=["lock","tune","sweep","rebuild","compare"])
    ap.add_argument("--seed", type=int, default=2025)
    ap.add_argument("--out_dir", type=str, default="/content")
    ap.add_argument("--lam", type=float, default=lam_default)
    ap.add_argument("--Vus", type=float, default=TARGET_default["Vus"])
    ap.add_argument("--Vcb", type=float, default=TARGET_default["Vcb"])
    ap.add_argument("--Vub", type=float, default=TARGET_default["Vub"])
    ap.add_argument("--J",   type=float, default=TARGET_default["J"])
    ap.add_argument("--card", type=str, default="")
    ap.add_argument("--cards", type=str, nargs="*", default=[])
    # starting knobs (use your v4n_plus2 outcome as defaults)
    ap.add_argument("--r12", type=float, default=1.062)
    ap.add_argument("--phi_u", type=float, default=1.861)
    ap.add_argument("--phi_d", type=float, default=1.311)
    ap.add_argument("--b_u", type=float, default=1.294)
    ap.add_argument("--p32", type=float, default=1.733)
    ap.add_argument("--a_d", type=float, default=1.333)
    ap.add_argument("--b_d", type=float, default=0.954)
    ap.add_argument("--g13", type=float, default=0.632)
    ap.add_argument("--adjust_ad", action="store_true")
    ap.add_argument("--N", type=int, default=400)  # for sweep

    args, unknown = ap.parse_known_args()

    lam = args.lam
    TARGET = dict(Vus=args.Vus, Vcb=args.Vcb, Vub=args.Vub, J=args.J)

    # baseline fixed O(1)
    au, cu, cd = au_def, cu_def, cd_def

    if args.mode == "rebuild":
        if not args.card:
            print("Need --card for rebuild"); sys.exit(0)
        lamc, Tc, kc, rc, sc = rebuild_from_card(load_card(args.card))
        print("Rebuilt CKM from card:")
        for i, rn in enumerate(["u","c","t"]):
            print(rn+" :", format_row([rc["A"][i,0], rc["A"][i,1], rc["A"][i,2]]))
        print(f"score={sc:.3e}")
        sys.exit(0)

    if args.mode == "compare":
        if not args.cards:
            print("Need --cards <list of card paths>"); sys.exit(0)
        compare_cards(args.cards); sys.exit(0)

    # start knobs
    knobs = dict(
        a_u=au, b_u=args.b_u, c_u=cu, phi_u=args.phi_u, g13=args.g13,
        a_d=args.a_d, b_d=args.b_d, c_d=cd, phi_d=args.phi_d, p32=args.p32, r12=args.r12
    )

    if args.mode == "lock":
        locked_k, r = lock_Vus(lam, TARGET, knobs, adjust_ad=args.adjust_ad)
        s = score_from_result(r, TARGET, lam)
        print("="*100); print("LOCKED |Vus|")
        print("="*100)
        print(f"r12 → {locked_k['r12']:.6f}  (a_d={locked_k['a_d']:.6f})")
        print(f"|Vus|={r['Vus']:.9f}, |Vcb|={r['Vcb']:.9f}, |Vub|={r['Vub']:.9f}, J={r['J']:.9e}")
        print(f"score={s:.3e}  unitarity={unitarity_error(r['V']):.3e}")
        # emit quick card
        RUN_TS = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")
        card = {
            "module":"UPT_TEXTURES_addons_v1","run_ts":RUN_TS+"Z",
            "inputs":{"lambda_in":lam,"targets":TARGET,"fixed":{"a_u":au,"c_u":cu,"c_d":cd}},
            "knobs":{k: float(locked_k[k]) for k in ["r12","phi_u","phi_d","b_u","p32","a_d","b_d","g13"]},
            "ckm":{"Vus":r["Vus"],"Vcb":r["Vcb"],"Vub":r["Vub"],"J":r["J"],"unitarity_err":unitarity_error(r["V"])},
            "score":s
        }
        out = emit_card(card, out_dir=args.out_dir, tag="locked")
        print("[CARD]", out)

    elif args.mode == "tune":
        tuned_k, r, s = micro_tune(lam, TARGET, knobs, iters=120)
        print("="*100); print("MICRO-TUNE RESULT"); print("="*100)
        for n in ["r12","phi_u","phi_d","b_u","p32","a_d","b_d","g13"]:
            print(f"{n:5s} = {tuned_k[n]:.6f}")
        print("\nCKM:")
        for i, rn in enumerate(["u","c","t"]):
            print(rn+" :", format_row([r["A"][i,0], r["A"][i,1], r["A"][i,2]]))
        print(f"score={s:.3e}  unitarity={unitarity_error(r['V']):.3e}")

    elif args.mode == "sweep":
        stats = batch_stability(lam, TARGET, knobs, N=args.N, out_dir=args.out_dir)
        print(json.dumps(stats, indent=2))

# =================== UPT_TEXTURES_v4n_plus2_fixed — bounded Nelder–Mead + auto restarts ===================
# Safe for Jupyter/Colab: ignores unknown CLI args, no fragile globals, prints all key info,
# and includes a |Vus|-locking helper with robust bracketing & fallbacks.

import numpy as np, cmath, math, random, json, os, sys, argparse, datetime as dt

# -------------------------- Inputs & Targets --------------------------
LAM_INPUT = 0.224299065421  # lattice λ you’ve been using
TARGET = dict(Vus=0.224298257358, Vcb=0.042199673766, Vub=0.003939999959, J=3.384301967056e-05)

# Baseline (v4i-ish) knobs you reported earlier
AU, BU0, CU, PHI_U0, G13_0 = 1.250, 1.180, 0.852, 1.995, 0.500
AD0, BD0, CD,  PHI_D0, P32_0, R12_0 = 1.377, 0.875, 1.621, 1.950, 1.800, 1.050

rng = np.random.default_rng(1234)

# -------------------------- Textures (NNI + lopsided 12_d + p32) --------------------------
def Y_u(lam,a,b,c,phi,gamma13):
    return np.array([
        [0.0,                 a*lam*cmath.exp(1j*phi),  gamma13*(lam**3)],
        [a*lam,               0.0,                      b*(lam**2)],
        [0.0,                 c*(lam**3),               1.0]
    ], dtype=complex)

def Y_d(lam,a,b,c,phi,p32,r12):
    return np.array([
        [0.0,                 (r12*a*lam)*cmath.exp(1j*phi),  0.0],
        [a*lam,               0.0,                            b*(lam**2)],
        [0.0,                 c*(lam**p32),                   1.0]
    ], dtype=complex)

def leftU_sorted(Y):
    U,s,Vh = np.linalg.svd(Y)
    idx = np.argsort(s)  # light→heavy
    return U[:,idx], s[idx]

def eval_ckm(lam, au,bu,cu,phi_u,g13, ad,bd,cd,phi_d,p32,r12):
    Yu = Y_u(lam, au,bu,cu, phi_u, g13)
    Yd = Y_d(lam, ad,bd,cd, phi_d, p32, r12)
    Uu, su = leftU_sorted(Yu)
    Ud, sd = leftU_sorted(Yd)
    V  = Uu.conj().T @ Ud
    A  = np.abs(V)
    # Jarlskog proxy (rephasing safe up to row/col permutations used here)
    J  = float(np.imag(V[0,0]*V[1,1]*np.conj(V[0,1])*np.conj(V[1,0])))
    return dict(V=V, A=A, su=su, sd=sd,
                Vus=float(A[0,1]), Vcb=float(A[1,2]), Vub=float(A[0,2]), J=abs(J))

def relerr(pred, data):
    return abs(pred-data)/(data if data!=0 else 1.0)

def unitarity_err(V):
    # max deviation of row/col orthonormality
    rows = [np.sum(V[i,:]*np.conj(V[i,:])) for i in range(3)]
    cols = [np.sum(V[:,j]*np.conj(V[:,j])) for j in range(3)]
    return float(max(max(abs(r-1.0) for r in rows), max(abs(c-1.0) for c in cols)))

def wolfenstein_from_V(V):
    lamW = abs(V[0,1])
    A    = abs(V[1,2])/(lamW**2 + 1e-18)
    z    = ( - V[0,0]*np.conj(V[0,2]) ) / ( V[1,0]*np.conj(V[1,2]) )  # ρ̄+iη̄
    return lamW, A, float(np.real(z)), float(np.imag(z))

def UT_angles_from_V(V):
    def ang180(z):
        ang = math.degrees(math.atan2(np.imag(z), np.real(z)))
        ang = (ang + 360.0) % 360.0
        return ang if ang<=180.0 else 360.0-ang
    alpha = ang180(- V[2,0]*np.conj(V[2,2]) / (V[0,0]*np.conj(V[0,2])))
    beta  = ang180(- V[1,0]*np.conj(V[1,2]) / (V[2,0]*np.conj(V[2,2])))
    gamma = ang180(- V[0,0]*np.conj(V[0,2]) / (V[1,0]*np.conj(V[1,2])))
    return alpha, beta, gamma

# score: max rel error on (Vus,Vcb,Vub, 0.5·J) + gentle pull to keep λ close to input
def score_from_result(r, lam_target=LAM_INPUT, wJ=0.5, wL=0.2):
    e_us = relerr(r["Vus"], TARGET["Vus"])
    e_cb = relerr(r["Vcb"], TARGET["Vcb"])
    e_ub = relerr(r["Vub"], TARGET["Vub"])
    e_J  = relerr(r["J"],   TARGET["J"])
    e_lam = abs(r["Vus"] - lam_target)/lam_target
    return max(e_us, e_cb, e_ub, wJ*e_J) + wL*e_lam

# -------------------------- Bounded Nelder–Mead --------------------------
PARAMS = ["r12","phi_u","phi_d","b_u","p32","a_d","b_d","g13"]
LO     = np.array([ 0.60,  0.00,  0.00,  0.60, 1.50, 0.80, 0.60, 0.20], dtype=float)
HI     = np.array([ 1.80,  2*np.pi, 2*np.pi, 1.60, 2.60, 1.90, 1.20, 0.90], dtype=float)

SEED_X = np.array([R12_0, PHI_U0, PHI_D0, BU0, P32_0, AD0, BD0, G13_0], dtype=float)
BASE_STEP = np.array([0.08, 0.25, 0.25, 0.08, 0.10, 0.06, 0.05, 0.06], dtype=float)

def clamp(x):
    return np.minimum(HI, np.maximum(LO, x))

def eval_from_x(lam, x):
    r12, phi_u, phi_d, b_u, p32, a_d, b_d, g13 = map(float, x)
    return eval_ckm(lam, AU,b_u,CU,phi_u,g13, a_d,b_d,CD,phi_d,p32,r12)

def score_from_x(lam, x):
    return score_from_result(eval_from_x(lam, x), lam_target=lam)

def nelder_mead(lam, x0, step, max_eval=2000, alpha=1.0, gamma=2.0, rho=0.5, sigma=0.5):
    N = len(x0)
    simplex = [clamp(x0.copy())]
    for i in range(N):
        xi = x0.copy(); xi[i] = xi[i] + step[i]
        simplex.append(clamp(xi))
    fvals = [score_from_x(lam, x) for x in simplex]
    evals = len(fvals)

    def sort_simplex():
        nonlocal simplex, fvals
        idx = np.argsort(fvals)
        simplex = [simplex[i] for i in idx]
        fvals   = [fvals[i]   for i in idx]

    sort_simplex()
    while evals < max_eval:
        sort_simplex()
        best, worst, second = simplex[0], simplex[-1], simplex[-2]
        fbest, fworst, fsecond = fvals[0], fvals[-1], fvals[-2]
        xbar = np.mean(simplex[:-1], axis=0)
        # reflect
        xr = clamp(xbar + alpha*(xbar - worst))
        fr = score_from_x(lam, xr); evals += 1
        if fr < fsecond and fr >= fbest:
            simplex[-1], fvals[-1] = xr, fr
            continue
        if fr < fbest:
            # expand
            xe = clamp(xbar + gamma*(xr - xbar))
            fe = score_from_x(lam, xe); evals += 1
            simplex[-1], fvals[-1] = (xe, fe) if fe < fr else (xr, fr)
            continue
        # contract
        xc = clamp(xbar + rho*(worst - xbar))
        fc = score_from_x(lam, xc); evals += 1
        if fc < fworst:
            simplex[-1], fvals[-1] = xc, fc
            continue
        # shrink
        for i in range(1, len(simplex)):
            simplex[i] = clamp(best + sigma*(simplex[i] - best))
            fvals[i] = score_from_x(lam, simplex[i]); evals += 1
            if evals >= max_eval: break
    sort_simplex()
    return simplex[0], fvals[0]

# -------------------------- |Vus|-locking helper --------------------------
# We vary r12 to match target |Vus|; if the target is out-of-bracket, we print endpoints and keep best.

def lock_Vus(lam, x, target_Vus=TARGET["Vus"], r12_lo=0.60, r12_hi=2.00, tol=5e-6, max_iter=80):
    x = x.copy()
    def f(r12):
        x[0] = float(r12)
        return eval_from_x(lam, x)["Vus"]
    lo, hi = r12_lo, r12_hi
    vlo, vhi = f(lo), f(hi)
    # monotonicity is not guaranteed globally; we still do guarded bisection by picking the closer side each step
    if min(abs(vlo-target_Vus), abs(vhi-target_Vus)) > 0.25:  # hopeless region (like your log’s 0.36–0.90)
        return x, dict(status="out_of_bracket", r12=r12_hi, Vus=vhi, Vus_lo=vlo, Vus_hi=vhi)
    best_r12, best_val, best_err = (lo, vlo, abs(vlo-target_Vus)) if abs(vlo-target_Vus) < abs(vhi-target_Vus) else (hi, vhi, abs(vhi-target_Vus))
    for _ in range(max_iter):
        mid = 0.5*(lo+hi)
        vm  = f(mid)
        err = abs(vm - target_Vus)
        if err < best_err:
            best_r12, best_val, best_err = mid, vm, err
        if err < tol: break
        # choose the side closer to target
        if abs(vlo - target_Vus) < abs(vhi - target_Vus):
            hi, vhi = mid, vm
        else:
            lo, vlo = mid, vm
    x[0] = best_r12
    return x, dict(status="locked", r12=best_r12, Vus=best_val)

# -------------------------- Reporting --------------------------

def headline_block(lam, r, label="CKM headlines"):
    eu, ec, eb, eJ = (relerr(r["Vus"],TARGET["Vus"]), relerr(r["Vcb"],TARGET["Vcb"]),
                      relerr(r["Vub"],TARGET["Vub"]), relerr(r["J"],TARGET["J"]))
    unit = unitarity_err(r["V"])
    sc   = score_from_result(r, lam_target=lam)
    lines = []
    lines += ["="*100, f"{label}", "="*100]
    for i, rn in enumerate(["u","c","t"]):
        lines.append(rn+" : " + "  ".join(f"{r['A'][i,j]:.9f}" for j in range(3)))
    lines += [
        "\nHeadlines vs targets:",
        f"  |Vus|  pred={r['Vus']:.9f}  data={TARGET['Vus']:.9f}  rel_err={eu:.3e}",
        f"  |Vcb|  pred={r['Vcb']:.9f}  data={TARGET['Vcb']:.9f}  rel_err={ec:.3e}",
        f"  |Vub|  pred={r['Vub']:.9f}  data={TARGET['Vub']:.9f}  rel_err={eb:.3e}",
        f"  J      pred={r['J']:.9e}  data={TARGET['J']:.9e}  rel_err={eJ:.3e}",
        f"[unitarity err] = {unit:.3e}",
        f"[score] = {sc:.3e}"
    ]
    return "\n".join(lines)


def wolf_ut_block(V, lam, title):
    lamW, A, rho, eta = wolfenstein_from_V(V)
    a,b,g = UT_angles_from_V(V)
    return "\n".join([
        "="*100,
        title,
        "="*100,
        f"λ (from Vus) ~ {lamW:.9f}  (input λ={lam:.12f})",
        f"A ~ {A:.6f}",
        f"ρ̄ ~ {rho:.6f},   η̄ ~ {eta:.6f}",
        f"α ~ {a:.3f}°,  β ~ {b:.3f}°,  γ ~ {g:.3f}°  |  α+β+γ ≈ {a+b+g:.3f}°",
    ])


def sensitivities_block(lam, x, delta=0.01):
    base = score_from_x(lam, x)
    deltas = {}
    for i, name in enumerate(PARAMS):
        x2 = x.copy(); x2[i] = float(min(HI[i], max(LO[i], x2[i]*(1.0+delta))))
        deltas[name] = score_from_x(lam, x2) - base
    lines = ["="*100, "SENSITIVITIES — Δscore for small bumps", "="*100]
    for k in PARAMS:
        lines.append(f"{k:5s} : Δscore≈ {deltas[k]:+.3e}")
    return "\n".join(lines)


# -------------------------- Pipeline --------------------------

def run_pipeline(lam=LAM_INPUT, out_dir="/content", restarts=12, max_eval=2200, use_plus2_seed=False, lock_Vus_to_target=False):
    os.makedirs(out_dir, exist_ok=True)
    # baseline report
    r0 = eval_ckm(lam, AU,BU0,CU,PHI_U0,G13_0, AD0,BD0,CD,PHI_D0,P32_0,R12_0)
    print(headline_block(lam, r0, label="BASELINE (v4i) — CKM headlines"))
    print()
    print(wolf_ut_block(r0["V"], lam, "WOLFENSTEIN & UT (baseline)"))
    print()

    # search space & NM restarts
    seed = SEED_X.copy() if not use_plus2_seed else np.array([2.000, 1.291, 1.253, 1.429, 1.601, 0.800, 1.180, 0.500], dtype=float)
    best_x, best_s = seed.copy(), score_from_x(lam, seed)
    tries = []
    for restart in range(restarts):
        jitter = np.array([
            (rng.random()-0.5)*0.60,   # r12 ±0.30
            (rng.random()-0.5)*1.50,   # phi_u ±0.75
            (rng.random()-0.5)*1.50,   # phi_d ±0.75
            (rng.random()-0.5)*0.40,   # b_u  ±0.20
            (rng.random()-0.5)*0.60,   # p32 ±0.30
            (rng.random()-0.5)*0.40,   # a_d  ±0.20
            (rng.random()-0.5)*0.30,   # b_d  ±0.15
            (rng.random()-0.5)*0.30,   # g13 ±0.15
        ])
        x0 = clamp(seed + jitter)
        step = BASE_STEP * (1.0 + 0.35*restart)
        x_star, s_star = nelder_mead(lam, x0, step, max_eval=max_eval)
        tries.append((s_star, x_star))
        if s_star < best_s:
            best_x, best_s = x_star, s_star

    # best of tries
    best_s, best_x = min(tries, key=lambda t: t[0]) if tries else (best_s, best_x)

    # optional |Vus| lock (adjust only r12)
    lock_info = None
    if lock_Vus_to_target:
        best_x, lock_info = lock_Vus(lam, best_x, TARGET["Vus"])  # may become out_of_bracket in extreme seeds

    # Print deltas vs seed
    print("="*100)
    print("TIGHTENED KNOBS (v4n_plus2)")
    print("="*100)
    for n,old,new in zip(PARAMS, SEED_X, best_x):
        print(f"{n:5s} = {new:8.3f}  (Δ={new-old:+.3f})")
    print(f"\na_u = {AU:.3f},  c_u = {CU:.3f},  c_d = {CD:.3f}")

    # Final headlines
    rB = eval_from_x(lam, best_x)
    print()
    print(headline_block(lam, rB, label="CKM (v4n_plus2 tightened) — headlines"))
    print()
    print(wolf_ut_block(rB["V"], lam, "WOLFENSTEIN & UT (v4n_plus2)"))
    print()

    # Stability (tight jitter)
    def jitter_tight(x, frac=0.02, dphi=0.05, dp32=0.05, dr12=0.02, dad=0.02, dbd=0.02, dg13=0.02):
        r12, phi_u, phi_d, b_u, p32, a_d, b_d, g13 = x
        def jj(v,f): return v*(1 + (random.random()-0.5)*2*f)
        def jp(p):  return p + (random.random()-0.5)*2*dphi
        return np.array([
            min(HI[0], max(LO[0], r12 + (random.random()-0.5)*2*dr12)),
            min(HI[1], max(LO[1], jp(phi_u))),
            min(HI[2], max(LO[2], jp(phi_d))),
            min(HI[3], max(LO[3], jj(b_u, frac))),
            min(HI[4], max(LO[4], p32 + (random.random()-0.5)*2*dp32)),
            min(HI[5], max(LO[5], jj(a_d, dad))),
            min(HI[6], max(LO[6], jj(b_d, dbd))),
            min(HI[7], max(LO[7], jj(g13, dg13))),
        ])

    def quick_score(x_try):
        r = eval_from_x(lam, x_try)
        return max(relerr(r["Vus"],TARGET["Vus"]), relerr(r["Vcb"],TARGET["Vcb"]),
                   relerr(r["Vub"],TARGET["Vub"]), 0.5*relerr(r["J"],TARGET["J"]))

    N=400
    errs=sorted(quick_score(jitter_tight(best_x)) for _ in range(N))
    p50,p90,p99 = errs[N//2], errs[int(0.9*N)], errs[int(0.99*N)]
    print("="*100)
    print("STABILITY (tight jitter)")
    print("="*100)
    print(f"median score   = {p50:.3e}")
    print(f"90th pct       = {p90:.3e}")
    print(f"99th pct       = {p99:.3e}")

    # Sensitivities
    print()
    print(sensitivities_block(lam, best_x, delta=0.01))

    # Artifacts
    ts = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")
    card = {
        "lam": lam,
        "best_x": {k: float(v) for k,v in zip(PARAMS, best_x)},
        "score": float(score_from_x(lam, best_x)),
        "unitarity": float(unitarity_err(rB["V"])) ,
        "ckm": {"Vus": rB["Vus"], "Vcb": rB["Vcb"], "Vub": rB["Vub"], "J": rB["J"]},
        "lock_info": lock_info,
    }
    card_path = os.path.join(out_dir, f"UPT_texture_card_v4n_plus2_{ts}.json")
    ascii_path = os.path.join(out_dir, f"reports/UPT_ascii_TEXTURES_v4n_plus2_{ts}.txt")
    md_path    = os.path.join(out_dir, f"reports/UPT_report_TEXTURES_v4n_plus2_{ts}.md")
    csv_path   = os.path.join(out_dir, f"reports/UPT_texture_ckm_v4n_plus2_{ts}.csv")
    os.makedirs(os.path.dirname(ascii_path), exist_ok=True)

    with open(card_path, "w") as f: json.dump(card, f, indent=2)
    with open(ascii_path, "w") as f:
        f.write(headline_block(lam, r0, label="BASELINE (seed) — CKM headlines")+"\n\n")
        f.write(wolf_ut_block(r0["V"], lam, "WOLFENSTEIN & UT (baseline)")+"\n\n")
        f.write("TIGHTENED KNOBS (v4n_plus2)\n"+"\n".join(f"{k:5s}={float(v):.6f}" for k,v in zip(PARAMS,best_x))+"\n\n")
        f.write(headline_block(lam, rB, label="CKM (v4n_plus2 tightened) — headlines")+"\n\n")
    with open(md_path, "w") as f:
        f.write(f"# TEXTURE v4n_plus2 — report\\n\\n- λ_input: **{lam:.12f}**\\n- timestamp: **{ts}Z**\\n\n")
        f.write("## CKM headlines (baseline)\\n\n")
        for i, rn in enumerate(["u","c","t"]):
            f.write("- "+rn+": "+"  ".join(f"{r0['A'][i,j]:.9f}" for j in range(3))+"\\n")
        f.write("\n## CKM headlines (tightened)\\n\n")
        for i, rn in enumerate(["u","c","t"]):
            f.write("- "+rn+": "+"  ".join(f"{rB['A'][i,j]:.9f}" for j in range(3))+"\\n")
    with open(csv_path, "w") as f:
        f.write("label,Vud,Vus,Vub\\n")
        f.write(f"u,{rB['A'][0,0]:.9f},{rB['A'][0,1]:.9f},{rB['A'][0,2]:.9f}\\n")
        f.write(f"c,{rB['A'][1,0]:.9f},{rB['A'][1,1]:.9f},{rB['A'][1,2]:.9f}\\n")
        f.write(f"t,{rB['A'][2,0]:.9f},{rB['A'][2,1]:.9f},{rB['A'][2,2]:.9f}\\n")

    print("="*100)
    print("EMIT ARTIFACTS")
    print("="*100)
    print("[BEGIN SECTION:write_files]")
    print(f"[KV] CARD  = {card_path}")
    print(f"[KV] ASCII = {ascii_path}")
    print(f"[KV] MD    = {md_path}")
    print(f"[KV] CSV   = {csv_path}")
    print("[END SECTION:write_files]")
    print(f"[KV] run_ts = {ts}Z")


# -------------------------- CLI (robust in notebooks) --------------------------

def _parse_cli():
    p = argparse.ArgumentParser(add_help=True)
    p.add_argument('--seed', type=int, default=1234)
    p.add_argument('--restarts', type=int, default=12)
    p.add_argument('--max_eval', type=int, default=2200)
    p.add_argument('--jitter_scale', type=float, default=1.0)
    p.add_argument('--out_dir', type=str, default='/content')
    p.add_argument('--lam', type=float, default=LAM_INPUT)
    p.add_argument('--mode', type=str, default='search', choices=['search','lock'])
    p.add_argument('--use_plus2_seed', action='store_true')
    args, _ = p.parse_known_args()  # ignore Jupyter’s -f ...
    return args

if __name__ == "__main__":
    args = _parse_cli()
    rng = np.random.default_rng(args.seed)
    run_pipeline(lam=args.lam,
                 out_dir=args.out_dir,
                 restarts=args.restarts,
                 max_eval=args.max_eval,
                 use_plus2_seed=args.use_plus2_seed,
                 lock_Vus_to_target=(args.mode=='lock'))

# =================== UPT_TEXTURES_v4n_plus2_REPORT — print-only module (no CLI) ===================
import numpy as np, cmath, math, random, datetime as dt

# ----- Lattice lambda & experiment targets -----
LAM_INPUT = 0.224299065421
TARGET = dict(Vus=0.224298257358, Vcb=0.042199673766, Vub=0.003939999959, J=3.384301967056e-05)

# ----- Best-fit knobs (v4n_plus2 tightened) -----
# final (to tune): r12, phi_u, phi_d, b_u, p32, a_d, b_d, g13
KNOBS = dict(
    r12 = 1.078,
    phi_u = 2.297,
    phi_d = 2.282,
    b_u = 1.275,
    p32 = 1.763,
    a_d = 1.241,
    b_d = 0.970,
    g13 = 0.470,
)
# fixed from v4i
FIXED = dict(a_u=1.250, c_u=0.852, c_d=1.621)

# ===== Texture builders (NNI + lopsided 12_d + continuous p32) =====
def Y_u(lam, a, b, c, phi, g13):
    return np.array([
        [0.0,                 a*lam*cmath.exp(1j*phi),  g13*(lam**3)],
        [a*lam,               0.0,                      b*(lam**2)],
        [0.0,                 c*(lam**3),               1.0]
    ], dtype=complex)

def Y_d(lam, a, b, c, phi, p32, r12):
    return np.array([
        [0.0,                 (r12*a*lam)*cmath.exp(1j*phi),  0.0],
        [a*lam,               0.0,                             b*(lam**2)],
        [0.0,                 c*(lam**p32),                    1.0]
    ], dtype=complex)

def leftU_sorted(Y):
    U, s, Vh = np.linalg.svd(Y)
    idx = np.argsort(s)  # light→heavy
    return U[:, idx], s[idx]

def eval_ckm(lam, K=KNOBS, F=FIXED):
    Yu = Y_u(lam, F["a_u"], K["b_u"], F["c_u"], K["phi_u"], K["g13"])
    Yd = Y_d(lam, K["a_d"], K["b_d"], F["c_d"], K["phi_d"], K["p32"], K["r12"])
    Uu, su = leftU_sorted(Yu)
    Ud, sd = leftU_sorted(Yd)
    V  = Uu.conj().T @ Ud
    A  = np.abs(V)
    J  = float(np.imag(V[0,0]*V[1,1]*np.conj(V[0,1])*np.conj(V[1,0])))
    uni = float(np.linalg.norm(V.conj().T @ V - np.eye(3)))
    return dict(V=V, A=A, su=su, sd=sd,
                Vus=float(A[0,1]), Vcb=float(A[1,2]), Vub=float(A[0,2]),
                J=abs(J), unitarity=uni)

def relerr(pred, data):
    return abs(pred - data)/(data if data != 0 else 1.0)

def score_from_result(r):
    # max relative error over |Vus|, |Vcb|, |Vub|, and 0.5*J (to de-weight J a bit)
    return max(
        relerr(r["Vus"], TARGET["Vus"]),
        relerr(r["Vcb"], TARGET["Vcb"]),
        relerr(r["Vub"], TARGET["Vub"]),
        0.5*relerr(r["J"], TARGET["J"])
    )

def wolfenstein_from_V(V):
    lamW = abs(V[0,1])
    A    = abs(V[1,2])/(lamW**2 + 1e-18)
    z    = ( - V[0,0]*np.conj(V[0,2]) ) / ( V[1,0]*np.conj(V[1,2]) )  # ρ̄ + i η̄
    return lamW, A, float(np.real(z)), float(np.imag(z))

def UT_angles_from_V(V):
    def ang180(z):
        ang = math.degrees(math.atan2(np.imag(z), np.real(z)))
        ang = (ang + 360.0) % 360.0
        return ang if ang <= 180.0 else 360.0 - ang
    alpha = ang180(- V[2,0]*np.conj(V[2,2]) / (V[0,0]*np.conj(V[0,2])))
    beta  = ang180(- V[1,0]*np.conj(V[1,2]) / (V[2,0]*np.conj(V[2,2])))
    gamma = ang180(- V[0,0]*np.conj(V[0,2]) / (V[1,0]*np.conj(V[1,2])))
    return float(alpha), float(beta), float(gamma)

def _print_table_row(label, vals, fmt="{: .9f}"):
    print(label, ":", "  ".join(fmt.format(float(v)).strip() for v in vals))

# ===== Stability (tight jitter) and sensitivity helpers =====
_LO = dict(r12=0.60, phi_u=0.00, phi_d=0.00, b_u=0.60, p32=1.50, a_d=0.80, b_d=0.60, g13=0.20)
_HI = dict(r12=1.80, phi_u=2*np.pi, phi_d=2*np.pi, b_u=1.60, p32=2.60, a_d=1.90, b_d=1.20, g13=0.90)

def _clamp_knobs(K):
    KK = {}
    for k,v in K.items():
        lo, hi = _LO[k], _HI[k]
        KK[k] = min(hi, max(lo, float(v)))
    return KK

def jitter_around(bestK, frac=0.02, dphi=0.05, dp32=0.05, dr12=0.02, dad=0.02, dbd=0.02, dg13=0.02, dbu=0.02):
    K = bestK.copy()
    def jj(val, f):
        return val*(1 + (random.random()-0.5)*2*f)
    def jp(ph):
        return ph + (random.random()-0.5)*2*dphi
    K["r12"]   = min(_HI["r12"], max(_LO["r12"], bestK["r12"] + (random.random()-0.5)*2*dr12))
    K["phi_u"] = min(_HI["phi_u"], max(_LO["phi_u"], jp(bestK["phi_u"])))
    K["phi_d"] = min(_HI["phi_d"], max(_LO["phi_d"], jp(bestK["phi_d"])))
    K["b_u"]   = min(_HI["b_u"],   max(_LO["b_u"],   jj(bestK["b_u"], dbu)))
    K["p32"]   = min(_HI["p32"],   max(_LO["p32"],   bestK["p32"] + (random.random()-0.5)*2*dp32))
    K["a_d"]   = min(_HI["a_d"],   max(_LO["a_d"],   jj(bestK["a_d"], dad)))
    K["b_d"]   = min(_HI["b_d"],   max(_LO["b_d"],   jj(bestK["b_d"], dbd)))
    K["g13"]   = min(_HI["g13"],   max(_LO["g13"],   jj(bestK["g13"], dg13)))
    return _clamp_knobs(K)

def quick_score(K):
    r = eval_ckm(LAM_INPUT, K, FIXED)
    return max(
        relerr(r["Vus"], TARGET["Vus"]),
        relerr(r["Vcb"], TARGET["Vcb"]),
        relerr(r["Vub"], TARGET["Vub"]),
        0.5*relerr(r["J"], TARGET["J"])
    )

def sensitivity_scan(bestK, bump=0.01, dphi=0.01):
    sens = {}
    base = quick_score(bestK)
    for k in ["r12","phi_u","phi_d","b_u","p32","a_d","b_d","g13"]:
        K = bestK.copy()
        if k in ["phi_u","phi_d"]:
            K[k] = min(_HI[k], max(_LO[k], bestK[k] + dphi))
        elif k in ["p32","r12"]:
            K[k] = min(_HI[k], max(_LO[k], bestK[k] + bump))
        else:
            K[k] = min(_HI[k], max(_LO[k], bestK[k]*(1.0 + bump)))
        sens[k] = quick_score(K) - base
    return sens

# ===== Main print routine =====
def print_texture_report():
    # compute
    knobs = _clamp_knobs(KNOBS)
    res = eval_ckm(LAM_INPUT, knobs, FIXED)
    score = score_from_result(res)
    lamW, A, rhoB, etaB = wolfenstein_from_V(res["V"])
    aA, bB, gG = UT_angles_from_V(res["V"])

    # 1) Headlines
    print("="*100)
    print("CKM TEXTURE — FINAL SNAPSHOT (v4n_plus2 tightened)")
    print("="*100, "\n")
    print("[HEADLINES: targets vs predictions]")
    print("quantity      predicted        target           rel_error")
    print("---------------------------------------------------------")
    print(f"|Vus|         {res['Vus']:.9f}     {TARGET['Vus']:.9f}      {relerr(res['Vus'], TARGET['Vus']):.3e}")
    print(f"|Vcb|         {res['Vcb']:.9f}     {TARGET['Vcb']:.9f}      {relerr(res['Vcb'], TARGET['Vcb']):.3e}")
    print(f"|Vub|         {res['Vub']:.9f}     {TARGET['Vub']:.9f}      {relerr(res['Vub'], TARGET['Vub']):.3e}")
    print(f"J             {res['J']:.9e} {TARGET['J']:.9e}  {relerr(res['J'], TARGET['J']):.3e}\n")

    # 2) CKM matrix magnitudes
    print("[CKM MAGNITUDES  | rows: u,c,t  cols: d,s,b ]")
    print("          d            s            b")
    _print_table_row("u", res["A"][0])
    _print_table_row("c", res["A"][1])
    _print_table_row("t", res["A"][2])
    print("\n[UNITARITY & SCORE]")
    print(f"unitarity error (||V†V−I||_F): {res['unitarity']:.3e}")
    print(f"score (max rel error on {{Vus,Vcb,Vub,0.5*J}}): {score:.3e}\n")

    # 3) Wolfenstein & unitarity triangle
    print("[WOLFENSTEIN & UNITARITY TRIANGLE]")
    print(f"λ(from |Vus|)  : {lamW:.9f}   (input λ={LAM_INPUT:.12f})")
    print(f"A              : {A:.6f}")
    print(f"ρ̄             : {rhoB:.6f}")
    print(f"η̄             : {etaB:.6f}")
    print(f"α              : {aA:.3f}°")
    print(f"β              : {bB:.3f}°")
    print(f"γ              : {gG:.3f}°")
    print(f"(α+β+γ ≈ {aA+bB+gG:.3f}°)\n")

    # 4) Knobs
    print("[KNOBS — FINAL VALUES]")
    print(f"r12   = {knobs['r12']:.3f}      (LEFT-lopsided factor for (12)_d)")
    print(f"φ_u   = {knobs['phi_u']:.3f} rad  (up-sector phase)")
    print(f"φ_d   = {knobs['phi_d']:.3f} rad  (down-sector phase)")
    print(f"b_u   = {knobs['b_u']:.3f}      (controls (23)_u ~ b_u * λ^2)")
    print(f"p32   = {knobs['p32']:.3f}      (continuous power on (32)_d: λ^p32)")
    print(f"a_d   = {knobs['a_d']:.3f}      (controls (12)_d ~ r12 * a_d * λ)")
    print(f"b_d   = {knobs['b_d']:.3f}      (controls (23)_d ~ b_d * λ^2)")
    print(f"g13   = {knobs['g13']:.3f}      (tiny (13)_u term ~ g13 * λ^3)")
    print("\n[FIXED (held from v4i)]")
    print(f"a_u = {FIXED['a_u']:.3f}   c_u = {FIXED['c_u']:.3f}   c_d = {FIXED['c_d']:.3f}\n")

    # 5) Stability (tight jitter)
    N = 400
    errs = []
    for _ in range(N):
        Ktry = jitter_around(knobs)
        errs.append(quick_score(Ktry))
    errs.sort()
    p50 = errs[N//2]
    p90 = errs[int(0.9*N)]
    p99 = errs[int(0.99*N)]
    print("[STABILITY — tight jitter around best knobs]")
    print("percentile    score")
    print("--------------------")
    print(f"median        {p50:.3e}")
    print(f"90th          {p90:.3e}")
    print(f"99th          {p99:.3e}\n")

    # 6) Sensitivities (small positive bumps)
    sens = sensitivity_scan(knobs, bump=0.01, dphi=0.01)
    order = sorted(sens.items(), key=lambda kv: -abs(kv[1]))
    print("[SENSITIVITY — Δscore for small + bumps (approx)]")
    print("knob   Δscore")
    print("---------------")
    for k, dv in order:
        print(f"{k:5s}  {dv:+.3e}")
    print("\n[KV] run_ts =", dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%SZ"))
    print("="*100)

# --- run once when appended ---
print_texture_report()
# =================== end UPT_TEXTURES_v4n_plus2_REPORT ============================================

# =================== UPT_PMNS_v1 — λ-power flavored PMNS fit + ASCII report ===================
import numpy as np, math, cmath, random, datetime as dt

# -----------------------------
# Targets (NuFIT-like, Normal Ordering; tweak if you have a house set)
# -----------------------------
DEG  = math.pi/180.0
PMNS_TARGET = dict(
    th12 = 33.44*DEG,   # θ12
    th23 = 49.20*DEG,   # θ23
    th13 =  8.57*DEG,   # θ13
    delta = 197.0*DEG   # Dirac phase (wrap-aware fit)
)

# convenience: compute the target J_CP for info (not used directly as a target)
def _J_from_angles(th12, th23, th13, delta):
    s12,s23,s13 = math.sin(th12), math.sin(th23), math.sin(th13)
    c12,c23,c13 = math.cos(th12), math.cos(th23), math.cos(th13)
    return 0.5**3 * (2*s12*c12)*(2*s23*c23)*(2*s13*c13)*c13*math.sin(delta)
PMNS_TARGET_J = _J_from_angles(**PMNS_TARGET)  # info only

# -----------------------------
# Wolfenstein-style λ (reuse the quark lattice step size)
# -----------------------------
LAM = 0.224299065421

# -----------------------------
# Building blocks
# -----------------------------
def R12(theta, phase=0.0):
    c,s = math.cos(theta), math.sin(theta)
    eip = cmath.exp(1j*phase)
    # phase sits in (12) complex rotation if desired
    return np.array([[ c,  s*eip, 0.0],
                     [-s*np.conj(eip),  c,  0.0],
                     [ 0.0, 0.0, 1.0]], dtype=complex)

def R23(theta, phase=0.0):
    c,s = math.cos(theta), math.sin(theta)
    eip = cmath.exp(1j*phase)
    return np.array([[1.0, 0.0, 0.0],
                     [0.0,   c,  s*eip],
                     [0.0, -s*np.conj(eip),  c]], dtype=complex)

def R13(theta, phase=0.0):
    c,s = math.cos(theta), math.sin(theta)
    eip = cmath.exp(1j*phase)
    return np.array([[  c, 0.0,  s*eip],
                     [0.0, 1.0, 0.0],
                     [-s*np.conj(eip),0.0,  c]], dtype=complex)

# charged-lepton left mixing: small λ-power rotations (U_e acts as dagger on left)
# exponents are fixed in the “FN spirit”: 12~λ^1, 23~λ^2, 13~λ^3
def U_e_left(a12, a23, a13, phi12, phi23, phi13):
    th12 = a12 * (LAM**1)
    th23 = a23 * (LAM**2)
    th13 = a13 * (LAM**3)
    # left mixing matrix (we will take dagger)
    Ue = R23(th23, phi23) @ R13(th13, phi13) @ R12(th12, phi12)
    return Ue

# neutrino mixing: CKM-like with O(1) angles, λ-suppressed 13 if desired
# allow continuous exponent p13 to decouple θ13 control
def U_nu(a12, a23, a13, p13, delta_nu):
    th12 = a12                       # O(1) in radians
    th23 = a23                       # O(1)
    th13 = a13 * (LAM**p13)          # small, controlled by exponent
    # PDG-like: Uν = R23 * R13(δ) * R12
    return R23(th23, 0.0) @ R13(th13, delta_nu) @ R12(th12, 0.0)

def pmns_from_knobs(K):
    # K: dict of knobs; see KNOBS_SEED and bounds below
    Ue  = U_e_left(K["ae12"], K["ae23"], K["ae13"], K["pe12"], K["pe23"], K["pe13"])
    Unu = U_nu(K["an12"], K["an23"], K["an13"], K["p13nu"], K["dnu"])
    U   = Ue.conj().T @ Unu  # U_PMNS = U_e^\dagger U_ν
    return U

# extract (θ12, θ23, θ13, δ) and J from a PMNS matrix (phase-convention neutral)
def angles_from_U(U):
    A = np.abs(U)
    s13 = float(A[0,2])
    s13 = min(max(s13, 0.0), 1.0)
    th13 = math.asin(s13)
    c13  = math.cos(th13)
    th12 = math.atan2(float(A[0,1]), float(A[0,0]))  # consistent with PDG when c13>0
    th23 = math.atan2(float(A[1,2]), float(A[2,2]))
    # Jarlskog (rephasing invariant)
    J = float(np.imag(U[0,0]*U[1,1]*np.conj(U[0,1])*np.conj(U[1,0])))
    # infer δ via PDG relation (careful with arcsin)
    s12,s23 = math.sin(th12), math.sin(th23)
    c12,c23 = math.cos(th12), math.cos(th23)
    denom = (c12*s12*c23*s23*(c13**2)*s13)
    if abs(denom) < 1e-16:
        delta = 0.0
    else:
        x = max(-1.0, min(1.0, J/denom))
        delta = math.asin(x)  # principal value
        # try to lift to [0,2π) using sign of cosδ from matrix elements (simple proxy)
        # proxy via invariant: Re(Ue1 Uμ3 Ue3* Uμ1*) ~ c12 c23 s13 c13^2 s12 s23 cosδ
        proxy = np.real(U[0,0]*U[1,2]*np.conj(U[0,2])*np.conj(U[1,0]))
        if proxy < 0:
            # cosδ < 0 → put δ in (π/2, 3π/2)
            if delta >= 0: delta = math.pi - delta
            else:          delta = -math.pi - delta
        delta = (delta + 2*math.pi) % (2*math.pi)
    return th12, th23, th13, delta, J

# circular error for δ (wrap-aware, relative to target magnitude)
def delta_err(pred, targ):
    # compute minimal angle difference on the circle
    d = abs(((pred - targ + math.pi) % (2*math.pi)) - math.pi)
    # use relative error vs |targ| if not too small; else scale by π
    scale = abs(targ) if abs(targ) > 1e-6 else math.pi
    return d/scale

# score: max of relative errors on the three angles and 0.5*relative error on δ
def score_pmns(th12, th23, th13, delta):
    e12 = abs(th12 - PMNS_TARGET["th12"])/PMNS_TARGET["th12"]
    e23 = abs(th23 - PMNS_TARGET["th23"])/PMNS_TARGET["th23"]
    e13 = abs(th13 - PMNS_TARGET["th13"])/PMNS_TARGET["th13"]
    ed  = delta_err(delta, PMNS_TARGET["delta"])
    return max(e12, e23, e13, 0.5*ed)

# -----------------------------
# Knobs, bounds, seed
# -----------------------------
# Charged-lepton tiny angles: ae12,ae23,ae13 (dimensionless O(1)), phases pe12,pe23,pe13
# Neutrino: an12,an23 in radians O(1), an13 O(1), p13nu in [1.0, 3.0], dnu in [0,2π]
KNOBS_SEED = dict(
    ae12= 0.8,  ae23= 1.2,  ae13= 0.8,
    pe12= 0.0,  pe23= 0.0,  pe13= 0.0,
    an12= 34.0*DEG, an23= 48.0*DEG, an13= 0.7, p13nu= 1.0, dnu= 1.1*math.pi
)

_BOUNDS = dict(
    ae12=(0.0, 2.0), ae23=(0.0, 2.0), ae13=(0.0, 2.0),
    pe12=(0.0, 2*math.pi), pe23=(0.0, 2*math.pi), pe13=(0.0, 2*math.pi),
    an12=(25*DEG, 40*DEG), an23=(38*DEG, 55*DEG), an13=(0.2, 1.2),
    p13nu=(0.5, 3.0), dnu=(0.0, 2*math.pi)
)

def _clamp(K):
    out = {}
    for k,v in K.items():
        lo,hi = _BOUNDS[k]
        out[k] = float(min(hi, max(lo, v)))
    return out

# -----------------------------
# Bounded Nelder–Mead (like in CKM module)
# -----------------------------
def _vec(K):
    return np.array([K[k] for k in KNOBS_SEED.keys()], dtype=float)

def _unvec(x):
    return {k: float(v) for k,v in zip(KNOBS_SEED.keys(), x)}

def _f_of_x(x):
    K = _clamp(_unvec(x))
    U = pmns_from_knobs(K)
    th12,th23,th13,delta,J = angles_from_U(U)
    return score_pmns(th12,th23,th13,delta)

def nelder_mead_bounded(x0, step, max_eval=2500, alpha=1.0, gamma=2.0, rho=0.5, sigma=0.5):
    N = len(x0)
    def clampx(x):
        K = _clamp(_unvec(x))
        return _vec(K)
    simp = [clampx(x0)]
    for i in range(N):
        xi = x0.copy(); xi[i] += step[i]; simp.append(clampx(xi))
    fvals = [ _f_of_x(s) for s in simp ]
    evals = len(fvals)

    def sortit():
        nonlocal simp, fvals
        idx = np.argsort(fvals)
        simp  = [simp[i]  for i in idx]
        fvals = [fvals[i] for i in idx]

    sortit()
    while evals < max_eval:
        sortit()
        best, worst, second = simp[0], simp[-1], simp[-2]
        fbest, fworst, fsecond = fvals[0], fvals[-1], fvals[-2]
        xbar = np.mean(simp[:-1], axis=0)
        # reflect
        xr = clampx(xbar + alpha*(xbar - worst))
        fr = _f_of_x(xr); evals += 1
        if fbest <= fr < fsecond:
            simp[-1], fvals[-1] = xr, fr; continue
        if fr < fbest:
            # expand
            xe = clampx(xbar + gamma*(xr - xbar))
            fe = _f_of_x(xe); evals += 1
            if fe < fr: simp[-1], fvals[-1] = xe, fe
            else:       simp[-1], fvals[-1] = xr, fr
            continue
        # contract
        xc = clampx(xbar + rho*(worst - xbar))
        fc = _f_of_x(xc); evals += 1
        if fc < fworst:
            simp[-1], fvals[-1] = xc, fc; continue
        # shrink
        for i in range(1, len(simp)):
            simp[i] = clampx(simp[0] + sigma*(simp[i] - simp[0]))
            fvals[i] = _f_of_x(simp[i]); evals += 1
            if evals >= max_eval: break

    sortit()
    return simp[0], fvals[0]

# -----------------------------
# Tight jitter & sensitivities (like CKM)
# -----------------------------
def jitter_tight(K, frac=0.02, dphi=0.05, dp=0.05):
    J = dict(K)
    def jj(val,f): return val*(1 + (random.random()-0.5)*2*f)
    def jp(ph):   return ph + (random.random()-0.5)*2*dphi
    J["ae12"] = min(_BOUNDS["ae12"][1], max(_BOUNDS["ae12"][0], jj(K["ae12"], frac)))
    J["ae23"] = min(_BOUNDS["ae23"][1], max(_BOUNDS["ae23"][0], jj(K["ae23"], frac)))
    J["ae13"] = min(_BOUNDS["ae13"][1], max(_BOUNDS["ae13"][0], jj(K["ae13"], frac)))
    J["pe12"] = (K["pe12"] + (random.random()-0.5)*2*dphi) % (2*math.pi)
    J["pe23"] = (K["pe23"] + (random.random()-0.5)*2*dphi) % (2*math.pi)
    J["pe13"] = (K["pe13"] + (random.random()-0.5)*2*dphi) % (2*math.pi)
    J["an12"] = min(_BOUNDS["an12"][1], max(_BOUNDS["an12"][0], jj(K["an12"], frac)))
    J["an23"] = min(_BOUNDS["an23"][1], max(_BOUNDS["an23"][0], jj(K["an23"], frac)))
    J["an13"] = min(_BOUNDS["an13"][1], max(_BOUNDS["an13"][0], jj(K["an13"], frac)))
    J["p13nu"] = min(_BOUNDS["p13nu"][1], max(_BOUNDS["p13nu"][0], K["p13nu"] + (random.random()-0.5)*2*dp))
    J["dnu"] = (K["dnu"] + (random.random()-0.5)*2*dphi) % (2*math.pi)
    return _clamp(J)

def quick_score(K):
    U = pmns_from_knobs(K)
    th12,th23,th13,delta,J = angles_from_U(U)
    return score_pmns(th12,th23,th13,delta)

def sensitivity_scan(Kbest, bump=0.01, dphi=0.01, dp=0.01):
    base = quick_score(Kbest)
    sens = {}
    for k in KNOBS_SEED.keys():
        K = dict(Kbest)
        lo,hi = _BOUNDS[k]
        if k.startswith("p"):   # exponent / delta
            K[k] = min(hi, max(lo, K[k] + (dp if "13" in k else dphi)))
        elif k.startswith("pe"):
            K[k] = (K[k] + dphi) % (2*math.pi)
        else:                   # amplitude-like
            K[k] = min(hi, max(lo, K[k]*(1.0 + bump)))
        sens[k] = quick_score(K) - base
    return sens

# -----------------------------
# Run: auto restarts then report
# -----------------------------
def run_pmns(max_eval=2500, restarts=10, seed=KNOBS_SEED):
    rng = np.random.default_rng(42)
    x0   = _vec(seed)
    step = np.array([0.08,0.08,0.08, 0.2,0.2,0.2, 0.10,0.10,0.08, 0.20,0.20], dtype=float)
    best_x, best_s = x0.copy(), _f_of_x(x0)
    tries = []
    for r in range(restarts):
        jitter = np.array([
            (rng.random()-0.5)*0.30, (rng.random()-0.5)*0.30, (rng.random()-0.5)*0.30,  # ae*
            (rng.random()-0.5)*1.00, (rng.random()-0.5)*1.00, (rng.random()-0.5)*1.00,  # pe*
            (rng.random()-0.5)*5*DEG, (rng.random()-0.5)*6*DEG, (rng.random()-0.5)*0.40, # an12, an23, an13
            (rng.random()-0.5)*0.50, (rng.random()-0.5)*1.00,                             # p13nu, dnu
        ])
        xi = _clamp(_unvec(x0 + jitter))
        xf, sf = nelder_mead_bounded(_vec(xi), step*(1.0+0.25*r), max_eval=max_eval)
        tries.append((sf, xf))
        if sf < best_s: best_x, best_s = xf, sf
    best_s, best_x = min(tries, key=lambda t: t[0]) if tries else (best_s, best_x)
    Kbest = _clamp(_unvec(best_x))
    U = pmns_from_knobs(Kbest)
    th12,th23,th13,delta,J = angles_from_U(U)
    uni = float(np.linalg.norm(U.conj().T @ U - np.eye(3)))
    score = score_pmns(th12,th23,th13,delta)

    # ----- print report -----
    A = np.abs(U)
    print("="*100)
    print("PMNS TEXTURE — FINAL SNAPSHOT (λ-power flavored)")
    print("="*100, "\n")

    # headlines vs targets
    def _re(val): return f"{val:.3e}"
    print("[HEADLINES: targets vs predictions]")
    print("quantity      predicted        target           rel_error")
    print("---------------------------------------------------------")
    print(f"θ12           {th12/DEG:11.6f}°   {PMNS_TARGET['th12']/DEG:11.6f}°   {_re(abs(th12-PMNS_TARGET['th12'])/PMNS_TARGET['th12'])}")
    print(f"θ23           {th23/DEG:11.6f}°   {PMNS_TARGET['th23']/DEG:11.6f}°   {_re(abs(th23-PMNS_TARGET['th23'])/PMNS_TARGET['th23'])}")
    print(f"θ13           {th13/DEG:11.6f}°   {PMNS_TARGET['th13']/DEG:11.6f}°   {_re(abs(th13-PMNS_TARGET['th13'])/PMNS_TARGET['th13'])}")
    print(f"δ_CP          {delta/DEG:11.6f}°   {PMNS_TARGET['delta']/DEG:11.6f}°   {_re(delta_err(delta, PMNS_TARGET['delta']))}")
    print(f"J_CP          {J: .9e}   (target est ~ {_J_from_angles(**PMNS_TARGET):.9e})\n")

    # |U_PMNS|
    print("[|U_PMNS|  | rows: e, μ, τ  cols: 1,2,3 ]")
    print("          1            2            3")
    print("e :", "  ".join(f"{A[0,j]:.9f}" for j in range(3)))
    print("μ :", "  ".join(f"{A[1,j]:.9f}" for j in range(3)))
    print("τ :", "  ".join(f"{A[2,j]:.9f}" for j in range(3)))
    print("\n[UNITARITY & SCORE]")
    print(f"unitarity error (||U†U−I||_F): {uni:.3e}")
    print(f"score (max rel error on {{θ12,θ23,θ13, 0.5·δ}}): {score:.3e}\n")

    # knobs
    print("[KNOBS — FINAL VALUES]")
    for k in KNOBS_SEED.keys():
        val = Kbest[k]
        if k.startswith("p") or k.startswith("a") or k.startswith("an"):
            print(f"{k:5s} = {val:.6f}")
        else:
            if k.startswith("pe") or k=="dnu":
                print(f"{k:5s} = {val:.6f} rad")
            else:
                print(f"{k:5s} = {val:.6f}")
    print()

    # stability
    N=300
    errs=[]
    for _ in range(N):
        Kt = jitter_tight(Kbest)
        Ux = pmns_from_knobs(Kt)
        t12,t23,t13,dlt,_J = angles_from_U(Ux)
        errs.append(score_pmns(t12,t23,t13,dlt))
    errs.sort()
    p50, p90, p99 = errs[N//2], errs[int(0.9*N)], errs[int(0.99*N)]
    print("[STABILITY — tight jitter]")
    print("percentile    score")
    print("--------------------")
    print(f"median        {p50:.3e}")
    print(f"90th          {p90:.3e}")
    print(f"99th          {p99:.3e}\n")

    # sensitivities
    sens = sensitivity_scan(Kbest)
    order = sorted(sens.items(), key=lambda kv: -abs(kv[1]))
    print("[SENSITIVITY — Δscore for small + bumps (approx)]")
    print("knob   Δscore")
    print("---------------")
    for k,dv in order:
        print(f"{k:5s}  {dv:+.3e}")
    print("\n[KV] run_ts =", dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%SZ"))
    print("="*100)

# run once
run_pmns()
# =================== end UPT_PMNS_v1 ============================================================

# =================== UPT_PMNS_emit_v1 — re-run, print, and write artifacts ===================
import os, json, csv, datetime as dt
import numpy as np

def _pmns_timestamp():
    return dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")

def _pmns_report_text(A, th12, th23, th13, delta, J, score, uni, Kbest, DEG, target_dict):
    lines = []
    lines.append("="*100)
    lines.append("PMNS TEXTURE — FINAL SNAPSHOT (λ-power flavored)")
    lines.append("="*100 + " \n")
    lines.append("[HEADLINES: targets vs predictions]")
    lines.append("quantity      predicted        target           rel_error")
    lines.append("---------------------------------------------------------")
    def re(x): return f"{x:.3e}"
    lines.append(f"θ12           {th12/DEG:11.6f}°   {target_dict['th12']/DEG:11.6f}°   {re(abs(th12-target_dict['th12'])/target_dict['th12'])}")
    lines.append(f"θ23           {th23/DEG:11.6f}°   {target_dict['th23']/DEG:11.6f}°   {re(abs(th23-target_dict['th23'])/target_dict['th23'])}")
    lines.append(f"θ13           {th13/DEG:11.6f}°   {target_dict['th13']/DEG:11.6f}°   {re(abs(th13-target_dict['th13'])/target_dict['th13'])}")
    # wrap-aware delta error (same definition as run)
    def delta_err(pred, targ):
        d = abs(((pred - targ + np.pi) % (2*np.pi)) - np.pi)
        scale = abs(targ) if abs(targ) > 1e-6 else np.pi
        return d/scale
    lines.append(f"δ_CP          {delta/DEG:11.6f}°   {target_dict['delta']/DEG:11.6f}°   {re(delta_err(delta, target_dict['delta']))}")
    # target J for info
    def J_from_angles(th12, th23, th13, delta):
        s12,s23,s13 = np.sin(th12), np.sin(th23), np.sin(th13)
        c12,c23,c13 = np.cos(th12), np.cos(th23), np.cos(th13)
        return 0.5**3 * (2*s12*c12)*(2*s23*c23)*(2*s13*c13)*c13*np.sin(delta)
    lines.append(f"J_CP          {J: .9e}   (target est ~ {J_from_angles(target_dict['th12'],target_dict['th23'],target_dict['th13'],target_dict['delta']):.9e})\n")
    lines.append("[|U_PMNS|  | rows: e, μ, τ  cols: 1,2,3 ]")
    lines.append("          1            2            3")
    lines.append("e : " + "  ".join(f"{float(A[0,j]):.9f}" for j in range(3)))
    lines.append("μ : " + "  ".join(f"{float(A[1,j]):.9f}" for j in range(3)))
    lines.append("τ : " + "  ".join(f"{float(A[2,j]):.9f}" for j in range(3)))
    lines.append("\n[UNITARITY & SCORE]")
    lines.append(f"unitarity error (||U†U−I||_F): {uni:.3e}")
    lines.append(f"score (max rel error on {{θ12,θ23,θ13, 0.5·δ}}): {score:.3e}\n")
    lines.append("[KNOBS — FINAL VALUES]")
    for k in Kbest:
        val = float(Kbest[k])
        if k in ("pe12","pe23","pe13","dnu"):
            lines.append(f"{k:5s} = {val:.6f} rad")
        else:
            lines.append(f"{k:5s} = {val:.6f}")
    lines.append("\n[KV] run_ts = " + dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%SZ"))
    lines.append("="*100)
    return "\n".join(lines)

def run_pmns_and_emit(out_dir="/content/reports"):
    # reuse objects/functions from UPT_PMNS_v1 cell
    # 1) re-optimize to get Kbest & U
    best_x_dummy, best_s_dummy = nelder_mead_bounded(_vec(KNOBS_SEED), np.array([0.08,0.08,0.08,0.2,0.2,0.2,0.10,0.10,0.08,0.20,0.20],float), max_eval=1)
    # call the real runner (prints too); then rebuild cleanly for artifacts
    rng = np.random.default_rng(42)
    x0   = _vec(KNOBS_SEED)
    step = np.array([0.08,0.08,0.08, 0.2,0.2,0.2, 0.10,0.10,0.08, 0.20,0.20], dtype=float)
    best_x, best_s = x0.copy(), _f_of_x(x0)
    tries = []
    for r in range(8):
        jitter = np.array([
            (rng.random()-0.5)*0.30, (rng.random()-0.5)*0.30, (rng.random()-0.30)*0.30,
            (rng.random()-0.5)*1.00, (rng.random()-0.5)*1.00, (rng.random()-0.5)*1.00,
            (rng.random()-0.5)*5*DEG, (rng.random()-0.5)*6*DEG, (rng.random()-0.5)*0.40,
            (rng.random()-0.5)*0.50, (rng.random()-0.5)*1.00,
        ])
        xi = _clamp(_unvec(x0 + jitter))
        xf, sf = nelder_mead_bounded(_vec(xi), step*(1.0+0.25*r), max_eval=1800)
        tries.append((sf, xf))
        if sf < best_s: best_x, best_s = xf, sf
    best_s, best_x = min(tries, key=lambda t: t[0]) if tries else (best_s, best_x)
    Kbest = _clamp(_unvec(best_x))
    U = pmns_from_knobs(Kbest)
    A = np.abs(U)
    th12,th23,th13,delta,J = angles_from_U(U)
    uni = float(np.linalg.norm(U.conj().T @ U - np.eye(3)))
    score = score_pmns(th12,th23,th13,delta)

    # 2) print human-friendly report (mirrors the on-screen one)
    print(_pmns_report_text(A, th12, th23, th13, delta, J, score, uni, Kbest, math.pi/180.0, PMNS_TARGET))

    # 3) write artifacts
    os.makedirs(out_dir, exist_ok=True)
    ts = _pmns_timestamp()
    ascii_path = os.path.join(out_dir, f"UPT_ascii_PMNS_{ts}.txt")
    md_path    = os.path.join(out_dir, f"UPT_report_PMNS_{ts}.md")
    json_path  = os.path.join(out_dir, f"UPT_pmns_card_{ts}.json")
    csv_path   = os.path.join(out_dir, f"UPT_pmns_matrix_{ts}.csv")

    # ASCII + MD (same content; MD is nice for viewers)
    text_block = _pmns_report_text(A, th12, th23, th13, delta, J, score, uni, Kbest, math.pi/180.0, PMNS_TARGET)
    with open(ascii_path, "w") as f: f.write(text_block)
    with open(md_path, "w") as f: f.write("```\n"+text_block+"\n```")

    # JSON card (angles in deg + rad, knobs, quality)
    card = dict(
        pmns=dict(
            angles_deg=dict(theta12=float(th12*180/np.pi), theta23=float(th23*180/np.pi),
                            theta13=float(th13*180/np.pi), delta=float(delta*180/np.pi)),
            angles_rad=dict(theta12=float(th12), theta23=float(th23),
                            theta13=float(th13), delta=float(delta)),
            J=float(J),
            unitarity_error=float(uni),
            score=float(score),
        ),
        targets_deg=dict(theta12=float(PMNS_TARGET['th12']*180/np.pi),
                         theta23=float(PMNS_TARGET['th23']*180/np.pi),
                         theta13=float(PMNS_TARGET['th13']*180/np.pi),
                         delta=float(PMNS_TARGET['delta']*180/np.pi)),
        knobs={k: float(v) for k,v in Kbest.items()},
        run_ts=ts
    )
    with open(json_path, "w") as f: json.dump(card, f, indent=2)

    # CSV of |U_PMNS|
    with open(csv_path, "w", newline="") as f:
        w = csv.writer(f)
        w.writerow(["row/col","1","2","3"])
        w.writerow(["e"] + [f"{float(A[0,j]):.9f}" for j in range(3)])
        w.writerow(["μ"] + [f"{float(A[1,j]):.9f}" for j in range(3)])
        w.writerow(["τ"] + [f"{float(A[2,j]):.9f}" for j in range(3)])

    print("\n" + "="*100)
    print("EMIT ARTIFACTS")
    print("="*100)
    print("[BEGIN SECTION:write_files]")
    print(f"[KV] ASCII = {ascii_path}")
    print(f"[KV] MD    = {md_path}")
    print(f"[KV] JSON  = {json_path}")
    print(f"[KV] CSV   = {csv_path}")
    print("[END SECTION:write_files]")

# one-call helper (writes to /content/reports)
run_pmns_and_emit("/content/reports")
# =================== end UPT_PMNS_emit_v1 ======================================================

# =================== UPT_PMNS_osc_v3 — PDG-rotation build, unitary, correct J sign ==================
import os, csv, json, math, datetime as dt
import numpy as np

# ----- angles/phase (degrees) -----
th12_deg = 33.440000
th23_deg = 49.200000
th13_deg = 8.570000
delta_deg = 197.000000

# convert to radians
th12 = math.radians(th12_deg)
th23 = math.radians(th23_deg)
th13 = math.radians(th13_deg)
delta = math.radians(delta_deg)

# Mass splittings (NO) for vacuum demo
dm21 = 7.42e-5
dm31 = 2.514e-3
dm32 = dm31 - dm21

# ----- Build U in PDG convention via rotation matrices (guarantees unitarity) -----
def U_pmns_PDG(th12, th23, th13, delta):
    s12, s23, s13 = np.sin(th12), np.sin(th23), np.sin(th13)
    c12, c23, c13 = np.cos(th12), np.cos(th23), np.cos(th13)

    # R12, R23 real rotations
    R12 = np.array([[ c12,  s12, 0.0],
                    [-s12,  c12, 0.0],
                    [ 0.0,  0.0, 1.0]], dtype=float)

    R23 = np.array([[1.0, 0.0,  0.0],
                    [0.0,  c23,  s23],
                    [0.0, -s23,  c23]], dtype=float)

    # U13 carries the Dirac phase ONLY in (13) and (31) slots (PDG: U_e3 = s13 e^{-iδ})
    U13 = np.array([[ c13,               0.0,              s13*np.exp(-1j*delta)],
                    [ 0.0,               1.0,                               0.0],
                    [-s13*np.exp(+1j*delta), 0.0,              c13          ]], dtype=complex)

    return R23 @ U13 @ R12  # PDG standard ordering

def jarlskog_from_angles(th12, th23, th13, delta):
    s12, s23, s13 = np.sin(th12), np.sin(th23), np.sin(th13)
    c12, c23, c13 = np.cos(th12), np.cos(th23), np.cos(th13)
    return s12*c12*s23*c23*s13*(c13**2)*np.sin(delta)

U = U_pmns_PDG(th12, th23, th13, delta)
A = np.abs(U)
uni_err = float(np.linalg.norm(U.conj().T @ U - np.eye(3)))

# Jarlskog (matrix vs angle formula) — signs must match in PDG build
J_from_U = float(np.imag(U[0,0]*U[1,1]*np.conj(U[0,1])*np.conj(U[1,0])))
J_formula = float(jarlskog_from_angles(th12, th23, th13, delta))

# ----- Exact 3ν vacuum probabilities -----
def prob_alpha_to_beta(alpha, beta, L_km, E_GeV, dm2=(dm21, dm31)):
    a, b = alpha, beta
    dm2_21, dm2_31 = dm2
    dm2_32 = dm2_31 - dm2_21
    coef = 1.267
    phases = {(1,0): coef*dm2_21*L_km/E_GeV,
              (2,0): coef*dm2_31*L_km/E_GeV,
              (2,1): coef*dm2_32*L_km/E_GeV}
    P = 1.0 if a == b else 0.0
    for (j,i) in [(1,0),(2,0),(2,1)]:
        Δ = phases[(j,i)]
        X = U[a,i]*np.conj(U[b,i])*np.conj(U[a,j])*U[b,j]
        P -= 4.0*float(np.real(X))*(math.sin(Δ)**2)
        P += 2.0*float(np.imag(X))*math.sin(2.0*Δ)
    if -1e-12 < P < 0: P = 0.0
    if 1 < P < 1+1e-12: P = 1.0
    return float(P)

experiments = [
    ("T2K", 295.0,   [0.4, 0.6, 0.8]),
    ("NOvA", 810.0,  [1.5, 2.0, 2.5]),
    ("DUNE", 1300.0, [1.5, 2.5, 3.5]),
]
channels = [("ν_μ→ν_e", 1,0), ("ν_μ→ν_μ", 1,1), ("ν_e→ν_e", 0,0)]

# ----- Print report (ASCII) -----
print("="*100)
print("PMNS — VACUUM OSCILLATION SNAPSHOT (PDG build)")
print("="*100, "\n")

print("[ANGLES & PHASE]")
print(f"θ12={th12_deg:.6f}°,  θ23={th23_deg:.6f}°,  θ13={th13_deg:.6f}°,  δ_CP={delta_deg:.6f}°")
print(f"J_CP (from U)={J_from_U:+.9e}   J_formula={J_formula:+.9e}")
print(f"unitarity error (||U†U−I||_F) = {uni_err:.3e}\n")

print("[|U_PMNS|  rows: e, μ, τ  cols: 1,2,3]")
print("          1            2            3")
print("e :", "  ".join(f"{A[0,j]:.9f}" for j in range(3)))
print("μ :", "  ".join(f"{A[1,j]:.9f}" for j in range(3)))
print("τ :", "  ".join(f"{A[2,j]:.9f}" for j in range(3)))
print()

for label, L, Es in experiments:
    print("="*100)
    print(f"{label} — L = {L:.0f} km (vacuum demo)")
    print("-"*100)
    header = "E[GeV]   " + "   ".join(f"{nm:>10s}" for nm,_,_ in channels)
    print(header)
    print("-"*100)
    for E in Es:
        vals = [prob_alpha_to_beta(a,b,L,E) for _,a,b in channels]
        print(f"{E:6.2f}   " + "   ".join(f"{v:10.6f}" for v in vals))
    print()

# ----- Emit artifacts -----
ts = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%S")
out_dir = "/content/reports"; os.makedirs(out_dir, exist_ok=True)
ascii_path = os.path.join(out_dir, f"UPT_ascii_PMNS_osc_{ts}.txt")
md_path    = os.path.join(out_dir, f"UPT_report_PMNS_osc_{ts}.md")
json_path  = os.path.join(out_dir, f"UPT_pmns_osc_card_{ts}.json")
csv_path   = os.path.join(out_dir, f"UPT_pmns_probs_{ts}.csv")

def _block():
    lines = []
    lines += [
        "="*100,
        "PMNS — VACUUM OSCILLATION SNAPSHOT (PDG build)",
        "="*100 + " ",
        "",
        "[ANGLES & PHASE]",
        f"θ12={th12_deg:.6f}°,  θ23={th23_deg:.6f}°,  θ13={th13_deg:.6f}°,  δ_CP={delta_deg:.6f}°",
        f"J_CP (from U)={J_from_U:+.9e}   J_formula={J_formula:+.9e}",
        f"unitarity error (||U†U−I||_F) = {uni_err:.3e}",
        "",
        "[|U_PMNS|  rows: e, μ, τ  cols: 1,2,3]",
        "          1            2            3",
        "e : " + "  ".join(f"{A[0,j]:.9f}" for j in range(3)),
        "μ : " + "  ".join(f"{A[1,j]:.9f}" for j in range(3)),
        "τ : " + "  ".join(f"{A[2,j]:.9f}" for j in range(3)),
        ""
    ]
    for label, L, Es in experiments:
        lines += [
            "="*100,
            f"{label} — L = {L:.0f} km (vacuum demo)",
            "-"*100,
            "E[GeV]   " + "   ".join(f"{nm:>10s}" for nm,_,_ in channels),
            "-"*100,
        ]
        for E in Es:
            vals = [prob_alpha_to_beta(a,b,L,E) for _,a,b in channels]
            lines.append(f"{E:6.2f}   " + "   ".join(f"{v:10.6f}" for v in vals))
        lines.append("")
    return "\n".join(lines)

text = _block()
with open(ascii_path, "w") as f: f.write(text)
with open(md_path, "w") as f: f.write("```\n"+text+"\n```")

sample = []
for label, L, Es in experiments:
    for E in Es:
        entry = dict(exp=label, L_km=L, E_GeV=E)
        for nm,a,b in channels:
            entry[nm] = prob_alpha_to_beta(a,b,L,E)
        sample.append(entry)

card = dict(
    angles_deg=dict(theta12=th12_deg, theta23=th23_deg, theta13=th13_deg, delta=delta_deg),
    dm2_eV2=dict(dm21=dm21, dm31=dm31, dm32=dm32),
    unitarity_error=uni_err,
    J_CP_from_U=J_from_U,
    J_CP_formula=J_formula,
    samples=sample,
    run_ts=ts
)
with open(json_path, "w") as f: json.dump(card, f, indent=2)

with open(csv_path, "w", newline="") as f:
    w = csv.writer(f)
    w.writerow(["exp","L_km","E_GeV","P(nu_mu->nu_e)","P(nu_mu->nu_mu)","P(nu_e->nu_e)"])
    for label, L, Es in experiments:
        for E in Es:
            vals = [prob_alpha_to_beta(a,b,L,E) for _,a,b in channels]
            w.writerow([label, f"{L:.0f}", f"{E:.2f}"] + [f"{v:.8f}" for v in vals])

print("="*100)
print("EMIT ARTIFACTS")
print("="*100)
print("[BEGIN SECTION:write_files]")
print(f"[KV] ASCII = {ascii_path}")
print(f"[KV] MD    = {md_path}")
print(f"[KV] JSON  = {json_path}")
print(f"[KV] CSV   = {csv_path}")
print("[END SECTION:write_files]")
# =================== end UPT_PMNS_osc_v3 ===========================================================

# =================== UPT_PMNS_osc_extras_v1 — anti-ν, sanity sums, angle extraction =================
import math, numpy as np

# Reuse U, A, experiments, channels, and prob_alpha_to_beta if they exist; otherwise rebuild quickly
def _have(varname): return varname in globals()

if not _have("U"):
    # minimal rebuild with your angles (degrees)
    th12_deg = 33.440000
    th23_deg = 49.200000
    th13_deg = 8.570000
    delta_deg = 197.000000
    th12, th23, th13, delta = map(math.radians, (th12_deg, th23_deg, th13_deg, delta_deg))
    def U_pmns_PDG(th12, th23, th13, delta):
        s12, s23, s13 = np.sin(th12), np.sin(th23), np.sin(th13)
        c12, c23, c13 = np.cos(th12), np.cos(th23), np.cos(th13)
        R12 = np.array([[ c12,  s12, 0.0],
                        [-s12,  c12, 0.0],
                        [ 0.0,  0.0, 1.0]], dtype=float)
        R23 = np.array([[1.0, 0.0,  0.0],
                        [0.0,  c23,  s23],
                        [0.0, -s23,  c23]], dtype=float)
        U13 = np.array([[ c13, 0.0,  s13*np.exp(-1j*delta)],
                        [ 0.0, 1.0,  0.0],
                        [-s13*np.exp(+1j*delta), 0.0, c13]], dtype=complex)
        return R23 @ U13 @ R12
    U = U_pmns_PDG(th12, th23, th13, delta)

if not _have("dm21"): dm21 = 7.42e-5
if not _have("dm31"): dm31 = 2.514e-3
if not _have("channels"):
    channels = [("ν_μ→ν_e", 1,0), ("ν_μ→ν_μ", 1,1), ("ν_e→ν_e", 0,0)]
if not _have("experiments"):
    experiments = [("T2K", 295.0, [0.4, 0.6, 0.8]),
                   ("NOvA", 810.0, [1.5, 2.0, 2.5]),
                   ("DUNE", 1300.0, [1.5, 2.5, 3.5])]

# Base vacuum probability using global U (same as you used)
def prob_alpha_to_beta_with_U(Umat, alpha, beta, L_km, E_GeV, dm2=(dm21, dm31)):
    a, b = alpha, beta
    dm2_21, dm2_31 = dm2
    dm2_32 = dm2_31 - dm2_21
    coef = 1.267
    phases = {(1,0): coef*dm2_21*L_km/E_GeV,
              (2,0): coef*dm2_31*L_km/E_GeV,
              (2,1): coef*dm2_32*L_km/E_GeV}
    P = 1.0 if a == b else 0.0
    for (j,i) in [(1,0),(2,0),(2,1)]:
        Δ = phases[(j,i)]
        X = Umat[a,i]*np.conj(Umat[b,i])*np.conj(Umat[a,j])*Umat[b,j]
        P -= 4.0*float(np.real(X))*(math.sin(Δ)**2)
        P += 2.0*float(np.imag(X))*math.sin(2.0*Δ)
    if -1e-12 < P < 0: P = 0.0
    if 1 < P < 1+1e-12: P = 1.0
    return float(P)

print("="*100)
print("ANTI-NEUTRINOS (vacuum) — use U* (δ → −δ)")
print("="*100)
Ubar = np.conj(U)
for label, L, Es in experiments:
    print("-"*100)
    print(f"{label} — L = {L:.0f} km")
    print("-"*100)
    header = "E[GeV]   " + "   ".join(f"{nm.replace('ν','ν̄'):>10s}" for nm,_,_ in channels)
    print(header)
    print("-"*100)
    for E in Es:
        vals = [prob_alpha_to_beta_with_U(Ubar, a,b,L,E) for _,a,b in channels]
        print(f"{E:6.2f}   " + "   ".join(f"{v:10.6f}" for v in vals))
    print()

# Probability-sum sanity: for each initial α and energy, sum_β P(α→β) ≈ 1
def row_sum(Umat, alpha, L, E):
    return sum(prob_alpha_to_beta_with_U(Umat, alpha, beta, L, E) for beta in (0,1,2))

print("="*100)
print("SANITY — probability row sums (should be ~1.000000)")
print("="*100)
labels = ["e","μ","τ"]
for label, L, Es in experiments:
    print(f"{label}  L={L:.0f} km")
    print("E[GeV]   " + "   ".join(f"sum P({ell}→*)".rjust(12) for ell in labels))
    for E in Es:
        sums = [row_sum(U, a, L, E) for a in (0,1,2)]
        print(f"{E:6.2f}   " + "   ".join(f"{s:12.6f}" for s in sums))
    print()

# Extract PDG angles/phases from U (up to charged-lepton rephasings)
def extract_pdg_angles_delta(Umat):
    Ue1,Ue2,Ue3 = Umat[0,0], Umat[0,1], Umat[0,2]
    theta13 = math.asin(abs(Ue3))
    c13 = math.cos(theta13)
    theta12 = math.atan2(abs(Ue2), abs(Ue1))
    Umu3, Utau3 = Umat[1,2], Umat[2,2]
    theta23 = math.atan2(abs(Umu3), abs(Utau3))
    # δ from the phase of Ue3 in PDG after rephasings: δ ≈ -arg(Ue3)
    delta = (-np.angle(Ue3)) % (2*math.pi)
    return tuple(map(math.degrees, (theta12, theta23, theta13, delta)))

th12_out, th23_out, th13_out, delta_out = extract_pdg_angles_delta(U)

print("="*100)
print("EXTRACTED ANGLES (from current U)")
print("="*100)
print(f"θ12 ≈ {th12_out:.6f}°")
print(f"θ23 ≈ {th23_out:.6f}°")
print(f"θ13 ≈ {th13_out:.6f}°")
print(f"δ_CP ≈ {delta_out:.6f}°")
# =================== end UPT_PMNS_osc_extras_v1 =====================================================

# =================== UPT_PMNS_MSW_tables_v1 — constant-density matter (ν and ν̄) ===================
import math, numpy as np

def _has(v): return v in globals()

# Reuse PMNS & splittings if present; else rebuild from your PDG angles/phase
if not _has("U"):
    th12_deg, th23_deg, th13_deg, delta_deg = 33.44, 49.20, 8.57, 197.0
    th12, th23, th13, delta = map(math.radians, (th12_deg, th23_deg, th13_deg, delta_deg))
    def U_pmns_PDG(th12, th23, th13, delta):
        s12,s23,s13 = np.sin(th12), np.sin(th23), np.sin(th13)
        c12,c23,c13 = np.cos(th12), np.cos(th23), np.cos(th13)
        R12 = np.array([[ c12,  s12, 0.0],
                        [-s12,  c12, 0.0],
                        [ 0.0,  0.0, 1.0]], dtype=float)
        R23 = np.array([[1.0, 0.0,  0.0],
                        [0.0,  c23,  s23],
                        [0.0, -s23,  c23]], dtype=float)
        U13 = np.array([[ c13, 0.0,  s13*np.exp(-1j*delta)],
                        [ 0.0, 1.0,  0.0],
                        [-s13*np.exp(+1j*delta), 0.0, c13]], dtype=complex)
        return R23 @ U13 @ R12
    U = U_pmns_PDG(th12, th23, th13, delta)

if not _has("dm21"): dm21 = 7.42e-5   # eV^2
if not _has("dm31"): dm31 = 2.514e-3  # eV^2 (NH)

# Default channels/experiments (match your earlier printouts)
if not _has("channels"):
    channels = [("ν_μ→ν_e", 1,0), ("ν_μ→ν_μ", 1,1), ("ν_e→ν_e", 0,0)]
if not _has("experiments"):
    experiments = [("T2K", 295.0, [0.4, 0.6, 0.8]),
                   ("NOvA", 810.0, [1.5, 2.0, 2.5]),
                   ("DUNE", 1300.0, [1.5, 2.5, 3.5])]

# ---- Constant-density matter engine (flavor-basis evolution) ----
# A(E, rho, Ye) in eV^2: A = 7.56e-5 * (rho[g/cc] * Ye) * E[GeV]
def A_eV2(E_GeV, rho_gcc=2.8, Ye=0.5):
    return 7.56e-5 * (rho_gcc * Ye) * E_GeV

# Build flavor mass^2 matrix, diagonalize to get matter eigenmodes
def eig_system_in_matter(Umat, dm21, dm31, E_GeV, rho_gcc=2.8, Ye=0.5, anti=False):
    # vacuum diag(m1^2, m2^2, m3^2) with m1^2=0, m2^2=dm21, m3^2=dm31
    M2_vac = np.diag([0.0, dm21, dm31])
    # flavor-basis M2: U M2 U†
    M2_flav = Umat @ M2_vac @ Umat.conj().T
    # matter potential (add to ee element): +A for ν, -A for ν̄
    A = A_eV2(E_GeV, rho_gcc, Ye)
    if anti:
        A = -A
        Uuse = Umat.conj()  # ν̄ effectively uses U*
    else:
        Uuse = Umat
    M2_flav = Uuse @ M2_vac @ Uuse.conj().T + np.diag([A, 0.0, 0.0])
    # Diagonalize Hermitian matrix
    w, Vmat = np.linalg.eigh(M2_flav)  # w ascending, columns of V are eigenvectors
    return w, Vmat

# Evolution operator S(L,E): S = V exp(-i * 1.267 * (w/E) * L) V†  (overall phase irrelevant)
def S_matter(Umat, L_km, E_GeV, dm21, dm31, rho_gcc=2.8, Ye=0.5, anti=False):
    w, Vmat = eig_system_in_matter(Umat, dm21, dm31, E_GeV, rho_gcc, Ye, anti=anti)
    phase = np.exp(-1j * 1.267 * (w / E_GeV) * L_km)
    return Vmat @ np.diag(phase) @ Vmat.conj().T

def P_alpha_to_beta_matter(Umat, a, b, L_km, E_GeV, dm21, dm31, rho_gcc=2.8, Ye=0.5, anti=False):
    S = S_matter(Umat, L_km, E_GeV, dm21, dm31, rho_gcc, Ye, anti=anti)
    # Probability = |S_{b a}|^2  (column = initial, row = final)
    return float(abs(S[b, a])**2)

# Convenience printer
def print_matter_tables(title, Umat, dm21, dm31, rho_gcc=2.8, Ye=0.5, anti=False):
    print("="*100)
    print(f"{title} — constant-density MSW  (ρ≈{rho_gcc:.2f} g/cc, Y_e≈{Ye:.2f})")
    print("="*100)
    tag = "ν̄" if anti else "ν"
    for label, L, Es in experiments:
        print("-"*100)
        print(f"{label} — L = {L:.0f} km")
        print("-"*100)
        hdr = "E[GeV]   " + "   ".join(f"{nm.replace('ν',tag):>10s}" for nm,_,_ in channels)
        print(hdr)
        print("-"*100)
        for E in Es:
            vals = [P_alpha_to_beta_matter(Umat, a,b, L, E, dm21, dm31, rho_gcc, Ye, anti=anti)
                    for _,a,b in channels]
            print(f"{E:6.2f}   " + "   ".join(f"{v:10.6f}" for v in vals))
        print()

# Row-sum check in matter
def print_row_sums_matter(title, Umat, dm21, dm31, rho_gcc=2.8, Ye=0.5, anti=False):
    print("="*100)
    print(f"SANITY — probability row sums in matter ({title})  (should be ~1.000000)")
    print("="*100)
    labels = ["e","μ","τ"]
    for label, L, Es in experiments:
        print(f"{label}  L={L:.0f} km")
        print("E[GeV]   " + "   ".join(f"sum P({ell}→*)".rjust(12) for ell in labels))
        for E in Es:
            S = S_matter(Umat, L, E, dm21, dm31, rho_gcc, Ye, anti=anti)
            sums = [float(np.sum(np.abs(S[:,a])**2)) for a in (0,1,2)]
            print(f"{E:6.2f}   " + "   ".join(f"{s:12.6f}" for s in sums))
        print()

# ---- Print tables (feel free to tweak rho/Ye if you want mantle vs crust) ----
RHO, YE = 2.8, 0.5   # Earth crust ballpark
print_matter_tables("NEUTRINOS (ν) in matter", U, dm21, dm31, rho_gcc=RHO, Ye=YE, anti=False)
print_matter_tables("ANTI-NEUTRINOS (ν̄) in matter", U, dm21, dm31, rho_gcc=RHO, Ye=YE, anti=True)
print_row_sums_matter("ν, crust",  U, dm21, dm31, rho_gcc=RHO, Ye=YE, anti=False)
print_row_sums_matter("ν̄, crust", U, dm21, dm31, rho_gcc=RHO, Ye=YE, anti=True)
# =================== end UPT_PMNS_MSW_tables_v1 =====================================================

# --- Optional: dump matter tables to CSV strings (copy/paste friendly) ---
def dump_csv_block(title, tag, exps, values_dict):
    print(f"[CSV] {title} ({tag})")
    for name, L, Es in exps:
        print(f"# {name}, L={L} km")
        print("E_GeV, P_mue, P_mumu, P_ee")
        for E in Es:
            Pmue, Pmumu, Pee = values_dict[(name, E)]
            print(f"{E:.2f},{Pmue:.6f},{Pmumu:.6f},{Pee:.6f}")
        print()

# Build dicts from the numbers you just printed (reuse your functions if available)
def collect_probs(Umat, dm21, dm31, exps, anti=False, rho=2.8, Ye=0.5):
    out = {}
    for name, L, Es in exps:
        for E in Es:
            Pmue = P_alpha_to_beta_matter(Umat, 1,0, L, E, dm21, dm31, rho, Ye, anti)
            Pmumu= P_alpha_to_beta_matter(Umat, 1,1, L, E, dm21, dm31, rho, Ye, anti)
            Pee  = P_alpha_to_beta_matter(Umat, 0,0, L, E, dm21, dm31, rho, Ye, anti)
            out[(name, E)] = (Pmue, Pmumu, Pee)
    return out

nu_vals  = collect_probs(U, dm21, dm31, experiments, anti=False, rho=2.8, Ye=0.5)
nubar_vals = collect_probs(U, dm21, dm31, experiments, anti=True,  rho=2.8, Ye=0.5)

dump_csv_block("NEUTRINOS in matter", "rho=2.8, Ye=0.5", experiments, nu_vals)
dump_csv_block("ANTI-NEUTRINOS in matter", "rho=2.8, Ye=0.5", experiments, nubar_vals)

# ============================ UPT_PMNS_OSC_FULL_v2 ============================
# Self-contained PMNS → probabilities in vacuum & constant-density matter.
# Prints human-friendly ASCII tables and writes CSVs. No CLI args.

import numpy as np, math, os, datetime as dt

# --------------------- PMNS (PDG convention) + helpers -----------------------
def pmns_pdg(th12_deg, th23_deg, th13_deg, delta_deg):
    t12, t23, t13, d = map(math.radians, (th12_deg, th23_deg, th13_deg, delta_deg))
    s12, c12 = math.sin(t12), math.cos(t12)
    s23, c23 = math.sin(t23), math.cos(t23)
    s13, c13 = math.sin(t13), math.cos(t13)
    ed = np.exp(-1j*d)
    U = np.array([
        [ c12*c13,                      s12*c13,                s13*ed ],
        [-s12*c23 - c12*s23*s13/ed,     c12*c23 - s12*s23*s13/ed,  s23*c13],
        [ s12*s23 - c12*c23*s13/ed,    -c12*s23 - s12*c23*s13/ed,  c23*c13],
    ], dtype=complex)
    return U

def jarlskog(U):
    return float(np.imag(U[0,0]*U[1,1]*np.conj(U[0,1])*np.conj(U[1,0])))

def jarlskog_formula(th12, th23, th13, delta_deg):
    s12, s23, s13 = map(lambda x: math.sin(math.radians(x)), (th12, th23, th13))
    c12, c23, c13 = math.cos(math.radians(th12)), math.cos(math.radians(th23)), math.cos(math.radians(th13))
    d = math.radians(delta_deg)
    return s12*c12*s23*c23*(s13*(c13**2))*math.sin(d)

def unitarity_error(U):
    I = np.eye(3, dtype=complex)
    M = U.conj().T @ U - I
    return float(np.linalg.norm(M, 'fro'))

# ---------------------- Oscillation phase & vacuum P -------------------------
def _phase(dm2, L_km, E_GeV):
    return 1.267*dm2*L_km/max(E_GeV, 1e-18)

def P_vac(U, alpha, beta, L_km, E_GeV, dm21, dm31):
    """Exact 3ν vacuum probabilities (0-based indices)."""
    dm32 = dm31 - dm21
    phases = {(1,0): _phase(dm21, L_km, E_GeV),
              (2,0): _phase(dm31, L_km, E_GeV),
              (2,1): _phase(dm32, L_km, E_GeV)}
    a, b = alpha, beta
    P = 1.0 if a == b else 0.0
    for (j,i) in [(1,0),(2,0),(2,1)]:  # <-- fixed 0-based pairs
        d = phases[(j,i)]
        X = U[a,i]*np.conj(U[b,i])*np.conj(U[a,j])*U[b,j]
        P -= 4.0*np.real(X)*(math.sin(d)**2)
        P += 2.0*np.imag(X)*math.sin(2.0*d)
    return float(min(1.0, max(0.0, P)))

# ------------------------- Constant-density MSW ------------------------------
# a = 2√2 G_F n_e E ≈ 7.632e-5 * rho[g/cc] * Ye * E[GeV]  (in eV^2)
def _a_eV2(E_GeV, rho_gcc, Ye):
    return 7.632e-5 * rho_gcc * Ye * E_GeV

def mixing_in_matter(Uvac, dm21, dm31, E_GeV, rho_gcc=2.8, Ye=0.5, anti=False):
    M2 = np.diag([0.0, dm21, dm31])                       # eV^2
    A  = np.diag([_a_eV2(E_GeV, rho_gcc, Ye), 0.0, 0.0])  # flavor basis
    if anti:
        Uuse = np.conj(Uvac)
        Heff = Uuse @ M2 @ Uuse.conj().T - A              # anti: A→−A, U→U*
    else:
        Uuse = Uvac
        Heff = Uuse @ M2 @ Uuse.conj().T + A
    lam, Um = np.linalg.eigh(Heff)
    return Um, lam

def P_matter(Uvac, alpha, beta, L_km, E_GeV, dm21, dm31, rho_gcc=2.8, Ye=0.5, anti=False):
    Um, lam = mixing_in_matter(Uvac, dm21, dm31, E_GeV, rho_gcc, Ye, anti=anti)
    dm21_m = lam[1] - lam[0]
    dm31_m = lam[2] - lam[0]
    return P_vac(Um, alpha, beta, L_km, E_GeV, dm21_m, dm31_m)

# --------------------------- Pretty printers --------------------------------
def row(s): print(s)
def hdr(title):
    row("="*100); row(title); row("="*100)
def ascii_matrix(title, A, rlabels, clabels):
    row(title)
    row(" " * 13 + "  ".join(f"{c:>12s}" for c in clabels))
    for i, rl in enumerate(rlabels):
        row(f" {rl:1s} : " + "  ".join(f"{A[i,j]:.9f}" for j in range(A.shape[1])))

def print_table_probs(label, L, Es, probs):
    row("-"*100)
    row(f"{label} — L = {L} km")
    row("-"*100)
    row("E[GeV]      ν_μ→ν_e      ν_μ→ν_μ      ν_e→ν_e")
    row("-"*100)
    for E, (pmue, pmumu, pee) in zip(Es, probs):
        row(f"{E:6.2f}     {pmue:10.6f}   {pmumu:10.6f}   {pee:10.6f}")
    row("")

def sanity_row_sums(label, L, Es, Uuse, dm21, dm31, matter=None, rho=2.8, Ye=0.5, anti=False):
    row(f"{label}  L={L} km")
    row("E[GeV]     sum P(e→*)     sum P(μ→*)     sum P(τ→*)")
    for E in Es:
        sums = []
        for a in [0,1,2]:
            s = 0.0
            for b in [0,1,2]:
                if matter is None:
                    s += P_vac(Uuse, a, b, L, E, dm21, dm31)
                else:
                    s += P_matter(Uuse, a, b, L, E, dm21, dm31, rho_gcc=rho, Ye=Ye, anti=anti)
            sums.append(s)
        row(f"  {E:4.2f}       {sums[0]:.6f}       {sums[1]:.6f}       {sums[2]:.6f}")
    row("")

# ------------------------------- CSV utils ----------------------------------
def ensure_dir(path):
    if path and not os.path.isdir(path): os.makedirs(path, exist_ok=True)

def write_csv_probs(fname, L, Es, probs, header):
    ensure_dir(os.path.dirname(fname))
    with open(fname, "w") as f:
        f.write(f"# {header}, L={L:.1f} km\n")
        f.write("E_GeV,P_mue,P_mumu,P_ee\n")
        for E,(pmue,pmumu,pee) in zip(Es, probs):
            f.write(f"{E:.2f},{pmue:.6f},{pmumu:.6f},{pee:.6f}\n")

# =============================== Main run ===================================
if __name__ == "__main__":
    # Angles & splittings (Normal Ordering demo)
    th12, th23, th13 = 33.44, 49.20, 8.57
    delta = 197.0
    dm21  = 7.42e-5     # eV^2
    dm31  = 2.517e-3    # eV^2 (NO)

    U = pmns_pdg(th12, th23, th13, delta)
    J_fromU  = jarlskog(U)
    J_pdg    = jarlskog_formula(th12, th23, th13, delta)
    uni_err  = unitarity_error(U)
    A = np.abs(U)

    # Snapshot
    hdr("PMNS — VACUUM SNAPSHOT (PDG build)")
    row(f"[ANGLES & PHASE]  θ12={th12:.6f}°,  θ23={th23:.6f}°,  θ13={th13:.6f}°,  δ_CP={delta:.6f}°")
    row(f"J_CP (from U)={J_fromU:+.12e}   J_formula={J_pdg:+.12e}")
    row(f"unitarity error (||U†U−I||_F) = {uni_err:.3e}\n")
    ascii_matrix("[|U_PMNS|  rows: e, μ, τ  cols: 1,2,3 ]", A, ["e","μ","τ"], ["1","2","3"])
    row("")

    # Experiments and energies
    experiments = [
        ("T2K",  295.0, [0.40, 0.60, 0.80]),
        ("NOvA", 810.0, [1.50, 2.00, 2.50]),
        ("DUNE", 1300.0,[1.50, 2.50, 3.50]),
    ]

    # VACUUM (ν)
    hdr("NEUTRINOS (ν) — VACUUM")
    vac_all = {}
    for name, L, Es in experiments:
        probs = []
        for E in Es:
            pmue  = P_vac(U, 1,0, L, E, dm21, dm31)
            pmumu = P_vac(U, 1,1, L, E, dm21, dm31)
            pee   = P_vac(U, 0,0, L, E, dm21, dm31)
            probs.append((pmue, pmumu, pee))
        print_table_probs(name, L, Es, probs)
        vac_all[name] = (L, Es, probs)

    # VACUUM (ν̄): use U* in vacuum
    hdr("ANTI-NEUTRINOS (ν̄) — VACUUM  (use U*)")
    vacbar_all = {}
    Ubar = np.conj(U)
    for name, L, Es in experiments:
        probs = []
        for E in Es:
            pmue  = P_vac(Ubar, 1,0, L, E, dm21, dm31)
            pmumu = P_vac(Ubar, 1,1, L, E, dm21, dm31)
            pee   = P_vac(Ubar, 0,0, L, E, dm21, dm31)
            probs.append((pmue, pmumu, pee))
        print_table_probs(name, L, Es, probs)
        vacbar_all[name] = (L, Es, probs)

    # Sanity (vacuum)
    hdr("SANITY — probability row sums (vacuum, should be ~1.000000)")
    for name, L, Es in experiments:
        sanity_row_sums(f"{name}  (vacuum)", L, Es, U, dm21, dm31)

    # MATTER (constant density)
    rho, Ye = 2.80, 0.50  # Earth's crust
    hdr(f"NEUTRINOS (ν) in matter — constant density (ρ≈{rho:.2f} g/cc, Y_e≈{Ye:.2f})")
    mat_all = {}
    for name, L, Es in experiments:
        probs = []
        for E in Es:
            pmue  = P_matter(U, 1,0, L, E, dm21, dm31, rho_gcc=rho, Ye=Ye, anti=False)
            pmumu = P_matter(U, 1,1, L, E, dm21, dm31, rho_gcc=rho, Ye=Ye, anti=False)
            pee   = P_matter(U, 0,0, L, E, dm21, dm31, rho_gcc=rho, Ye=Ye, anti=False)
            probs.append((pmue, pmumu, pee))
        print_table_probs(name, L, Es, probs)
        mat_all[name] = (L, Es, probs)

    hdr(f"ANTI-NEUTRINOS (ν̄) in matter — constant density (ρ≈{rho:.2f} g/cc, Y_e≈{Ye:.2f})")
    matbar_all = {}
    for name, L, Es in experiments:
        probs = []
        for E in Es:
            pmue  = P_matter(U, 1,0, L, E, dm21, dm31, rho_gcc=rho, Ye=Ye, anti=True)
            pmumu = P_matter(U, 1,1, L, E, dm21, dm31, rho_gcc=rho, Ye=Ye, anti=True)
            pee   = P_matter(U, 0,0, L, E, dm21, dm31, rho_gcc=rho, Ye=Ye, anti=True)
            probs.append((pmue, pmumu, pee))
        print_table_probs(name, L, Es, probs)
        matbar_all[name] = (L, Es, probs)

    # Sanity (matter ν, ν̄)
    hdr("SANITY — probability row sums in matter (ν, crust)")
    for name, L, Es in experiments:
        sanity_row_sums(name, L, Es, U, dm21, dm31, matter=True, rho=rho, Ye=Ye, anti=False)
    hdr("SANITY — probability row sums in matter (ν̄, crust)")
    for name, L, Es in experiments:
        sanity_row_sums(name, L, Es, U, dm21, dm31, matter=True, rho=rho, Ye=Ye, anti=True)

    # ---------------------------- CSV emission -------------------------------
    out_dir = "/content/reports"  # change if desired
    ts = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%SZ")
    for label, store in [("vac",vac_all), ("vacbar",vacbar_all), ("mat",mat_all), ("matbar",matbar_all)]:
        for name, (L, Es, probs) in store.items():
            fname = os.path.join(out_dir, f"UPT_{label}_{name}_{ts}.csv")
            header = f"{name} {label.upper()}"
            write_csv_probs(fname, L, Es, probs, header)

    row("="*100)
    row("EMIT ARTIFACTS")
    row("="*100)
    row("[BEGIN SECTION:write_files]")
    for label, store in [("vac",vac_all), ("vacbar",vacbar_all), ("mat",mat_all), ("matbar",matbar_all)]:
        for name in store.keys():
            row(f"[KV] CSV = {out_dir}/UPT_{label}_{name}_{ts}.csv")
    row("[END SECTION:write_files]")
    row(f"[KV] run_ts = {ts}")
# ============================ /END MODULE ====================================

# -------- Matter vs Vacuum delta tables + CP asymmetry (appendix) --------
    def print_diff_block(title, vac_store, mat_store, vacbar_store, matbar_store):
        row("="*100)
        row(title)
        row("="*100)
        for name, (L, Es, vac_probs) in vac_store.items():
            _, _, mat_probs    = mat_store[name]
            _, _, vacbar_probs = vacbar_store[name]
            _, _, matbar_probs = matbar_store[name]

            row("-"*100)
            row(f"{name} — L = {L:.1f} km   (ν: matter − vacuum)   and  CP asym A_CP = P(νμ→νe) − P(ν̄μ→ν̄e)")
            row("-"*100)
            row("E[GeV]   ΔP_mue(ν)   ΔP_mumu(ν)  ΔP_ee(ν)   A_CP(vac)   A_CP(matter)")
            row("-"*100)
            for E, pv, pm, pvb, pmb in zip(
                Es, vac_probs, mat_probs, vacbar_probs, matbar_probs
            ):
                pmue_v, pmumu_v, pee_v = pv
                pmue_m, pmumu_m, pee_m = pm
                pmue_vb, _, _          = pvb
                pmue_mb, _, _          = pmb
                d_mue  = pmue_m  - pmue_v
                d_mumu = pmumu_m - pmumu_v
                d_ee   = pee_m   - pee_v
                A_vac  = pmue_v  - pmue_vb
                A_mat  = pmue_m  - pmue_mb
                row(f"{E:6.2f}   {d_mue:+.6f}   {d_mumu:+.6f}  {d_ee:+.6f}   {A_vac:+.6f}   {A_mat:+.6f}")
            row("")

    print_diff_block(
        "MATTER vs VACUUM — deltas and CP asymmetry (your angles, crust density)",
        vac_all, mat_all, vacbar_all, matbar_all
    )

# ========= APPENDIX: CONSOLIDATED EXPORT (Vacuum vs Matter + CP Asym) =========
import csv, datetime as _dt

def _nowstamp():
    return _dt.datetime.now(_dt.timezone.utc).strftime("%Y%m%d-%H%M%S") + "Z"

def _pretty_header(title):
    print("="*100)
    print(title)
    print("="*100)

def _print_side_by_side(name, L, Es, vac, mat, vacb, matb):
    print("-"*100)
    print(f"{name} — L = {L:.1f} km   (ν: matter − vacuum)   |   A_CP = P(νμ→νe) − P(ν̄μ→ν̄e)")
    print("-"*100)
    print("E[GeV] |  Pmue_v   Pmue_m   ΔPmue  ||  Pmumu_v  Pmumu_m  ΔPμμ  ||  Pee_v    Pee_m    ΔPee  ||  A_CP(vac)  A_CP(mat)")
    print("-"*100)
    for E, pv, pm, pvb, pmb in zip(Es, vac, mat, vacb, matb):
        pmue_v, pmumu_v, pee_v = pv
        pmue_m, pmumu_m, pee_m = pm
        pmue_vb, _, _          = pvb
        pmue_mb, _, _          = pmb
        d_mue  = pmue_m  - pmue_v
        d_mumu = pmumu_m - pmumu_v
        d_ee   = pee_m   - pee_v
        A_vac  = pmue_v  - pmue_vb
        A_mat  = pmue_m  - pmue_mb
        print(f"{E:6.2f} | {pmue_v:8.6f} {pmue_m:8.6f} {d_mue:+8.6f}  ||  "
              f"{pmumu_v:8.6f} {pmumu_m:8.6f} {d_mumu:+7.6f}  ||  "
              f"{pee_v:8.6f} {pee_m:8.6f} {d_ee:+7.6f}  ||  {A_vac:+10.6f}  {A_mat:+10.6f}")
    print("")

def _collect_rows(exps_dicts):
    # exps_dicts: (label, store) where store[name] = (L, Es, [(Pmue,Pmumu,Pee)...])
    # we assume your four stores share keys & Es order (as already ensured upstream)
    rows = []
    for name in exps_dicts[0][1].keys():
        L, Es, vac = exps_dicts[0][1][name]
        _, _, mat  = exps_dicts[1][1][name]
        _, _, vcb  = exps_dicts[2][1][name]
        _, _, mtb  = exps_dicts[3][1][name]
        for E, pv, pm, pvb, pmb in zip(Es, vac, mat, vcb, mtb):
            pmue_v, pmumu_v, pee_v = pv
            pmue_m, pmumu_m, pee_m = pm
            pmue_vb, _, _          = pvb
            pmue_mb, _, _          = pmb
            rows.append({
                "experiment": name, "L_km": f"{L:.1f}", "E_GeV": f"{E:.2f}",
                "Pmue_v": pmue_v, "Pmue_m": pmue_m, "dPmue": pmue_m - pmue_v,
                "Pmmu_v": pmumu_v, "Pmmu_m": pmumu_m, "dPmmu": pmumu_m - pmumu_v,
                "Pee_v":  pee_v,   "Pee_m":  pee_m,   "dPee":  pee_m  - pee_v,
                "Acp_vac": pmue_v - pmue_vb,
                "Acp_mat": pmue_m - pmue_mb,
            })
    return rows

# Pretty side-by-side printouts
_pretty_header("MATTER vs VACUUM — compact side-by-side (ν channels + CP asym)")
for _name, (L, Es, vac_probs) in vac_all.items():
    _, _, mat_probs    = mat_all[_name]
    _, _, vacbar_probs = vacbar_all[_name]
    _, _, matbar_probs = matbar_all[_name]
    _print_side_by_side(_name, L, Es, vac_probs, mat_probs, vacbar_probs, matbar_probs)

# Consolidated CSV
_stamp = _nowstamp()
_csv_path = f"/content/reports/UPT_matter_vs_vacuum_CONSOLIDATED_{_stamp}.csv"
_fields = ["experiment","L_km","E_GeV",
           "Pmue_v","Pmue_m","dPmue",
           "Pmmu_v","Pmmu_m","dPmmu",
           "Pee_v","Pee_m","dPee",
           "Acp_vac","Acp_mat"]

_all_rows = _collect_rows([
    ("vac",    vac_all),
    ("mat",    mat_all),
    ("vacbar", vacbar_all),
    ("matbar", matbar_all),
])

with open(_csv_path, "w", newline="") as f:
    w = csv.DictWriter(f, fieldnames=_fields)
    w.writeheader()
    for r in _all_rows:
        w.writerow({k: r[k] for k in _fields})

print("="*100)
print("EMIT ARTIFACT — consolidated CSV (Vacuum vs Matter + CP asymmetry)")
print("="*100)
print(f"[KV] CSV = {_csv_path}")

# ===================== APPENDIX: HIGHLIGHTS & QUICK SANITY (clean) =====================
from math import isfinite

def _sep(title):
    print("="*100)
    print(title)
    print("="*100)

def _cols(triples):
    # triples: [(Pmue, Pmumu, Pee), ...]
    mue = [t[0] for t in triples]
    mumu = [t[1] for t in triples]
    ee = [t[2] for t in triples]
    return mue, mumu, ee

def _argmax_abs(xs):
    i = max(range(len(xs)), key=lambda k: abs(xs[k]))
    return i, xs[i]

def _experiment_highlights(name, L, Es, vac, mat, vacbar, matbar):
    mue_v,  mumu_v,  ee_v  = _cols(vac)
    mue_m,  mumu_m,  ee_m  = _cols(mat)
    mue_vb, _,       _     = _cols(vacbar)
    mue_mb, _,       _     = _cols(matbar)

    # deltas (matter − vacuum) for νμ→νe etc.
    d_mue  = [pm - pv for pm, pv in zip(mue_m,  mue_v)]
    d_mumu = [pm - pv for pm, pv in zip(mumu_m, mumu_v)]
    d_ee   = [pm - pv for pm, pv in zip(ee_m,   ee_v)]

    # CP asymmetry A_CP = P(νμ→νe) − P(ν̄μ→ν̄e)
    A_vac = [pv - pvb for pv, pvb in zip(mue_v,  mue_vb)]
    A_mat = [pm - pmb for pm, pmb in zip(mue_m,  mue_mb)]

    im_dmue, vmax_dmue = _argmax_abs(d_mue)
    im_Avac, vmax_Avac = _argmax_abs(A_vac)
    im_Amat, vmax_Amat = _argmax_abs(A_mat)

    print(f"{name} — L = {L:.1f} km")
    print("-"*100)
    print(" strongest matter boost in νμ→νe:")
    print(f"   E = {Es[im_dmue]:.2f} GeV | ΔP_mue = {vmax_dmue:+.6f} "
          f"(P_mue^mat = {mue_m[im_dmue]:.6f}, P_mue^vac = {mue_v[im_dmue]:.6f})")
    print(" peak CP asymmetry |A_CP| (vacuum and matter):")
    print(f"   vacuum:  E = {Es[im_Avac]:.2f} GeV | A_CP^vac = {vmax_Avac:+.6f}")
    print(f"   matter:  E = {Es[im_Amat]:.2f} GeV | A_CP^mat = {vmax_Amat:+.6f}")
    print(" quick row-sum sanity (μ-row, inferred Pμ→τ = 1 − Pμe − Pμμ):")
    for i, E in enumerate(Es):
        mu_tau_v = 1.0 - mue_v[i] - mumu_v[i]
        mu_tau_m = 1.0 - mue_m[i] - mumu_m[i]
        print(f"   E={E:5.2f} GeV | vac: Pμe+Pμμ+Pμτ = {mue_v[i]+mumu_v[i]+mu_tau_v: .6f} "
              f"(Pμτ={mu_tau_v: .6f}) | mat: {mue_m[i]+mumu_m[i]+mu_tau_m: .6f} "
              f"(Pμτ={mu_tau_m: .6f})")
    print()

def _compact_sanity(name, L, Es, vac, mat, vacbar, matbar):
    # Ensure each listed probability is finite and in [0,1], and inferred Pμτ is in [0,1].
    def ok_prob(p): return isfinite(p) and (-1e-9 <= p <= 1.0+1e-9)

    flags = []
    for i, E in enumerate(Es):
        pv  = vac[i]      # (Pmue, Pmumu, Pee) in vacuum
        pm  = mat[i]      # in matter
        pvb = vacbar[i]   # antineutrino vacuum
        pmb = matbar[i]   # antineutrino matter

        for tag, triple in [("vac", pv), ("mat", pm), ("vacbar", pvb), ("matbar", pmb)]:
            if not all(ok_prob(x) for x in triple):
                flags.append((E, tag, triple))

        # inferred Pμτ (row-sum closure for μ row) using the channels we printed
        for tag, (pmue, pmumu) in [("vac", pv[:2]), ("mat", pm[:2]),
                                   ("vacbar", pvb[:2]), ("matbar", pmb[:2])]:
            pmutau = 1.0 - pmue - pmumu
            if not ok_prob(pmutau):
                flags.append((E, f"{tag} (inferred Pμτ)", (pmutau,)))

    if flags:
        print(f"!! probability sanity issues in {name}:")
        for E, tag, triple in flags[:12]:
            print(f"  E={E:.2f} GeV [{tag}] -> {tuple(round(x,6) for x in triple)}")
    else:
        print(f"[{name}] probability bounds OK on listed channels and inferred Pμτ.")

# ---------------- run reports ----------------
_sep("HIGHLIGHTS — where matter & CP effects peak")
for exp_name in vac_all.keys():
    L, Es, vac_probs     = vac_all[exp_name]
    _, _, mat_probs      = mat_all[exp_name]
    _, _, vacbar_probs   = vacbar_all[exp_name]
    _, _, matbar_probs   = matbar_all[exp_name]
    _experiment_highlights(exp_name, L, Es, vac_probs, mat_probs, vacbar_probs, matbar_probs)

_sep("QUICK SANITY — probability bounds")
for exp_name in vac_all.keys():
    L, Es, vac_probs     = vac_all[exp_name]
    _, _, mat_probs      = mat_all[exp_name]
    _, _, vacbar_probs   = vacbar_all[exp_name]
    _, _, matbar_probs   = matbar_all[exp_name]
    _compact_sanity(exp_name, L, Es, vac_probs, mat_probs, vacbar_probs, matbar_probs)

# ===================== MODULE: DENSITY & HIERARCHY SWEEPS (robust + globals-safe) =====================
import os, csv, math, datetime as dt
import numpy as np
from math import isfinite

# --- tiny CSV helper ---
def _write_csv(path, header, rows):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", newline="") as f:
        w = csv.writer(f)
        if header: w.writerow(header)
        for r in rows: w.writerow(r)

def _now_tag():
    return dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%SZ")

# --- accept dict OR list for experiments ---
# valid forms:
#  {"T2K": (295.0,[0.4,0.6,0.8]), "NOvA": (810.0,[...]), "DUNE": (1300.0,[...])}
#  [("T2K",(295.0,[...])), ("NOvA",(810.0,[...])), ...]
#  [("T2K",295.0,[...]), ("NOvA",810.0,[...]), ...]
def _norm_experiments(experiments):
    out = []
    if isinstance(experiments, dict):
        for name, val in experiments.items():
            if isinstance(val, (list, tuple)) and len(val) == 2:
                L, Es = val
                out.append((str(name), float(L), list(Es)))
            else:
                raise ValueError(f"Bad experiments value for {name!r}: expected (L, [Es]).")
    elif isinstance(experiments, (list, tuple)):
        for item in experiments:
            if isinstance(item, (list, tuple)) and len(item) == 2:
                name, val = item
                if not (isinstance(val, (list, tuple)) and len(val) == 2):
                    raise ValueError(f"Bad experiments entry {item!r}: expected (name,(L,[Es])).")
                L, Es = val
                out.append((str(name), float(L), list(Es)))
            elif isinstance(item, (list, tuple)) and len(item) == 3:
                name, L, Es = item
                out.append((str(name), float(L), list(Es)))
            else:
                raise ValueError(f"Bad experiments entry {item!r}.")
    else:
        raise TypeError("experiments must be dict or list/tuple.")
    return out

# --- defaults ---
Ye_default   = 0.50
rho_baseline = 2.80

# --- require your earlier probability functions ---
if "P_vac" not in globals() or "P_matter" not in globals():
    raise RuntimeError("This module expects P_vac(...) and P_matter(...) to be defined earlier.")

def _flip_hierarchy(dm21_in, dm31_in):
    # keep |dm31|, flip sign for IH
    return dm21_in, -abs(dm31_in)

def _triple_vac(U, L, E, dm21_, dm31_):
    return (
        float(P_vac(U, 1, 0, L, E, dm21_, dm31_)),  # P(νμ→νe)
        float(P_vac(U, 1, 1, L, E, dm21_, dm31_)),  # P(νμ→νμ)
        float(P_vac(U, 0, 0, L, E, dm21_, dm31_)),  # P(νe→νe)
    )

def _triple_mat(U, L, E, dm21_, dm31_, rho, Ye, anti=False):
    return (
        float(P_matter(U, 1, 0, L, E, dm21_, dm31_, rho, Ye, anti=anti)),
        float(P_matter(U, 1, 1, L, E, dm21_, dm31_, rho, Ye, anti=anti)),
        float(P_matter(U, 0, 0, L, E, dm21_, dm31_, rho, Ye, anti=anti)),
    )

# --- REPORT A: density sweep ---
def report_density_sweep(U, experiments, dm21_, dm31_, Ye=Ye_default, rhos=(0.0, 1.5, 2.8, 3.5, 5.0)):
    exps = _norm_experiments(experiments)
    tag = _now_tag()
    print("="*100)
    print("DENSITY SWEEP — P(νμ→νe) vs ρ  (Ye=%.2f)" % Ye)
    print("="*100)
    rows = []
    for name, L, Es in exps:
        print(f"{name} — L = {L:.1f} km")
        print("-"*100)
        print("E[GeV] " + "  ".join(f"ρ={r:>4.1f}" for r in rhos))
        for E in Es:
            vals = [ _triple_mat(U, L, E, dm21_, dm31_, rho, Ye, anti=False)[0] for rho in rhos ]
            print(f"{E:6.2f}  " + "  ".join(f"{v: .6f}" for v in vals))
            rows.append([name, L, E] + vals)
        print()
    out = f"/content/reports/UPT_density_sweep_{tag}.csv"
    _write_csv(out, ["exp","L_km","E_GeV"] + [f"rho_{r}" for r in rhos], rows)
    print("[KV] CSV =", out)
    return out

# --- REPORT B: hierarchy comparison ---
def report_hierarchy_compare(U, experiments, dm21_NH, dm31_NH, Ye=Ye_default, rho=rho_baseline):
    exps = _norm_experiments(experiments)
    dm21_IH, dm31_IH = _flip_hierarchy(dm21_NH, dm31_NH)
    tag = _now_tag()
    print("="*100)
    print("HIERARCHY COMPARISON — NH vs IH  (vacuum & matter, ρ=%.2f, Ye=%.2f)" % (rho, Ye))
    print("="*100)
    rows = []
    for name, L, Es in exps:
        print(f"{name} — L = {L:.1f} km")
        print("-"*100)
        print("E[GeV] |  Pmue_v(NH)  Pmue_v(IH)  |  Pmue_m(NH)  Pmue_m(IH)  | A_CP^vac(NH)  A_CP^vac(IH) | A_CP^mat(NH)  A_CP^mat(IH)")
        print("-"*100)
        for E in Es:
            # vacuum ν/ν̄
            pmue_v_NH  = _triple_vac(U,          L, E, dm21_NH, dm31_NH)[0]
            pmue_v_IH  = _triple_vac(U,          L, E, dm21_IH, dm31_IH)[0]
            pmue_vb_NH = _triple_vac(np.conj(U), L, E, dm21_NH, dm31_NH)[0]
            pmue_vb_IH = _triple_vac(np.conj(U), L, E, dm21_IH, dm31_IH)[0]
            # matter ν/ν̄
            pmue_m_NH  = _triple_mat(U, L, E, dm21_NH, dm31_NH, rho, Ye, anti=False)[0]
            pmue_m_IH  = _triple_mat(U, L, E, dm21_IH, dm31_IH, rho, Ye, anti=False)[0]
            pmue_mb_NH = _triple_mat(U, L, E, dm21_NH, dm31_NH, rho, Ye, anti=True)[0]
            pmue_mb_IH = _triple_mat(U, L, E, dm21_IH, dm31_IH, rho, Ye, anti=True)[0]

            A_vac_NH = pmue_v_NH - pmue_vb_NH
            A_vac_IH = pmue_v_IH - pmue_vb_IH
            A_mat_NH = pmue_m_NH - pmue_mb_NH
            A_mat_IH = pmue_m_IH - pmue_mb_IH

            print(f"{E:6.2f} |   {pmue_v_NH: .6f}   {pmue_v_IH: .6f}  |   {pmue_m_NH: .6f}   {pmue_m_IH: .6f}  |"
                  f"    {A_vac_NH: .6f}     {A_vac_IH: .6f}  |    {A_mat_NH: .6f}     {A_mat_IH: .6f}")
            rows.append([name, L, E, pmue_v_NH, pmue_v_IH, pmue_m_NH, pmue_m_IH, A_vac_NH, A_vac_IH, A_mat_NH, A_mat_IH])
        print()
    out = f"/content/reports/UPT_hierarchy_compare_{tag}.csv"
    _write_csv(out, ["exp","L_km","E_GeV","Pmue_v_NH","Pmue_v_IH","Pmue_m_NH","Pmue_m_IH","Acp_vac_NH","Acp_vac_IH","Acp_mat_NH","Acp_mat_IH"], rows)
    print("[KV] CSV =", out)
    return out

# --- REPORT C: CP asymmetry panel (reads/writes globals safely) ---
def _build_all_tables_if_needed(U, experiments, dm21_, dm31_, rho=rho_baseline, Ye=Ye_default):
    g = globals()
    keys = ("vac_all","mat_all","vacbar_all","matbar_all")
    if all(k in g for k in keys):
        return g["vac_all"], g["mat_all"], g["vacbar_all"], g["matbar_all"]

    exps = _norm_experiments(experiments)
    vac_all, mat_all, vacbar_all, matbar_all = {}, {}, {}, {}
    for name, L, Es in exps:
        vac_probs, mat_probs, vacbar_probs, matbar_probs = [], [], [], []
        for E in Es:
            vac_probs.append(    _triple_vac(U,          L, E, dm21_, dm31_))
            vacbar_probs.append( _triple_vac(np.conj(U), L, E, dm21_, dm31_))
            mat_probs.append(    _triple_mat(U, L, E, dm21_, dm31_, rho, Ye, anti=False))
            matbar_probs.append( _triple_mat(U, L, E, dm21_, dm31_, rho, Ye, anti=True))
        vac_all[name]    = (L, Es, vac_probs)
        mat_all[name]    = (L, Es, mat_probs)
        vacbar_all[name] = (L, Es, vacbar_probs)
        matbar_all[name] = (L, Es, matbar_probs)

    # publish to globals so later calls can reuse without recompute
    g["vac_all"]    = vac_all
    g["mat_all"]    = mat_all
    g["vacbar_all"] = vacbar_all
    g["matbar_all"] = matbar_all
    return vac_all, mat_all, vacbar_all, matbar_all

def report_cp_panel(U, experiments, dm21_, dm31_, rho=rho_baseline, Ye=Ye_default):
    vac_all, mat_all, vacbar_all, matbar_all = _build_all_tables_if_needed(U, experiments, dm21_, dm31_, rho=rho, Ye=Ye)
    print("="*100)
    print("CP ASYMMETRY PANEL — A_CP = P(νμ→νe) − P(ν̄μ→ν̄e)")
    print("="*100)
    for name in vac_all.keys():
        L, Es, vac_probs = vac_all[name]
        _, _, mat_probs  = mat_all[name]
        _, _, vbar_probs = vacbar_all[name]
        _, _, mbar_probs = matbar_all[name]
        print(f"{name} — L = {L:.1f} km")
        print("-"*100)
        print("E[GeV]   A_CP(vac)   A_CP(mat)")
        print("-"*100)
        for i, E in enumerate(Es):
            Acv = vac_probs[i][0] - vbar_probs[i][0]
            Acm = mat_probs[i][0] - mbar_probs[i][0]
            print(f"{E:6.2f}   {Acv: .6f}     {Acm: .6f}")
        print()

# ---------------- run the three reports ----------------
# (A) density sweep around crust values
_ = report_density_sweep(U, experiments, dm21, dm31, Ye=Ye_default, rhos=(0.0, 1.5, 2.8, 3.5, 5.0))

# (B) NH vs IH comparison
_ = report_hierarchy_compare(U, experiments, dm21, dm31, Ye=Ye_default, rho=rho_baseline)

# (C) CP asymmetry panel (now globals-safe)
report_cp_panel(U, experiments, dm21, dm31, rho=rho_baseline, Ye=Ye_default)
# ===================== /MODULE ===============================================================

# ===================== MODULE: BI-PROBABILITY PANEL (vacuum vs matter) =====================
import os, csv, math, datetime as dt
import numpy as np

# expects: U (PDG build), P_vac(U, a,b,L,E,dm21,dm31), P_matter(U, a,b,L,E,dm21,dm31,rho,Ye,anti),
# dm21, dm31, and experiments (dict or list) to be defined already.

RHO_BI   = 2.80   # g/cc
YE_BI    = 0.50
DEGS_SET = [0.0, 90.0, 180.0, 270.0]

# choose “near 1st max” energies for each experiment (tweak as you like)
E_PICK = {"T2K": 0.60, "NOvA": 2.00, "DUNE": 2.50}

def _now_tag():
    return dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%SZ")

def _norm_experiments(experiments):
    out = []
    if isinstance(experiments, dict):
        for name, (L, Es) in experiments.items():
            out.append((str(name), float(L), list(Es)))
    else:
        for item in experiments:
            if len(item) == 2:
                name, (L, Es) = item
            else:
                name, L, Es = item
            out.append((str(name), float(L), list(Es)))
    return out

def build_U_with_delta(delta_deg, base_U):
    """Rebuild U with same angles but new δ. Assumes PDG parameterization builder exists as make_U_pdg(θ12,θ23,θ13,δ)."""
    if "make_U_pdg" not in globals():
        # infer angles back from |U| (robust for standard ranges)
        # θ13 from |U_e3|, θ12 from |U_e2|, θ23 from |U_μ3|
        s13 = abs(base_U[0,2])
        th13 = math.asin(s13)
        c13 = math.cos(th13)
        s12 = abs(base_U[0,1]) / max(c13,1e-12)
        s12 = min(1.0, max(0.0, s12))
        th12 = math.asin(s12)
        s23 = abs(base_U[1,2]) / max(c13,1e-12)
        s23 = min(1.0, max(0.0, s23))
        th23 = math.asin(s23)
        δ = math.radians(delta_deg)
        # PDG build inline
        c12, s12 = math.cos(th12), math.sin(th12)
        c23, s23 = math.cos(th23), math.sin(th23)
        c13, s13 = math.cos(th13), math.sin(th13)
        e_miδ = np.exp(-1j*δ)
        U = np.array([
            [ c12*c13,                s12*c13,               s13*e_miδ ],
            [-s12*c23 - c12*s23*s13*e_miδ,  c12*c23 - s12*s23*s13*e_miδ,  s23*c13],
            [ s12*s23 - c12*c23*s13*e_miδ, -c12*s23 - s12*c23*s13*e_miδ,  c23*c13]
        ], dtype=complex)
        return U
    else:
        # if you kept the angle scalars around, use them; else fall back to extracting as above.
        raise RuntimeError("make_U_pdg is not available here. Either define it or use the fallback above.")

def bi_prob_panel(U_base, experiments, dm21, dm31, rho=RHO_BI, Ye=YE_BI, E_custom=None):
    exps = _norm_experiments(experiments)
    tag = _now_tag()
    print("="*100)
    print("BI-PROBABILITY — P(νμ→νe) vs P(ν̄μ→ν̄e)  (δ = 0°,90°,180°,270°)")
    print("="*100)
    for name, L, Es in exps:
        E0 = E_custom.get(name, E_PICK.get(name, Es[len(Es)//2])) if E_custom else E_PICK.get(name, Es[len(Es)//2])
        print(f"{name} — L = {L:.1f} km,  E = {E0:.2f} GeV  |  ρ={rho:.2f} g/cc, Y_e={Ye:.2f}")
        print("-"*100)
        print("delta  |   Pv  Pvb  Acp(vac)   ||   Pm  Pmb  Acp(mat)")
        print("-"*100)
        rows = [["delta_deg","Pv","Pvb","Acp_vac","Pm","Pmb","Acp_mat","L_km","E_GeV","rho","Ye"]]
        for d in DEGS_SET:
            U = build_U_with_delta(d, U_base)
            # vacuum
            Pv  = float(P_vac(U,          1,0, L, E0, dm21, dm31))
            Pvb = float(P_vac(np.conj(U), 1,0, L, E0, dm21, dm31))
            Acv = Pv - Pvb
            # matter
            Pm  = float(P_matter(U, 1,0, L, E0, dm21, dm31, rho, Ye, anti=False))
            Pmb = float(P_matter(U, 1,0, L, E0, dm21, dm31, rho, Ye, anti=True))
            Acm = Pm - Pmb
            print(f"{d:5.0f}° | {Pv: .5f} {Pvb: .5f}  {Acv: .5f}   ||  {Pm: .5f} {Pmb: .5f}  {Acm: .5f}")
            rows.append([d, Pv, Pvb, Acv, Pm, Pmb, Acm, L, E0, rho, Ye])
        print()
        # emit CSV per experiment
        out = f"/content/reports/UPT_biprob_{name}_{tag}.csv"
        os.makedirs(os.path.dirname(out), exist_ok=True)
        with open(out,"w",newline="") as f:
            csv.writer(f).writerows(rows)
        print("[KV] CSV =", out)
        print()

# -------- run it --------
bi_prob_panel(U, experiments, dm21, dm31, rho=RHO_BI, Ye=YE_BI)
# ===================== /MODULE ================================================================

# ===================== MODULE: PREM-lite multi-slab matter (amplitude-level) =====================
# Purpose:
#   • Exact constant-density evolution per slab: S = V exp(-i * 1.267 * (L/E) * Λ) V†,
#     where H_tilde = U diag(0,Δm21,Δm31) U† + a diag(1,0,0), a = ± 7.632e-5 * ρ[g/cc] * Ye * E[GeV]
#   • Multi-slab profile = ordered product of slabs’ S-matrices.
#   • Prints side-by-side tables: Vacuum vs Constant(ρ=2.8) vs Two-Slab (crust→mantle) for T2K/NOvA/DUNE.
#   • Emits CSVs.

import os, csv, math, datetime as dt
import numpy as np

# --- Assumes these already exist in your session: U (PDG build), dm21, dm31, experiments (T2K/NOvA/DUNE).
# If not, you can adapt from your PMNS PDG builder earlier.

# Constants for matter potential (eV^2 units)
_A_COEF = 7.632e-5  # a = ± 7.632e-5 * ρ[g/cc] * Ye * E[GeV]

def _now_tag():
    return dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%SZ")

def _norm_experiments(experiments):
    out = []
    if isinstance(experiments, dict):
        for name, (L, Es) in experiments.items():
            out.append((str(name), float(L), list(Es)))
    else:
        for item in experiments:
            if len(item) == 2:
                name, (L, Es) = item
            else:
                name, L, Es = item
            out.append((str(name), float(L), list(Es)))
    return out

def _evolution_operator_const(U, L_km, E_GeV, dm21, dm31, rho=None, Ye=0.5, anti=False):
    """
    Exact constant-density evolution operator S in flavor basis.
    If rho is None → VACUUM (a=0).
    """
    # H_tilde in eV^2 (no 1/(2E) here; we fold units into the 1.267 factor below)
    M2 = np.diag([0.0, dm21, dm31])
    H = U @ M2 @ U.conj().T
    if rho is not None:
        sign = -1.0 if anti else +1.0
        a = sign * _A_COEF * rho * Ye * E_GeV
        H = H + np.diag([a, 0.0, 0.0])

    # Diagonalize H = V Λ V†
    evals, V = np.linalg.eigh(H)
    phase = -1j * 1.267 * (L_km / max(E_GeV, 1e-12)) * evals
    S = V @ np.diag(np.exp(phase)) @ V.conj().T
    return S

def P_vac_amp(U, alpha, beta, L_km, E_GeV, dm21, dm31):
    S = _evolution_operator_const(U, L_km, E_GeV, dm21, dm31, rho=None, Ye=0.5, anti=False)
    amp = S[beta, alpha]
    return float(np.real(amp*np.conj(amp)))

def P_matter_const_amp(U, alpha, beta, L_km, E_GeV, dm21, dm31, rho, Ye=0.5, anti=False):
    S = _evolution_operator_const(U, L_km, E_GeV, dm21, dm31, rho=rho, Ye=Ye, anti=anti)
    amp = S[beta, alpha]
    return float(np.real(amp*np.conj(amp)))

def P_matter_profile(U, alpha, beta, segments, E_GeV, dm21, dm31, Ye=0.5, anti=False):
    """
    segments = list of (L_km, rho_gcc) covering the full baseline in order.
    Returns probability |(S_total)_{β α}|^2 with S_total = Π slabs S_slab (right-most = first slab).
    """
    S_tot = np.eye(3, dtype=complex)
    for L_km, rho in segments:
        S_seg = _evolution_operator_const(U, L_km, E_GeV, dm21, dm31, rho=rho, Ye=Ye, anti=anti)
        S_tot = S_seg @ S_tot
    amp = S_tot[beta, alpha]
    return float(np.real(amp*np.conj(amp)))

# --- Toy profiles: constant crust vs two-slab crust→upper-mantle (very simple PREM-lite)
def toy_two_slab(L_km, frac_crust=0.15, rho_crust=2.80, rho_mantle=3.40):
    L1 = max(0.0, min(L_km, frac_crust*L_km))
    L2 = max(0.0, L_km - L1)
    return [(L1, rho_crust), (L2, rho_mantle)]

def constant_profile(L_km, rho=2.80):
    return [(L_km, rho)]

# --- Reporter: compare Vacuum vs Constant(ρ=2.8) vs Two-Slab for νμ→νe, νμ→νμ, νe→νe
def report_prem_lite(U, experiments, dm21, dm31, Ye=0.5,
                     rho_const=2.80, rho_mantle=3.40, frac_crust=0.15):
    exps = _norm_experiments(experiments)
    tag = _now_tag()
    print("="*100)
    print("PREM-lite — Vacuum vs Constant(ρ=2.8) vs Two-Slab (crust→mantle)")
    print("="*100)
    for name, L, Es in exps:
        print(f"{name} — L = {L:.1f} km  |  Ye={Ye:.2f}  |  two-slab: crust {frac_crust:.2f}×L @ {rho_const:.2f} g/cc → mantle @ {rho_mantle:.2f} g/cc")
        print("-"*100)
        print("E[GeV] |   Pmue_v   Pmue_c   Pmue_2s   ||   Pmumu_v  Pmumu_c  Pmumu_2s  ||   Pee_v    Pee_c    Pee_2s")
        print("-"*100)
        rows = [["E_GeV","Pmue_v","Pmue_const","Pmue_twoslab",
                 "Pmumu_v","Pmumu_const","Pmumu_twoslab",
                 "Pee_v","Pee_const","Pee_twoslab","L_km","Ye","rho_const","rho_mantle","frac_crust"]]
        for E in Es:
            # Vacuum
            Pmue_v  = P_vac_amp(U, 1,0, L, E, dm21, dm31)
            Pmumu_v = P_vac_amp(U, 1,1, L, E, dm21, dm31)
            Pee_v   = P_vac_amp(U, 0,0, L, E, dm21, dm31)

            # Constant crust density
            Pmue_c  = P_matter_const_amp(U, 1,0, L, E, dm21, dm31, rho=rho_const, Ye=Ye, anti=False)
            Pmumu_c = P_matter_const_amp(U, 1,1, L, E, dm21, dm31, rho=rho_const, Ye=Ye, anti=False)
            Pee_c   = P_matter_const_amp(U, 0,0, L, E, dm21, dm31, rho=rho_const, Ye=Ye, anti=False)

            # Two-slab profile
            segs    = toy_two_slab(L, frac_crust=frac_crust, rho_crust=rho_const, rho_mantle=rho_mantle)
            Pmue_2s = P_matter_profile(U, 1,0, segs, E, dm21, dm31, Ye=Ye, anti=False)
            Pmumu_2s= P_matter_profile(U, 1,1, segs, E, dm21, dm31, Ye=Ye, anti=False)
            Pee_2s  = P_matter_profile(U, 0,0, segs, E, dm21, dm31, Ye=Ye, anti=False)

            print(f"{E:6.2f} |  {Pmue_v: .6f} {Pmue_c: .6f} {Pmue_2s: .6f}   ||   {Pmumu_v: .6f} {Pmumu_c: .6f} {Pmumu_2s: .6f}   ||   {Pee_v: .6f} {Pee_c: .6f} {Pee_2s: .6f}")
            rows.append([E, Pmue_v, Pmue_c, Pmue_2s,
                         Pmumu_v, Pmumu_c, Pmumu_2s,
                         Pee_v, Pee_c, Pee_2s, L, Ye, rho_const, rho_mantle, frac_crust])

        # emit CSV per experiment
        out = f"/content/reports/UPT_prem_lite_{name}_{tag}.csv"
        os.makedirs(os.path.dirname(out), exist_ok=True)
        with open(out,"w",newline="") as f:
            csv.writer(f).writerows(rows)
        print("[KV] CSV =", out)
        print()

# ---------------- Run it with your current U, dm^2, experiments ----------------
# Example mantle density & crust fraction; tweak freely.
report_prem_lite(U, experiments, dm21, dm31, Ye=0.50, rho_const=2.80, rho_mantle=3.40, frac_crust=0.15)
# ===================== /MODULE ====================================================================

# ===================== MODULE: (E,L) sweep heatmaps + peak finder =====================
# Scans energies across each experiment’s baseline for vacuum & matter (constant ρ),
# prints compact peak summaries, and emits CSV heatmaps for ν and ν̄ appearance.

import os, csv, math, datetime as dt
import numpy as np

def _tag():
    return dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%SZ")

def _require(func_names):
    missing = [f for f in func_names if f not in globals()]
    if missing:
        raise RuntimeError(
            "Missing required functions: %s\n"
            "Make sure you ran the PMNS + vacuum/matter modules first." % ", ".join(missing)
        )

# We’ll use your existing helpers from previous cells:
#  - P_vac_amp(U, alpha, beta, L_km, E_GeV, dm21, dm31)
#  - P_matter_const_amp(U, alpha, beta, L_km, E_GeV, dm21, dm31, rho, Ye=0.5, anti=False)

def _ensure_dir(path):
    os.makedirs(os.path.dirname(path), exist_ok=True)

def _linspace_inclusive(a, b, n):
    if n <= 1: return np.array([a], dtype=float)
    return np.linspace(a, b, n)

def sweep_EL_and_emit(U, experiments, dm21, dm31,
                      rho=2.80, Ye=0.50,
                      E_ranges=None, N_E=61,
                      channels=((1,0,"Pmue"),), # list of (alpha,beta,label)
                      out_prefix="/content/reports/UPT_ELsweep"):
    """
    For each experiment baseline L and energy range:
      • Build vacuum & matter arrays for requested channels (ν and ν̄).
      • Print peak P and where it occurs.
      • Emit CSVs (vac, mat, vacbar, matbar) for each channel.
    """
    _require(["P_vac_amp","P_matter_const_amp"])
    tag = _tag()

    # Default energy spans per experiment if none provided
    # (T2K around 0.3–1.0, NOvA ~1–3, DUNE ~1–5)
    default_ranges = {
        "T2K":  (0.3, 1.0),
        "NOvA": (1.0, 3.0),
        "DUNE": (1.0, 5.0),
    }
    # Normalize experiments: list of (name, L_km, energies_to_print_if_any)
    exp_list = []
    if isinstance(experiments, dict):
        for name, (L, Es_or_none) in experiments.items():
            name = str(name)
            if E_ranges and name in E_ranges:
                Emin, Emax = E_ranges[name]
            else:
                Emin, Emax = default_ranges.get(name, (0.3, 5.0))
            Es = _linspace_inclusive(Emin, Emax, N_E) if (not Es_or_none) else np.array(Es_or_none, float)
            exp_list.append((name, float(L), Es))
    else:
        # list-like [(name,(L,Es))] or [(name,L,Es)]
        for item in experiments:
            if len(item) == 2:
                name, (L, Es) = item
            else:
                name, L, Es = item
            name = str(name)
            if E_ranges and name in E_ranges:
                Emin, Emax = E_ranges[name]
                Es = _linspace_inclusive(Emin, Emax, N_E)
            elif Es is None or len(Es) == 0:
                Emin, Emax = default_ranges.get(name, (0.3, 5.0))
                Es = _linspace_inclusive(Emin, Emax, N_E)
            exp_list.append((name, float(L), np.array(Es, float)))

    print("="*100)
    print("E–L SWEEP — appearance heatmaps & peaks (vacuum vs matter; ν and ν̄)")
    print("="*100)

    for name, L, Es in exp_list:
        print(f"{name} — L = {L:.1f} km  |  ρ={rho:.2f} g/cc, Y_e={Ye:.2f}")
        print("-"*100)
        # For each requested channel
        for (alpha, beta, label) in channels:
            # Arrays
            Pv   = np.zeros_like(Es)
            Pm   = np.zeros_like(Es)
            Pvb  = np.zeros_like(Es)
            Pmb  = np.zeros_like(Es)

            for i, E in enumerate(Es):
                Pv[i]  = P_vac_amp(U, alpha, beta, L, E, dm21, dm31)
                Pm[i]  = P_matter_const_amp(U, alpha, beta, L, E, dm21, dm31, rho=rho, Ye=Ye, anti=False)
                Pvb[i] = P_vac_amp(U, alpha, beta, L, E, dm21, dm31)  # same as Pv; included for symmetry
                Pmb[i] = P_matter_const_amp(U, alpha, beta, L, E, dm21, dm31, rho=rho, Ye=Ye, anti=True)

            # Peaks
            i_v_max  = int(np.argmax(Pv));   Ev_max,  Pv_max  = float(Es[i_v_max]),  float(Pv[i_v_max])
            i_m_max  = int(np.argmax(Pm));   Em_max,  Pm_max  = float(Es[i_m_max]),  float(Pm[i_m_max])
            i_mb_max = int(np.argmax(Pmb));  Emb_max, Pmb_max = float(Es[i_mb_max]), float(Pmb[i_mb_max])

            # Quick print block
            print(f"[{label}] vacuum peak:  P={Pv_max:.6f} at E={Ev_max:.3f} GeV")
            print(f"[{label}] matter  peak:  P={Pm_max:.6f} at E={Em_max:.3f} GeV")
            print(f"[{label}] anti-ν matter peak:  P={Pmb_max:.6f} at E={Emb_max:.3f} GeV")

            # Emit CSVs
            base = f"{out_prefix}_{name}_{label}_{tag}"
            _ensure_dir(base + "_vac.csv")
            headers = ["E_GeV", f"{label}_vac", f"{label}_mat", f"{label}_vacbar", f"{label}_matbar",
                       "L_km", "rho_gcc", "Y_e"]

            with open(base+"_all.csv","w",newline="") as f:
                w = csv.writer(f); w.writerow(headers)
                for E, a,b,c,d in zip(Es, Pv, Pm, Pvb, Pmb):
                    w.writerow([float(E), float(a), float(b), float(c), float(d), L, rho, Ye])

            # Also provide split files (handy for plotting)
            for tag2, arr in [("vac",Pv), ("mat",Pm), ("vacbar",Pvb), ("matbar",Pmb)]:
                with open(base+f"_{tag2}.csv","w",newline="") as f:
                    w = csv.writer(f); w.writerow(["E_GeV", f"{label}_{tag2}", "L_km", "rho_gcc", "Y_e"])
                    for E, val in zip(Es, arr):
                        w.writerow([float(E), float(val), L, rho, Ye])

            print(f"[KV] CSV(all) = {base}_all.csv")
            print(f"[KV] CSV(vac) = {base}_vac.csv")
            print(f"[KV] CSV(mat) = {base}_mat.csv")
            print(f"[KV] CSV(vacbar) = {base}_vacbar.csv")
            print(f"[KV] CSV(matbar) = {base}_matbar.csv")
            print()

        # A tiny one-line separator per experiment
        print()

# ---- Run with your current U, experiments, and dm^2 (appearance channel νμ→νe by default)
sweep_EL_and_emit(
    U, experiments, dm21, dm31,
    rho=2.80, Ye=0.50,
    # Example: override ranges if you like
    # E_ranges={"T2K":(0.3,1.0), "NOvA":(1.0,3.0), "DUNE":(1.0,5.0)},
    N_E=121,
    channels=((1,0,"Pmue"),)  # add (1,1,"Pmumu"), (0,0,"Pee") if desired
)
# ===================== /MODULE ====================================================================

# ===================== MODULE: RESULTS MANIFEST (bundle all artifacts + knobs) =====================
import os, re, json, math, glob, datetime as dt
from collections import defaultdict

def _nowtag():
    return dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%SZ")

def _safe(val):
    try:
        return float(val)
    except Exception:
        return None

def _fileinfo(path):
    try:
        return dict(
            path=path,
            bytes=os.path.getsize(path),
            mtime_iso=dt.datetime.utcfromtimestamp(os.path.getmtime(path)).strftime("%Y-%m-%dT%H:%M:%SZ"),
        )
    except Exception:
        return dict(path=path, bytes=None, mtime_iso=None)

def _maybe(name, default=None):
    return globals().get(name, default)

# --------- harvest “state” from your session (best-effort, survives missing vars) ----------
# CKM / texture knobs (v4n_plus2 you’ve been using)
CKM_STATE = {
    "lam_input" : _maybe("lam", None),
    "targets"   : {
        "Vus": _maybe("TARGET", {}).get("Vus") if "TARGET" in globals() else None,
        "Vcb": _maybe("TARGET", {}).get("Vcb") if "TARGET" in globals() else None,
        "Vub": _maybe("TARGET", {}).get("Vub") if "TARGET" in globals() else None,
        "J":   _maybe("TARGET", {}).get("J")   if "TARGET" in globals() else None,
    },
    "nni_lopsided_knobs": {
        # fixed
        "a_u": _safe(_maybe("au", 1.250)),
        "c_u": _safe(_maybe("cu", 0.852)),
        "c_d": _safe(_maybe("cd", 1.621)),
        # tuned (v4n_plus2 defaults as fallback)
        "r12"  : _safe(_maybe("r12_f",   _maybe("r12",   1.078))),
        "phi_u": _safe(_maybe("phi_u_f", _maybe("phi_u", 2.297))),
        "phi_d": _safe(_maybe("phi_d_f", _maybe("phi_d", 2.282))),
        "b_u"  : _safe(_maybe("b_u_f",   _maybe("b_u",   1.275))),
        "p32"  : _safe(_maybe("p32_f",   _maybe("p32",   1.763))),
        "a_d"  : _safe(_maybe("a_d_f",   _maybe("a_d",   1.241))),
        "b_d"  : _safe(_maybe("b_d_f",   _maybe("b_d",   0.970))),
        "g13"  : _safe(_maybe("g13_f",   _maybe("g13",   0.470))),
    },
}

# PMNS angles & phase (your PDG build)
PMNS_STATE = {
    "angles_deg": {
        "theta12": _safe(_maybe("theta12_deg", 33.440)),
        "theta23": _safe(_maybe("theta23_deg", 49.200)),
        "theta13": _safe(_maybe("theta13_deg",  8.570)),
        "deltaCP": _safe(_maybe("deltaCP_deg",197.000)),
    },
    "dm2_eV2": {
        "dm21": _safe(_maybe("dm21", 7.42e-5)),
        "dm31": _safe(_maybe("dm31", 2.517e-3)),  # NH default; your IH tables override internally
    },
    "matter_defaults": {
        "rho_gcc": _safe(_maybe("rho_baseline", 2.80)),
        "Ye": _safe(_maybe("Ye_default", 0.50)),
    }
}

# Experiments (name → baseline)
EXPERIMENTS = {}
if "experiments" in globals():
    try:
        for name, tpl in experiments.items():
            L = float(tpl[0] if isinstance(tpl, (list,tuple)) else tpl)
            EXPERIMENTS[str(name)] = L
    except Exception:
        # fall back to typical baselines
        EXPERIMENTS = {"T2K":295.0, "NOvA":810.0, "DUNE":1300.0}
else:
    EXPERIMENTS = {"T2K":295.0, "NOvA":810.0, "DUNE":1300.0}

# --------- gather artifacts in /content and /content/reports ----------
roots = ["/content", "/content/reports"]
patterns = [
    "UPT_*",
    "UPT_*_*.csv",
    "UPT_*_*.txt",
    "UPT_*_*.md",
    "UPT_*_*.json",
    "UPT_*_*.png",
]
found = []
for r in roots:
    for pat in patterns:
        found.extend(glob.glob(os.path.join(r, pat)))
found = sorted(set(found))

# classify files by a simple “kind” key
KIND_RULES = [
    ("ckm_card",       re.compile(r"UPT_texture_card.*\.json$")),
    ("ckm_report",     re.compile(r"UPT_report_TEXTURES_.*\.(md|txt)$")),
    ("ckm_ascii",      re.compile(r"UPT_ascii_TEXTURES_.*\.txt$")),
    ("ckm_csv",        re.compile(r"UPT_texture_ckm_.*\.csv$")),
    ("pmns_card",      re.compile(r"UPT_pmns_card.*\.json$")),
    ("pmns_report",    re.compile(r"UPT_report_PMNS.*\.(md|txt)$")),
    ("pmns_ascii",     re.compile(r"UPT_ascii_PMNS.*\.txt$")),
    ("pmns_matrix",    re.compile(r"UPT_pmns_matrix_.*\.csv$")),
    ("pmns_osc_card",  re.compile(r"UPT_pmns_osc_card.*\.json$")),
    ("pmns_osc_ascii", re.compile(r"UPT_ascii_PMNS_osc_.*\.txt$")),
    ("pmns_osc_csv",   re.compile(r"UPT_pmns_probs_.*\.csv$")),
    ("vac_csv",        re.compile(r"UPT_vac_.*\.csv$")),
    ("vacbar_csv",     re.compile(r"UPT_vacbar_.*\.csv$")),
    ("mat_csv",        re.compile(r"UPT_mat_.*\.csv$")),
    ("matbar_csv",     re.compile(r"UPT_matbar_.*\.csv$")),
    ("mvv_csv",        re.compile(r"UPT_matter_vs_vacuum_CONSOLIDATED_.*\.csv$")),
    ("density_csv",    re.compile(r"UPT_density_sweep_.*\.csv$")),
    ("hier_csv",       re.compile(r"UPT_hierarchy_compare_.*\.csv$")),
    ("biprob_csv",     re.compile(r"UPT_biprob_.*\.csv$")),
    ("prem_csv",       re.compile(r"UPT_prem_lite_.*\.csv$")),
    ("ELsweep_all",    re.compile(r"UPT_ELsweep_.*_all\.csv$")),
    ("ELsweep_vac",    re.compile(r"UPT_ELsweep_.*_vac\.csv$")),
    ("ELsweep_mat",    re.compile(r"UPT_ELsweep_.*_mat\.csv$")),
    ("ELsweep_vacbar", re.compile(r"UPT_ELsweep_.*_vacbar\.csv$")),
    ("ELsweep_matbar", re.compile(r"UPT_ELsweep_.*_matbar\.csv$")),
    ("generic",        re.compile(r".*")),  # fallback
]

grouped = defaultdict(list)
for path in found:
    kind = "generic"
    for k, rx in KIND_RULES:
        if rx.search(path.replace("\\","/")):
            kind = k
            break
    grouped[kind].append(_fileinfo(path))

# sort each bucket by mtime, newest last
for k in grouped:
    grouped[k] = sorted(grouped[k], key=lambda d: (d["mtime_iso"] or ""), reverse=False)

manifest = {
    "created_utc": _nowtag(),
    "ckm_state": CKM_STATE,
    "pmns_state": PMNS_STATE,
    "experiments_km": EXPERIMENTS,
    "artifacts": grouped,
}

# write manifest
out_dir = "/content/reports"
os.makedirs(out_dir, exist_ok=True)
out_path = os.path.join(out_dir, f"UPT_RESULTS_MANIFEST_{_nowtag()}.json")
with open(out_path, "w") as f:
    json.dump(manifest, f, indent=2)

# pretty print a human-friendly index
def _count(k): return len(grouped.get(k, []))
print("="*100)
print("RESULTS MANIFEST — bundle of knobs + artifacts")
print("="*100)
print(f"CKM λ(input): {CKM_STATE['lam_input']}")
if CKM_STATE["targets"]["Vus"] is not None:
    print("Targets: |Vus|=%.9f  |Vcb|=%.9f  |Vub|=%.9f  J=%.3e" % (
        CKM_STATE["targets"]["Vus"], CKM_STATE["targets"]["Vcb"], CKM_STATE["targets"]["Vub"], CKM_STATE["targets"]["J"]))

print("\nTexture knobs (tuned): r12=%.3f  phi_u=%.3f  phi_d=%.3f  b_u=%.3f  p32=%.3f  a_d=%.3f  b_d=%.3f  g13=%.3f" % (
    CKM_STATE["nni_lopsided_knobs"]["r12"], CKM_STATE["nni_lopsided_knobs"]["phi_u"], CKM_STATE["nni_lopsided_knobs"]["phi_d"],
    CKM_STATE["nni_lopsided_knobs"]["b_u"], CKM_STATE["nni_lopsided_knobs"]["p32"], CKM_STATE["nni_lopsided_knobs"]["a_d"],
    CKM_STATE["nni_lopsided_knobs"]["b_d"], CKM_STATE["nni_lopsided_knobs"]["g13"]))

a = PMNS_STATE["angles_deg"]
print("\nPMNS angles (deg): θ12=%.3f  θ23=%.3f  θ13=%.3f  δ=%.3f" % (a["theta12"], a["theta23"], a["theta13"], a["deltaCP"]))
print("dm² (eV²): dm21=%.3e  dm31=%.3e" % (PMNS_STATE["dm2_eV2"]["dm21"], PMNS_STATE["dm2_eV2"]["dm31"]))
print("Matter defaults: ρ=%.2f g/cc, Y_e=%.2f" % (PMNS_STATE["matter_defaults"]["rho_gcc"], PMNS_STATE["matter_defaults"]["Ye"]))

print("\nARTIFACT COUNTS (newest last within each bucket)")
print("  ckm_card: %-3d   ckm_report/ascii/csv: %d/%d/%d" % (_count("ckm_card"), _count("ckm_report"), _count("ckm_ascii"), _count("ckm_csv")))
print("  pmns_card: %-3d  pmns_report/ascii/matrix: %d/%d/%d" % (_count("pmns_osc_card"), _count("pmns_report"), _count("pmns_osc_ascii"), _count("pmns_matrix")))
print("  vac/mat/vacbar/matbar CSV: %d/%d/%d/%d" % (_count("vac_csv"), _count("mat_csv"), _count("vacbar_csv"), _count("matbar_csv")))
print("  matter_vs_vacuum CSV: %d   density_sweep CSV: %d   hierarchy CSV: %d" % (_count("mvv_csv"), _count("density_csv"), _count("hier_csv")))
print("  bi-prob CSV: %d   prem-lite CSV: %d   ELsweep (all/vac/mat/vacbar/matbar): %d/%d/%d/%d/%d" % (
    _count("biprob_csv"), _count("prem_csv"), _count("ELsweep_all"), _count("ELsweep_vac"),
    _count("ELsweep_mat"), _count("ELsweep_vacbar"), _count("ELsweep_matbar")))

# show the newest few in each class
def _tail(k, n=2):
    arr = grouped.get(k, [])
    return arr[-n:] if len(arr) >= n else arr

def _print_tail(k, title, n=2):
    items = _tail(k, n=n)
    if not items: return
    print(f"\nNewest {title}:")
    for it in items:
        print("  - %(path)s  (%(bytes)s bytes, mtime=%(mtime_iso)s)" % it)

_print_tail("ckm_card", "CKM cards")
_print_tail("pmns_osc_card", "PMNS osc cards")
_print_tail("mvv_csv", "matter-vs-vacuum CSV")
_print_tail("density_csv", "density sweep CSV")
_print_tail("hier_csv", "hierarchy compare CSV")
_print_tail("biprob_csv", "bi-probability CSV")
_print_tail("prem_csv", "PREM-lite CSV")
_print_tail("ELsweep_all", "E–L sweep (all) CSV", n=3)

print("\n[KV] MANIFEST = %s" % out_path)
# ===================== /MODULE ====================================================================

# ===================== MODULE: FIGURES FROM CSVs (vac vs matter + CP asym) =====================
import os, glob, csv, math
import matplotlib.pyplot as plt
from datetime import datetime, timezone

OUT_DIR = "/content/reports"
os.makedirs(OUT_DIR, exist_ok=True)

def _latest(pattern):
    paths = sorted(glob.glob(pattern))
    return paths[-1] if paths else None

def _load_two_col_csv(path, want_cols):
    # Returns dict[col] -> list of floats, aligned by row order
    data = {c: [] for c in want_cols}
    if not path: return data
    with open(path, "r") as f:
        rdr = csv.reader(f)
        for row in rdr:
            if not row or row[0].strip().startswith("#"):  # skip comments/blank
                continue
            # header line detection
            if ("E_GeV" in row[0]) or ("E[GeV]" in row[0]):
                continue
            # try parse floats
            try:
                vals = [float(x) for x in row]
            except:
                continue
            # map by requested indices
            # assume columns: E_GeV, P_mue, P_mumu, P_ee
            if len(vals) >= 4:
                data[want_cols[0]].append(vals[0])
                data[want_cols[1]].append(vals[1])  # P_mue
            elif len(vals) >= 2:
                data[want_cols[0]].append(vals[0])
                data[want_cols[1]].append(vals[1])
    return data

def _plot_lines(x, y_series, labels, title, savename):
    plt.figure()
    for y in y_series:
        plt.plot(x, y)
    plt.xlabel("E [GeV]")
    plt.ylabel("Probability")
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    path = os.path.join(OUT_DIR, savename)
    plt.savefig(path, dpi=140)
    plt.close()
    print(f"[FIG] {title} -> {path}")

def _bar_plot(x_labels, y_vals, title, savename):
    plt.figure()
    xs = range(len(x_labels))
    plt.bar(xs, y_vals)
    plt.xticks(xs, x_labels)
    plt.ylabel("A_CP")
    plt.title(title)
    plt.grid(True, axis="y", alpha=0.3)
    plt.tight_layout()
    path = os.path.join(OUT_DIR, savename)
    plt.savefig(path, dpi=140)
    plt.close()
    print(f"[FIG] {title} -> {path}")

stamp = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%SZ")

# ---- Experiments to render
exps = [
    ("T2K",  "UPT_vac_T2K_*",   "UPT_mat_T2K_*",   "UPT_vacbar_T2K_*",   "UPT_matbar_T2K_*"),
    ("NOvA", "UPT_vac_NOvA_*",  "UPT_mat_NOvA_*",  "UPT_vacbar_NOvA_*",  "UPT_matbar_NOvA_*"),
    ("DUNE", "UPT_vac_DUNE_*",  "UPT_mat_DUNE_*",  "UPT_vacbar_DUNE_*",  "UPT_matbar_DUNE_*"),
]

for name, vac_pat, mat_pat, vbar_pat, mbar_pat in exps:
    vac   = _latest(os.path.join(OUT_DIR, vac_pat + ".csv"))
    mat   = _latest(os.path.join(OUT_DIR, mat_pat + ".csv"))
    vbar  = _latest(os.path.join(OUT_DIR, vbar_pat + ".csv"))
    mbar  = _latest(os.path.join(OUT_DIR, mbar_pat + ".csv"))

    # appearance channel P(νμ→νe)
    cols = ["E", "Pmue"]
    dv   = _load_two_col_csv(vac,  cols)
    dm   = _load_two_col_csv(mat,  cols)
    dvb  = _load_two_col_csv(vbar, cols)
    dmb  = _load_two_col_csv(mbar, cols)

    if dv["E"] and dm["E"]:
        _plot_lines(
            dv["E"],
            [dv["Pmue"], dm["Pmue"]],
            ["vacuum", "matter"],
            f"{name}: P(νμ→νe) vacuum vs matter",
            f"FIG_{name}_Pmue_vac_vs_matter_{stamp}.png",
        )
    if dvb["E"] and dmb["E"]:
        _plot_lines(
            dvb["E"],
            [dvb["Pmue"], dmb["Pmue"]],
            ["vacuum (anti)", "matter (anti)"],
            f"{name}: P(ν̄μ→ν̄e) vacuum vs matter",
            f"FIG_{name}_Pmuebar_vac_vs_matter_{stamp}.png",
        )

# ---- CP asymmetry bars (uses your consolidated CSV if present)
mvv_csv = _latest(os.path.join(OUT_DIR, "UPT_matter_vs_vacuum_CONSOLIDATED_*.csv"))
if mvv_csv:
    # we expect blocks with columns including: E_GeV, Pmue_v, Pmue_m, Pmue_vbar, Pmue_mbar, A_CP(vac), A_CP(mat)
    # we’ll scan for each experiment tag line like: "# T2K", "# NOvA", "# DUNE"
    def _parse_block(lines, tag):
        rows = []
        active = False
        for ln in lines:
            L = ln.strip()
            if not L: continue
            if L.startswith("#"):
                active = (tag in L)
                continue
            if active and not L.startswith("#"):
                try:
                    parts = [p.strip() for p in L.split(",")]
                    if len(parts) >= 7:
                        E   = float(parts[0])
                        Acv = float(parts[-2])  # A_CP(vac)
                        Acm = float(parts[-1])  # A_CP(mat)
                        rows.append((E, Acv, Acm))
                except:
                    pass
        return rows

    with open(mvv_csv, "r") as f:
        lines = f.readlines()

    for tag in ["T2K", "NOvA", "DUNE"]:
        rows = _parse_block(lines, tag)
        if not rows: continue
        Es   = [r[0] for r in rows]
        Acv  = [r[1] for r in rows]
        Acm  = [r[2] for r in rows]
        labels = [f"{E:.2f}" for E in Es]
        _bar_plot(labels, Acv, f"{tag}: A_CP (vacuum)", f"FIG_{tag}_Acp_vac_{stamp}.png")
        _bar_plot(labels, Acm, f"{tag}: A_CP (matter)",  f"FIG_{tag}_Acp_matter_{stamp}.png")

print("[FIG] done.")
# ===================== /MODULE ====================================================================

# ===================== MODULE: ZIP ALL ARTIFACTS =====================
import os, glob, zipfile, datetime as dt

root = "/content/reports"
paths = sorted(glob.glob(os.path.join(root, "UPT_*")))
# include latest manifest if present
manifests = sorted(glob.glob(os.path.join(root, "UPT_RESULTS_MANIFEST_*.json")))
paths += manifests[-1:] if manifests else []

stamp = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%d-%H%M%SZ")
zip_path = os.path.join(root, f"UPT_ALL_ARTIFACTS_{stamp}.zip")

with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as z:
    for p in paths:
        arcname = os.path.basename(p)
        z.write(p, arcname)

print(f"[ZIP] {zip_path}  ({len(paths)} files)")
# ===================== /MODULE ======================================
